---

title: Fast Utils

keywords: fastai
sidebar: home_sidebar

summary: "Wrappers and helpers for interacting with fastai."
description: "Wrappers and helpers for interacting with fastai."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: notebooks/09_fast_utils.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="most_common_errors" class="doc_header"><code>most_common_errors</code><a href="https://github.com/hdmamin/incendio/tree/master/incendio/fast_utils.py#L14" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>most_common_errors</code>(<strong><code>interp</code></strong>)</p>
</blockquote>

<pre><code>More concise version of `most_confused`. Find the single most common
error for each true class.

Parameters
----------
interp: fastai ClassificationInterpretation

Returns
-------
dict[str, str]: Map true class to its most common incorrectly predicted
    class.</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="n_groups" class="doc_header"><code>n_groups</code><a href="https://github.com/hdmamin/incendio/tree/master/incendio/fast_utils.py#L34" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>n_groups</code>(<strong><code>learn</code></strong>)</p>
</blockquote>

<pre><code>Quickly get the number of layer groups in a fastai learner.

Parameters
----------
learn: fastai Learner
    It seems that the models themselves don't actually specify groups the
    way I've been doing in torch. Guessing this is because they've reworked
    the API so everything is Sequential and can be indexed into. This does
    make it easier to experiment with different groupings.

Returns
-------
int: 4 for default awd_lstm.</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h4 id="complete_sentence" class="doc_header"><code>complete_sentence</code><a href="https://github.com/hdmamin/incendio/tree/master/incendio/fast_utils.py#L53" class="source_link" style="float:right">[source]</a></h4><blockquote><p><code>complete_sentence</code>(<strong><code>learn</code></strong>, <strong><code>start_phrase</code></strong>, <strong><code>n_words</code></strong>=<em><code>30</code></em>, <strong><code>n_samples</code></strong>=<em><code>3</code></em>, <strong><code>temp</code></strong>=<em><code>0.75</code></em>)</p>
</blockquote>

<pre><code>Generate text from a given input. This is a decent way to get a glimpse
of how a language model is doing.

Parameters
----------
learn: fastai Learner
start_phrase: str
    The prompt (start of a sentence) that you want the model to complete.
n_words: int
    Number of words to generate after the given prompt.
n_samples: int
    Number of sample sentences to generate.
temp: float

Returns
-------
list[str]: Each item is one completed sentence.</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="LRPicker" class="doc_header"><code>class</code> <code>LRPicker</code><a href="https://github.com/hdmamin/incendio/tree/master/incendio/fast_utils.py#L78" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>LRPicker</code>(<strong><code>sugg_type</code></strong>:<code>('lr_min', 'lr_steep', 'avg')</code>=<em><code>'lr_min'</code></em>, <strong><code>resolve</code></strong>:<code>('avg', 'auto', 'manual')</code>=<em><code>'avg'</code></em>, <strong><code>tol_ratio</code></strong>=<em><code>0.1</code></em>)</p>
</blockquote>

<pre><code>Take user-suggested LR into account as an "anchor" to ensure `lr_find`
doesn't choose something too far out of the ordinary.</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="ULMFineTuner" class="doc_header"><code>class</code> <code>ULMFineTuner</code><a href="https://github.com/hdmamin/incendio/tree/master/incendio/fast_utils.py#L159" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>ULMFineTuner</code>(<strong><code>learn</code></strong>, <strong><code>name_fmt</code></strong>=<em><code>'cls_stage_{}'</code></em>)</p>
</blockquote>

<pre><code>Fine tune language model using ULM Fit procedure. I noticed the built-in
`fine_tune` method does not unfreeze 1 layer at a time as the paper
describes - not sure if they found that to be a better practice or if it's
just simpler for an automated method.

Originally, part of the reason for building this was to also decrease the
batch size at each stage since unfreezing eats up more memory with stored
gradients. However, I decided I'd rather not have to account for changing
batch size when selecting each stage's LR (we could run lr_find before each
stage but I opted for the simpler approach).</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="FastLabelEncoder" class="doc_header"><code>class</code> <code>FastLabelEncoder</code><a href="https://github.com/hdmamin/incendio/tree/master/incendio/fast_utils.py#L219" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>FastLabelEncoder</code>(<strong><code>learn</code></strong>)</p>
</blockquote>

<pre><code>Use a fastai learner to mimic a sklearn label encoder. This can be
useful because our standard evaluation code is often built when we are
trying out simple baseline models (e.g. logistic regression using sklearn).</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

