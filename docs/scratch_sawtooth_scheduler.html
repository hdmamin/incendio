---

title: Title

keywords: fastai
sidebar: home_sidebar



---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: notebooks/scratch_sawtooth_scheduler.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> autoreload
<span class="o">%</span><span class="k">autoreload</span> 2
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">boto3</span>
<span class="kn">from</span> <span class="nn">collections.abc</span> <span class="k">import</span> <span class="n">Iterable</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">operator</span> <span class="k">import</span> <span class="n">lt</span><span class="p">,</span> <span class="n">gt</span><span class="p">,</span> <span class="n">add</span><span class="p">,</span> <span class="n">sub</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">tabulate</span> <span class="k">import</span> <span class="n">tabulate</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">from</span> <span class="nn">accio.s3tool</span> <span class="k">import</span> <span class="n">S3tool</span>
<span class="kn">from</span> <span class="nn">htools</span> <span class="k">import</span> <span class="n">auto_repr</span><span class="p">,</span> <span class="n">valuecheck</span><span class="p">,</span> <span class="n">save</span>
<span class="kn">from</span> <span class="nn">incendio.utils</span> <span class="k">import</span> <span class="n">DEVICE</span>
<span class="kn">from</span> <span class="nn">incendio.optimizers</span> <span class="k">import</span> <span class="n">variable_lr_optimizer</span><span class="p">,</span> <span class="n">update_optimizer</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nd">@auto_repr</span>
<span class="k">class</span> <span class="nc">TorchCallback</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">on_train_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">lrs</span><span class="p">,</span> <span class="n">lr_mult</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="n">val_stats</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">on_epoch_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="n">val_stats</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">on_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">epoch</span><span class="p">,</span> <span class="n">stats</span><span class="p">,</span> <span class="n">val_stats</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">on_batch_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">sum_i</span><span class="p">,</span> <span class="n">stats</span><span class="p">):</span>
        <span class="k">pass</span>

    <span class="k">def</span> <span class="nf">on_batch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">sum_i</span><span class="p">,</span> <span class="n">stats</span><span class="p">):</span>
        <span class="k">pass</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">SchedulerMixin</span><span class="p">(</span><span class="n">TorchCallback</span><span class="p">):</span>

    <span class="n">verbose</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">on_train_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">plot_lrs</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">out_dir</span><span class="p">,</span> <span class="s1">&#39;lrs.png&#39;</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">update_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lrs</span><span class="p">[</span><span class="n">n</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">IndexError</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="n">update_optimizer</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">optim</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">lr_mult</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_mult</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">:</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="n">f</span><span class="s1">&#39;Set learning rate to </span><span class="si">{lr:.4f}</span><span class="s1">.&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">plot_lrs</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Display learning rate by iteration.</span>

<span class="sd">        Note: If the plot is not as smooth as expected, this likely</span>
<span class="sd">        means that there are very few iterations per epoch</span>
<span class="sd">        (i.e. the batch size is very large, at least in relative terms).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lrs</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Iteration&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Learning Rate&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Learning Rate Schedule&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">path</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">savefig</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">SawtoothScheduler</span><span class="p">(</span><span class="n">SchedulerMixin</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">add</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">priority</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">=</span> <span class="n">add</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">patience</span> <span class="o">=</span> <span class="n">patience</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">priority</span> <span class="o">=</span> <span class="n">priority</span>
        
        <span class="c1"># These are reset in `on_train_begin`, but types remain the same.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lrs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">since_improve</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recent_best</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_mult</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="k">def</span> <span class="nf">on_train_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">lrs</span><span class="p">,</span> <span class="n">lr_mult</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Wrapper to schedule learning rates depending on chosen method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        restarts: bool</span>
<span class="sd">            If True, use schedule with restarts. If False, use regular</span>
<span class="sd">            cosine annealing that spans whole duration of training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        np.array: LR for each iteration (i.e. output[i] is the LR to use</span>
<span class="sd">            at iteration i).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lrs</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">since_improve</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">recent_best</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_mult</span> <span class="o">=</span> <span class="n">lr_mult</span>

    <span class="k">def</span> <span class="nf">on_batch_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">sum_i</span><span class="p">,</span> <span class="n">stats</span><span class="p">):</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;loss&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span> <span class="k">return</span>
        
        <span class="n">lr</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">trainer</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">loss</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">recent_best</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">recent_best</span> <span class="o">=</span> <span class="n">loss</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">since_improve</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">lr</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span>
        <span class="k">elif</span> <span class="n">loss</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">recent_best</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">since_improve</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">patience</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">since_improve</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">lr</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">add</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">since_improve</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">since_improve</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">lr</span> <span class="o">*=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>
        <span class="n">update_optimizer</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">optim</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">lr_mult</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">lr_mult</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_size</span><span class="p">,</span> <span class="n">out_size</span><span class="p">)</span>
        <span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">out_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">groups</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">fc1</span><span class="p">,</span> <span class="n">fc2</span><span class="p">])</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">g</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">groups</span><span class="p">:</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">g</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">variable_lr_optimizer</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="p">[</span><span class="mf">2e-3</span><span class="p">,</span> <span class="mf">3e-3</span><span class="p">])</span>
<span class="nb">max</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">opt</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.003</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">class</span> <span class="nc">CosineLRScheduler</span><span class="p">(</span><span class="n">SchedulerMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Learning rate scheduler that makes updates each batch.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">warm</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">restarts</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cycle_len</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">cycle_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
                 <span class="n">min_lr</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">priority</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        warm: float</span>
<span class="sd">            Percent of training run (or cycle length) devoted to the increasing</span>
<span class="sd">            portion of the schedule. Default 0.3.</span>
<span class="sd">        restarts: bool</span>
<span class="sd">            Specifies whether to use restarts, i.e. use a cyclical LR.</span>
<span class="sd">            True: Version of cosine annealing with restarts. In one</span>
<span class="sd">                  cycle, LR starts high and gradually decreases.</span>
<span class="sd">                  At the start of the next cycle, it is</span>
<span class="sd">                  immediately increased again.</span>
<span class="sd">            False: Version of cosine annealing where LR increases</span>
<span class="sd">                   for first 30% of training, then decreases for</span>
<span class="sd">                   remaining 70%.</span>
<span class="sd">        cycle_len: int</span>
<span class="sd">            Number of epochs contained in a single cycle. Only used</span>
<span class="sd">            when scheduler uses restarts.</span>
<span class="sd">        cycle_decay: float</span>
<span class="sd">            Scalar to decay the learning rate at the end of each cycle.</span>
<span class="sd">            This is only used with restarts, since the regular cosine</span>
<span class="sd">            annealing already decays the LR over time.</span>
<span class="sd">            E.g. 1.0 will use no decay.</span>
<span class="sd">            0.9 means that cycle 2 LRs = cycle 1 LRs * 0.9,</span>
<span class="sd">            cycle 3 LRs = cycle 1 LRs * .81,</span>
<span class="sd">            etc.</span>
<span class="sd">        min_lr: float</span>
<span class="sd">            Minimum learning rate. If None is specified, it will be set</span>
<span class="sd">            to max_lr / 10.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warm</span> <span class="o">=</span> <span class="n">warm</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cycle_len</span> <span class="o">=</span> <span class="n">cycle_len</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cycle_decay</span> <span class="o">=</span> <span class="n">cycle_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">restarts</span> <span class="o">=</span> <span class="n">restarts</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">=</span> <span class="n">verbose</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span> <span class="o">=</span> <span class="n">min_lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">priority</span> <span class="o">=</span> <span class="n">priority</span>

        <span class="c1"># Set in `on_train_begin()`.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lrs</span> <span class="o">=</span> <span class="kc">None</span>             <span class="c1"># Iterable[float]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batches_per_e</span> <span class="o">=</span> <span class="kc">None</span>   <span class="c1"># int</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batches</span> <span class="o">=</span> <span class="kc">None</span>         <span class="c1"># int</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_lr</span> <span class="o">=</span> <span class="kc">None</span>          <span class="c1"># float</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_mult</span> <span class="o">=</span> <span class="kc">None</span>         <span class="c1"># float</span>

    <span class="k">def</span> <span class="nf">on_train_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">lrs</span><span class="p">,</span> <span class="n">lr_mult</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Wrapper to schedule learning rates depending on chosen method.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        restarts: bool</span>
<span class="sd">            If True, use schedule with restarts. If False, use regular</span>
<span class="sd">            cosine annealing that spans whole duration of training.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        np.array: LR for each iteration (i.e. output[i] is the LR to use</span>
<span class="sd">            at iteration i).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batches_per_e</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">dl_train</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">batches</span> <span class="o">=</span> <span class="n">epochs</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batches_per_e</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_lr</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">lrs</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">lrs</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">)</span> <span class="k">else</span> <span class="n">lrs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr_mult</span> <span class="o">=</span> <span class="n">lr_mult</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_lr</span> <span class="o">/</span> <span class="mi">10</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">restarts</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">batches</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle_len</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;Training will be less than 1 full cycle.&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">restarts</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lrs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cosine_restarts_schedule</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lrs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cosine_schedule</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">on_batch_begin</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">trainer</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">sum_i</span><span class="p">,</span> <span class="n">stats</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">update_lr</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">sum_i</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">_cosine_anneal</span><span class="p">(</span><span class="n">batches</span><span class="p">,</span> <span class="n">lr1</span><span class="p">,</span> <span class="n">lr2</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Helper function for _cosine_schedule().</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        batches: int</span>
<span class="sd">            Number of batches in segment.</span>
<span class="sd">        lr1: float</span>
<span class="sd">            Learning rate at start of segment.</span>
<span class="sd">        lr2: float</span>
<span class="sd">            Learning rate at end of segment.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        np.array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">lr2</span> <span class="o">+</span> <span class="p">(</span><span class="n">lr1</span> <span class="o">-</span> <span class="n">lr2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">i</span><span class="o">/</span><span class="n">batches</span><span class="p">))</span><span class="o">/</span><span class="mi">2</span>

    <span class="k">def</span> <span class="nf">_cosine_schedule</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Cosine annealing scheduler. Computes learning rates for each</span>
<span class="sd">        iteration.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        np.array</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">seg1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cosine_anneal</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">warm</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batches</span><span class="p">),</span>
                                   <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_lr</span><span class="p">)</span>
        <span class="n">seg2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_cosine_anneal</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batches</span><span class="p">)),</span>
                                   <span class="bp">self</span><span class="o">.</span><span class="n">max_lr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">seg1</span><span class="p">,</span> <span class="n">seg2</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">_cosine_restarts_schedule</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Cosine annealing with restarts.&quot;&quot;&quot;</span>
        <span class="n">cycles</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batches</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cycle_len</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batches_per_e</span><span class="p">)))</span>
        <span class="n">cycle_batches</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle_len</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">batches_per_e</span>
        <span class="n">lrs</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">_cosine_anneal</span><span class="p">(</span><span class="n">cycle_batches</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_lr</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span><span class="p">)</span>
               <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">cycle_decay</span> <span class="o">*</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">cycles</span><span class="p">)]</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span><span class="n">lrs</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

</div>
 

