{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Losses\n",
    "\n",
    "> Custom loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from htools import add_docstring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def soft_label_cross_entropy_with_logits(y_pred, y_true, reduction='mean'):\n",
    "    \"\"\"Compute cross entropy with soft labels. PyTorch's built in \n",
    "    multiclass cross entropy functions require us to pass in integer\n",
    "    indices, which doesn't allow for soft labels which are shaped like\n",
    "    a one hot encoding. FastAI's label smoothing loss uniformly divides\n",
    "    uncertainty over all classes, which again does not allow us to pass\n",
    "    in our own soft labels.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred: torch.FloatTensor\n",
    "        Logits output by the model. \n",
    "        Shape (bs, num_classes).\n",
    "    y_true: torch.FloatTensor\n",
    "        Soft labels, where values are between 0 and 1. \n",
    "        Shape (bs, num_classes).\n",
    "    reduction: str\n",
    "        One of ('mean', 'sum', 'none'). This determines how to reduce\n",
    "        the output of the function, similar to most PyTorch\n",
    "        loss functions.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.FloatTensor: If reduction is 'none', this will have shape\n",
    "        (bs, ). If 'mean' or 'sum', this will be be a tensor with a \n",
    "        single value (no shape).\n",
    "    \"\"\"\n",
    "    res = (-y_true * F.log_softmax(y_pred)).sum(-1)\n",
    "    if reduction == 'none': return res\n",
    "    return getattr(res, reduction)(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@add_docstring(soft_label_cross_entropy_with_logits)\n",
    "def soft_label_cross_entropy(y_pred, y_true, reduction='mean'):\n",
    "    \"\"\"Same as `soft_label_cross_entropy_with_logits` but operates on\n",
    "    softmax output instead of logits. The version with logits is \n",
    "    recommended for numerical stability. Below is the docstring for the logits \n",
    "    version. The only difference in this version is that y_pred will not be\n",
    "    logits.\n",
    "    \"\"\"\n",
    "    res = -y_true * torch.log(y_pred)\n",
    "    res = torch.where(torch.isnan(res), torch.zeros_like(res), res).sum(-1)\n",
    "    if reduction == 'none': return res\n",
    "    return getattr(res, reduction)(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
