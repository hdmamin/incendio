{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T21:21:53.650926Z",
     "start_time": "2021-08-07T21:21:52.335912Z"
    }
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T21:21:53.675070Z",
     "start_time": "2021-08-07T21:21:53.652914Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T21:21:56.575553Z",
     "start_time": "2021-08-07T21:21:54.494331Z"
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "import boto3\n",
    "from collections.abc import Iterable\n",
    "from comet_ml import Experiment\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from operator import lt, gt, add, sub\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tabulate import tabulate\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "\n",
    "try:\n",
    "    from accio.s3tool import S3tool\n",
    "except ImportError:\n",
    "    warnings.warn('Accio not available.')\n",
    "from htools import auto_repr, valuecheck, save, delegate\n",
    "from incendio.data import BotoUploader, dataloader_subset\n",
    "from incendio.optimizers import variable_lr_optimizer, update_optimizer\n",
    "from incendio.utils import DEVICE, is_builtin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T21:22:08.250839Z",
     "start_time": "2021-08-07T21:22:08.209208Z"
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "@auto_repr\n",
    "class TorchCallback:\n",
    "\n",
    "    def on_train_begin(self, trainer, epochs, lrs, lr_mult, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_begin(self, trainer, epoch, val_stats):\n",
    "        pass\n",
    "\n",
    "    def on_batch_begin(self, trainer, i, sum_i):\n",
    "        pass\n",
    "    \n",
    "    def after_zero_grad(self, trainer, i, sum_i, xb, yb):\n",
    "        pass\n",
    "    \n",
    "    def after_forward(self, trainer, i, sum_i):\n",
    "        pass\n",
    "    \n",
    "    def after_loss(self, trainer, i, sum_i):\n",
    "        pass\n",
    "        \n",
    "    def after_backward(self, trainer, i, sum_i):\n",
    "        pass\n",
    "    \n",
    "    def after_step(self, trainer, i, sum_i):\n",
    "        pass\n",
    "\n",
    "    def on_batch_end(self, trainer, i, sum_i):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_end(self, trainer, epoch, val_stats):\n",
    "        pass\n",
    "    \n",
    "    def on_train_end(self, trainer, epoch, val_stats):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BasicConfig(TorchCallback):\n",
    "    \"\"\"Handles basic model tasks like putting the model on the GPU\n",
    "    and switching between train and eval modes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, order=0):\n",
    "        self.order = order\n",
    "\n",
    "    def on_train_begin(self, trainer, epochs, lrs, lr_mult, **kwargs):\n",
    "        trainer.net.to(DEVICE)\n",
    "        if not trainer.optim:\n",
    "            trainer.optim = variable_lr_optimizer(\n",
    "                trainer.net, lrs, lr_mult, trainer.optim_type, trainer.eps\n",
    "            )\n",
    "        else:\n",
    "            update_optimizer(trainer.optim, lrs, lr_mult=lr_mult)\n",
    "        trainer.logger.info(trainer.optim)\n",
    "        if kwargs.get('clean') is True: trainer.cleanup(confirmed=True)\n",
    "\n",
    "    def on_epoch_begin(self, trainer, *args, **kwargs):\n",
    "        trainer.net.train()\n",
    "\n",
    "    def on_train_end(self, trainer, *args, **kwargs):\n",
    "        trainer.logger.info('Training complete. Model in eval mode.')\n",
    "        trainer.net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class StatsHandler(TorchCallback):\n",
    "    \"\"\"This updates metrics at the end of each epoch to account for\n",
    "    potentially varying batch sizes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, order=5):\n",
    "        self.order = order\n",
    "\n",
    "    def on_epoch_begin(self, trainer, epoch, val_stats):\n",
    "        \"\"\"Resets stats at the start of each epoch.\"\"\"\n",
    "        trainer.stats.clear()\n",
    "\n",
    "    def on_epoch_end(self, trainer, epoch, val_stats):\n",
    "        \"\"\"Computes (possibly weighted) averages of mini-batch stats\n",
    "        at the end of each epoch.\n",
    "        \"\"\"\n",
    "        for group in (trainer.stats, val_stats):\n",
    "            for k, v in group.items():\n",
    "                if k == 'batch_size': continue\n",
    "                group[k] = np.average(v, weights=group['batch_size'])\n",
    "            group.pop('batch_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-07T21:22:10.967348Z",
     "start_time": "2021-08-07T21:22:10.926830Z"
    }
   },
   "outputs": [],
   "source": [
    "# export\n",
    "class SubsetHandler(TorchCallback):\n",
    "    \"\"\"This lets us train on small subsets of data which is often a good idea\n",
    "    during the early phases of model development and debugging (e.g. try to\n",
    "    overfit 1 or n batches).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, order=1):\n",
    "        \"\"\"Important for order to be smaller than MetricPrinter or any\n",
    "        LR scheduler callbacks.\n",
    "        \"\"\"\n",
    "        self.order = order\n",
    "        self.using_subset = False\n",
    "        \n",
    "    def on_train_begin(self, trainer, *args, **kwargs):\n",
    "        n_batches = kwargs.get('overfit_batches')\n",
    "        if n_batches is None: return\n",
    "        \n",
    "        # Tell trainer to use subset instead of full dataloaders for this run. \n",
    "        # These settings will be reset at the end of the run.\n",
    "        trainer._dl_train_curr = dataloader_subset(trainer.dl_train, n_batches)\n",
    "        trainer._dl_val_curr = dataloader_subset(trainer.dl_val, n_batches)\n",
    "        self.using_subset = True\n",
    "\n",
    "    def on_train_end(self, trainer, *args, **kwargs):\n",
    "        if self.using_subset:\n",
    "            trainer._dl_train_curr = trainer.dl_train\n",
    "            trainer._dl_val_curr = trainer.dl_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MetricPrinter(TorchCallback):\n",
    "    \"\"\"Prints metrics at the end of each epoch. This is one of the\n",
    "    default callbacks provided in BaseModel - it does not need to\n",
    "    be passed in explicitly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pbar_metric='loss', batch_freq=1, order=10):\n",
    "        \"\"\"Order must be higher than StatsHandler, otherwise\n",
    "        metrics will be printed before they're aggregated.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        pbar_metric: str\n",
    "            Metric to update in the tqdm progress bar. This will be \n",
    "            the value computed on the most recent mini batch in the\n",
    "            training set.\n",
    "        batch_freq: int\n",
    "            How often to update the tqdm progress bar with mini batch\n",
    "            stats. The default of 1 means we'll update it every mini\n",
    "            batch, which can be too fast to read if mini batches\n",
    "            are processed quickly (e.g. if batch size is small and the\n",
    "            forward pass is fast).\n",
    "        order: int\n",
    "        \"\"\"\n",
    "        self.pbar_metric = pbar_metric\n",
    "        self.order = order\n",
    "        self.batch_freq = batch_freq\n",
    "\n",
    "    def on_train_begin(self, trainer, *args, **kwargs):\n",
    "        trainer.logger = trainer.get_logger(\n",
    "            os.path.join(trainer.out_dir, 'train.log'),\n",
    "            fmt='\\n%(message)s'\n",
    "        )\n",
    "        \n",
    "    def on_epoch_begin(self, trainer, epoch, val_stats):\n",
    "        \"\"\"Create progress bar.\"\"\"\n",
    "        trainer.pbar = tqdm(trainer._dl_train_curr, leave=False)\n",
    "\n",
    "    def on_epoch_end(self, trainer, epoch, val_stats):\n",
    "        \"\"\"Print stats and close progress bar.\"\"\"\n",
    "        data = [[k, v, val_stats[k]] for k, v in trainer.stats.items()]\n",
    "        table = tabulate(data, headers=['Metric', 'Train', 'Validation'],\n",
    "                         tablefmt='github', floatfmt='.4f')\n",
    "        trainer.logger.info(\n",
    "            f'\\n{\"=\"*5}\\n\\nEpoch {epoch}\\n\\n{table}\\n'\n",
    "        )\n",
    "        trainer.pbar.close()\n",
    "        \n",
    "    def on_batch_end(self, trainer, i, sum_i):\n",
    "        \"\"\"Update progress bar with batch stats.\"\"\"\n",
    "        if sum_i % self.batch_freq != 0: \n",
    "            return\n",
    "        kwargs = {self.pbar_metric: \n",
    "                  format(trainer.stats[self.pbar_metric][-1], '.4f')}\n",
    "        trainer.pbar.set_postfix(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BatchMetricPrinter(TorchCallback):\n",
    "    \"\"\"Prints mini batch metrics to help us see if a model is \n",
    "    learning early in training (helpful for debugging). We\n",
    "    remove the callback after the specified number of prints\n",
    "    so that it isn't called unnecessarily throughout the whole\n",
    "    training process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_freq, n_prints=float('inf'), order=10):\n",
    "        \"\"\"Order must be higher than StatsHandler, otherwise\n",
    "        metrics will be printed before they're aggregated.\n",
    "        \"\"\"\n",
    "        self.order = order\n",
    "        self.batch_freq = batch_freq\n",
    "        self.n_prints = n_prints\n",
    "        self.curr_prints = 0\n",
    "\n",
    "    def on_batch_end(self, trainer, i, sum_i):\n",
    "        if sum_i % batch_freq:\n",
    "            self.curr_prints += 1\n",
    "            metric_str = \"\\n\".join(\n",
    "                f'{k}={round(v[-1], 4)}' for k, v in trainer.stats.items()\n",
    "            )\n",
    "            trainer.logger.info(f'Batch {sum_i}\\n: {metric_str}')\n",
    "        if self.curr_prints >= self.n_prints:\n",
    "            trainer.callbacks.pop(type(self).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class EarlyStopper(TorchCallback):\n",
    "\n",
    "    @valuecheck\n",
    "    def __init__(self, metric, goal:('max', 'min'), min_improvement=0.0,\n",
    "                 patience=3, order=15):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        metric: str\n",
    "            Quantity to monitor. This will always be computed on the\n",
    "            validation set.\n",
    "        goal: str\n",
    "            Indicates what we want to do to the metric in question.\n",
    "            Either 'min' or 'max'. E.g. metric 'loss' should have goal 'min'\n",
    "            while metric 'precision' should have goal 'max'.\n",
    "        min_improvement: float\n",
    "            Amount of change needed to qualify as improvement. For example,\n",
    "            min_improvement of 0.0 means any improvement is sufficient. With\n",
    "            a min_improvent of 0.2, we will stop training even if the\n",
    "            quantity improves by, for example, 0.1.\n",
    "        patience: int\n",
    "            Number of acceptable epochs without improvement. E.g. patience=0\n",
    "            means the metric must improve every epoch for training to continue.\n",
    "        \"\"\"\n",
    "        # Will use op like: self.op(new_val, current_best)\n",
    "        if goal == 'min':\n",
    "            self.init_metric = self.best_metric = float('inf')\n",
    "            self.op = lt\n",
    "            self.op_best = sub\n",
    "        elif goal == 'max':\n",
    "            self.init_metric = self.best_metric = float('-inf')\n",
    "            self.op = gt\n",
    "            self.op_best = add\n",
    "\n",
    "        self.order = order\n",
    "        self.metric = metric\n",
    "        self.min_improvement = min_improvement\n",
    "        self.patience = patience\n",
    "        self.since_improvement = 0\n",
    "\n",
    "    def on_train_begin(self, trainer, *args, **kwargs):\n",
    "        \"\"\"Resets tracked variables at start of training.\"\"\"\n",
    "        self.best_metric = self.init_metric\n",
    "        self.since_improvement = 0\n",
    "\n",
    "    def on_epoch_end(self, trainer, epoch, val_stats):\n",
    "        # Error handling.\n",
    "        new_val = val_stats.get(self.metric)\n",
    "        if new_val is None:\n",
    "            trainer.logger.info(f'EarlyStopper could not find {self.metric}. '\n",
    "                                f'Callback behavior may not be enforced.')\n",
    "            return\n",
    "\n",
    "        # Expected behavior.\n",
    "        if self.op(new_val, self.op_best(self.best_metric, self.min_improvement)):\n",
    "            self.best_metric = new_val\n",
    "            self.since_improvement = 0\n",
    "        else:\n",
    "            self.since_improvement += 1\n",
    "            if self.since_improvement > self.patience:\n",
    "                trainer.logger.info(\n",
    "                    f'EarlyStopper halting training: validation {self.metric} '\n",
    "                    f'has not improved enough in {self.since_improvement} epochs.'\n",
    "                )\n",
    "                trainer._stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class PerformanceThreshold(TorchCallback):\n",
    "\n",
    "    @valuecheck\n",
    "    def __init__(self, metric, goal:('min', 'max'), threshold, skip_epochs=0,\n",
    "                 split:('train', 'val')='val', order=15):\n",
    "        self.order = order\n",
    "        self.metric = metric\n",
    "        self.threshold = threshold\n",
    "        self.skip_epochs = skip_epochs\n",
    "        self.split = split\n",
    "        self.op = gt if goal == 'min' else lt\n",
    "\n",
    "    def on_epoch_end(self, trainer, epoch, val_stats):\n",
    "        if epoch < self.skip_epochs:\n",
    "            return\n",
    "\n",
    "        # Error handling.\n",
    "        data = val_stats if self.split == 'val' else trainer.stats\n",
    "        new_val = data.get(self.metric)\n",
    "        if new_val is None:\n",
    "            trainer.logger.info(f'{self.metric.title()} not found in metrics. '\n",
    "                                 'PerformanceThreshold may not be enforced.')\n",
    "            return\n",
    "\n",
    "        # Expected behavior.\n",
    "        if self.op(new_val, self.threshold):\n",
    "            trainer.logger.info(\n",
    "                f'PerformanceThreshold halting training: {self.metric} '\n",
    "                f'of {new_val:.4f} did not meet threshold.'\n",
    "            )\n",
    "            trainer._stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ModelCheckpoint(TorchCallback):\n",
    "\n",
    "    @valuecheck\n",
    "    def __init__(self, metric='loss', goal:('max', 'min')='min', \n",
    "                 fname='trainer.pkl', metric_fname='best_val_metrics.json', \n",
    "                 order=25):\n",
    "        # Will use op like: self.op(new_val, current_best)\n",
    "        if goal == 'min':\n",
    "            self.init_metric = self.best_metric = float('inf')\n",
    "            self.op = lt\n",
    "            self.op_best = sub\n",
    "        elif goal == 'max':\n",
    "            self.init_metric = self.best_metric = float('-inf')\n",
    "            self.op = gt\n",
    "            self.op_best = add\n",
    "\n",
    "        self.fname = fname\n",
    "        self.metric_fname = metric_fname\n",
    "        self.order = order\n",
    "        self.metric = metric\n",
    "        self.metric_path = None\n",
    "\n",
    "    def on_train_begin(self, trainer, *args, **kwargs):\n",
    "        self.best_metric = self.init_metric\n",
    "        self.metric_path = os.path.join(trainer.out_dir, self.metric_fname)\n",
    "        self.last_saved_epoch = None\n",
    "\n",
    "    def on_epoch_end(self, trainer, epoch, val_stats):\n",
    "        new_val = val_stats.get(self.metric)\n",
    "        # Error handling.\n",
    "        if new_val is None:\n",
    "            trainer.logger.info(f'{self.metric} not found in metrics.'\n",
    "                                 'ModelCheckpoint may not save models.')\n",
    "            return\n",
    "\n",
    "        # Expected behavior.\n",
    "        if self.op(new_val, self.best_metric):\n",
    "            trainer.logger.info(\n",
    "                f'Saving model. {self.metric.title()} improved from '\n",
    "                f'{self.best_metric:.4f} to {new_val:.4f}.'\n",
    "            )\n",
    "            trainer.save(self.fname)\n",
    "            save({k: round(v, 5) for k, v in val_stats.items()},\n",
    "                 self.metric_path)\n",
    "            self.best_metric = new_val\n",
    "            self.last_saved_epoch = epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class MetricHistory(TorchCallback):\n",
    "    \"\"\"Separate from StatsHandler in case we don't want to log outputs.\"\"\"\n",
    "\n",
    "    def __init__(self, fname='history.csv', \n",
    "                 plot_fname='history.png', order=90):\n",
    "        self.train_hist = []\n",
    "        self.val_hist = []\n",
    "        self.fname, self.f_path = fname, None\n",
    "        self.plot_fname, self.plot_path = plot_fname, None\n",
    "        self.order = order\n",
    "\n",
    "    def on_train_begin(self, trainer, *args, **kwargs):\n",
    "        self.train_hist.clear()\n",
    "        self.val_hist.clear()\n",
    "        # Better to catch errors in names at start of training than at end.\n",
    "        if self.fname:\n",
    "            self.f_path = os.path.join(trainer.out_dir, self.fname)\n",
    "        if self.plot_fname: \n",
    "            self.plot_path = os.path.join(trainer.out_dir, self.plot_fname)\n",
    "                  \n",
    "\n",
    "    def on_epoch_end(self, trainer, epoch, val_stats):\n",
    "        self.train_hist.append(trainer.stats.copy())\n",
    "        self.val_hist.append(val_stats.copy())\n",
    "\n",
    "    def on_train_end(self, trainer, epoch, val_stats):\n",
    "        self.df = pd.concat([\n",
    "            pd.DataFrame(self.train_hist),\n",
    "            pd.DataFrame(self.val_hist)\\\n",
    "              .rename(lambda x: f'val_{x}', axis='columns')\n",
    "        ], axis=1)\n",
    "        if self.f_path: self.df.round(5).to_csv(self.f_path, index=False)\n",
    "        self.plot(self.plot_path)\n",
    "\n",
    "    def plot(self, path=None):\n",
    "        df_cols = self.df.shape[1]\n",
    "        n_rows = max(1, df_cols // 4)\n",
    "        n_cols = df_cols // (n_rows * 2)\n",
    "        fig, ax = plt.subplots(n_rows, n_cols, figsize=(12, 3*n_rows))\n",
    "        if isinstance(ax, Iterable): \n",
    "            ax = ax.flatten()\n",
    "        else:\n",
    "            # If no extra metrics are specified, we only have 1 plot with loss.\n",
    "            ax = [ax]\n",
    "        for col, axi in zip(self.df.columns, ax):\n",
    "            axi.plot(self.df[col], label='train')\n",
    "            axi.plot(self.df[f'val_{col}'], label='val')\n",
    "            axi.set_title(col.title())\n",
    "            axi.set_xlabel('Epoch')\n",
    "            axi.set_ylabel('Score')\n",
    "            axi.legend()\n",
    "        plt.tight_layout()\n",
    "        if path:\n",
    "            plt.savefig(path)\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class S3Uploader(TorchCallback):\n",
    "    \"\"\"Upload model and logs to S3 when training finishes.\"\"\"\n",
    "\n",
    "    def __init__(self, bucket, prefix, order=95):\n",
    "        \"\"\"See Accio documentation for parameter details.\"\"\"\n",
    "        self.bucket = bucket\n",
    "        self.prefix = prefix\n",
    "        self.order = order\n",
    "\n",
    "    def on_train_end(self, trainer, *args, **kwargs):\n",
    "        \"\"\"Upload files to s3 once training completes.\"\"\"\n",
    "        paths = [f.path for f in os.scandir(trainer.out_dir)\n",
    "                 if f.is_file() and not f.name.startswith('.')]\n",
    "        s3 = S3tool()\n",
    "        try:\n",
    "            s3.upload_files(paths, self.bucket, self.prefix)\n",
    "        except Exception as e:\n",
    "            trainer.logger.error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BotoS3Uploader(TorchCallback):\n",
    "    \"\"\"Upload model and logs to S3 when training finishes. This version of the\n",
    "    callback does not rely on any GoGuardian packages.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bucket, s3_dir='', retain_tree=True, recurse=True, \n",
    "                 keep_fn=None, order=95):\n",
    "        \"\"\"       \n",
    "        Parameters\n",
    "        ----------\n",
    "        bucket: str\n",
    "            Name of s3 bucket to upload to. For a single project, I generally\n",
    "            stick to a single bucket so we can usually keep this fixed. We can\n",
    "            always change the attribute later if necessary.\n",
    "        s3_dir: str\n",
    "            If provided, this will pre prepended to each path: {s3_dir}/{path}.\n",
    "            Otherwise, s3 paths will be the same as local paths.\n",
    "        retain_tree: bool\n",
    "            If True, the local file structure will be retained. Otherwise,\n",
    "            only the base name is kept. All four combinations of retain_tree \n",
    "            (True/False) and s3_dir (empty/non-empty) are supported.\n",
    "        recurse: bool\n",
    "            If True, upload all files in subdirectories as well.\n",
    "        keep_fn: None or callable\n",
    "            If provided, this should be a function that accepts a filename as\n",
    "            input and returns a boolean specifying whether to include it in the\n",
    "            upload or not.\n",
    "        \"\"\"\n",
    "        self.bucket = bucket\n",
    "        self.s3_dir = s3_dir\n",
    "        self.retain_tree = retain_tree\n",
    "        self.recurse = recurse\n",
    "        self.keep_fn = keep_fn\n",
    "        self.order = order\n",
    "\n",
    "    def on_train_end(self, trainer, *args, **kwargs):\n",
    "        \"\"\"Upload files to s3 once training completes.\"\"\"\n",
    "        paths = [f.path for f in os.scandir(trainer.out_dir)\n",
    "                 if f.is_file() and not f.name.startswith('.')]\n",
    "        s3 = BotoUploader(self.bucket, self.verbose)\n",
    "        try:\n",
    "            self.s3.upload_folder(trainer.out_dir, self.s3_dir, \n",
    "                                  self.retain_tree, self.recurse, self.keep_fn)\n",
    "        except Exception as e:\n",
    "            trainer.logger.error(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'builtins'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# export\n",
    "class CometCallback(TorchCallback):\n",
    "    \"\"\"Logs metrics to comet ML. Loss is logged per batch, everything else is\n",
    "    logged per epoch.\n",
    "    \n",
    "    Note: This could use some more work to add more options to store other\n",
    "    things, but I want to use what I have so far on my self-supervised \n",
    "    learning experiments.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, project_name, exp_name=None, tags=(), order=100):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        project_name: str\n",
    "            Name of the project in Comet.\n",
    "        exp_name: str\n",
    "            Name of the current training run (I typically use names like v1,\n",
    "            v2, etc.).\n",
    "        tags: Iterable[str]\n",
    "            Optional: add tags to the training run. These are things you want\n",
    "            to be highly visible when looking at many different runs in the\n",
    "            Comet UI. This will typically be major choices like the name of a\n",
    "            model architecture or pre-training task, not small specifics like\n",
    "            the value of epsilon for an Adam optimizer.\n",
    "        order: int\n",
    "            Determines order callbacks are executed in.\n",
    "        \"\"\"\n",
    "        # High order so this comes after MetricHandler.\n",
    "        self.project_name = project_name\n",
    "        self.order = order\n",
    "        self.exp_name = exp_name\n",
    "        self.tags = list(tags)\n",
    "        # Define when training begins so reported time is more meaningful.\n",
    "        self.exp = None\n",
    "    \n",
    "    def on_train_begin(self, trainer, epochs, lrs, lr_mult, **kwargs):\n",
    "        self.exp = Experiment(project_name=self.project_name)\n",
    "        if self.exp_name: self.exp.set_name(self.exp_name)\n",
    "        if self.tags: self.exp.add_tags(self.tags)\n",
    "        params = {k: v for k, v in vars(trainer).items() if is_builtin(v)}\n",
    "        self.exp.log_parameters(\n",
    "            dict(**params, **kwargs, epochs=epochs, lrs=lrs, lr_mult=lr_mult)\n",
    "        )\n",
    "\n",
    "    def on_train_end(self, trainer, epoch, val_stats):\n",
    "        self.exp.end()\n",
    "\n",
    "    def on_epoch_end(self, trainer, epoch, val_stats):\n",
    "        self.exp.log_metrics(trainer.stats, prefix='train', epoch=epoch)\n",
    "        self.exp.log_metrics(val_stats, prefix='val', epoch=epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CometGradientCallback(CometCallback):\n",
    "    \"\"\"Contains standard Comet.ml tracking but adds some information about \n",
    "    gradients. At the moment, this creates a single bar plot at the end of \n",
    "    training displaying the average gradient magnitudes by layer. Eventually,\n",
    "    we might want to update this to show how magnitudes change by epoch. The\n",
    "    only issue here is for a model with many layers (e.g. hundreds), this\n",
    "    becomes tricky to visualize in a useful way with the added time dimension.\n",
    "    \"\"\"\n",
    "\n",
    "    def on_train_begin(self, trainer, epochs, lrs, lr_mult, **kwargs):\n",
    "        super().on_train_begin(trainer, epochs, lrs, lr_mult, **kwargs)\n",
    "        self.means = defaultdict(list)\n",
    "        self.stds = defaultdict(list)\n",
    "\n",
    "    def after_backward(self, trainer, i, sum_i):\n",
    "        for name, weights in trainer.net.named_parameters():\n",
    "            if 'bias' in name or not weights.requires_grad:\n",
    "                continue\n",
    "            abs_grads = np.abs(weights.grad.detach().cpu().numpy())\n",
    "            self.means[name].append(abs_grads.mean())\n",
    "            self.stds[name].append(abs_grads.std())\n",
    "\n",
    "    def on_train_end(self, trainer, epoch, val_stats):\n",
    "        \"\"\"Create bar plot and save it to comet.ml.\"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "        plt.bar(range(len(self.means)),\n",
    "                [np.mean(v) for v in self.means.values()],\n",
    "                yerr=[np.mean(v) for v in self.stds.values()],\n",
    "                align='edge', alpha=.7)\n",
    "        plt.xticks(range(len(self.means)),\n",
    "                   labels=[''.join(k.split('.')[:-1])\n",
    "                           for k in self.means.keys()],\n",
    "                   rotation=60)\n",
    "        plt.tight_layout()\n",
    "        self.exp.log_figure('grad_avg', fig)\n",
    "        super().on_train_end(trainer, epoch, val_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class EC2Closer(TorchCallback):\n",
    "\n",
    "    def __init__(self, timeout=5, order=100):\n",
    "        self.timeout = timeout\n",
    "        self.order = order\n",
    "\n",
    "    def on_train_end(self, trainer, *args, **kwargs):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=self.timeout).json()\n",
    "        except requests.ReadTimeout as e:\n",
    "            trainer.logger.info('Request timed out. Failed to '\n",
    "                                'shutdown instance.')\n",
    "            return\n",
    "\n",
    "        id_, region = r['instanceId'], r['region']\n",
    "        ec2 = boto3.client('ec2', region_name=region)\n",
    "        ec2.stop_instances(InstanceIds=[id_], DryRun=debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class ModelUnfreezer(TorchCallback):\n",
    "    \"\"\"Gradually unfreeze a model during training.\n",
    "    \"\"\"\n",
    "\n",
    "    @valuecheck\n",
    "    def __init__(self, i2n, unfreeze_type:('groups', 'layers')='groups',\n",
    "                 mode:('batch', 'epoch')='epoch', order=25):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        i2n: dict\n",
    "            Maps index of batch/epoch to the number of layers or groups\n",
    "            to unfreeze at that point in time. Batches and epochs are\n",
    "            both zero-indexed. Note that batch refers to the global\n",
    "            batch number (e.g. if there are 100 batches per epoch, the\n",
    "            first batch of the second epoch is batch #101.)\n",
    "        unfreeze_type: str\n",
    "            Specifies whether to unfreeze groups or layers.\n",
    "        mode: str\n",
    "            Specifies whether the indices in `i2n` refer to batches or\n",
    "            epochs.\n",
    "        order: int\n",
    "            Determine place in the callback queue. Smaller numbers are\n",
    "            executed earlier.\n",
    "\n",
    "        Examples\n",
    "        --------\n",
    "        This will create a callback that unfreezes the last 2 layer\n",
    "        groups at epoch 2, the last 3 groups at epoch 10, and the\n",
    "        last 4 groups at epoch 25.\n",
    "\n",
    "        ModelUnfreezer(\n",
    "            i2n={2: 2, 10: 3, 25: 4},\n",
    "            unfreeze_type='groups',\n",
    "            mode='epoch'\n",
    "        )\n",
    "        \"\"\"\n",
    "        self.order = order\n",
    "        self.i2kwargs = {i: {f'n_{unfreeze_type}': n}\n",
    "                         for i, n in i2n.items()}\n",
    "        self.mode = mode\n",
    "\n",
    "    def on_batch_begin(self, trainer, i, sum_i):\n",
    "        if self.mode != 'batch': \n",
    "            return\n",
    "        kwargs = self.i2kwargs.get(sum_i, None)\n",
    "        if kwargs: \n",
    "            trainer.unfreeze(**kwargs, msg_pre=f'Global batch {sum_i}: ')\n",
    "\n",
    "    def on_epoch_begin(self, trainer, epoch, val_stats):\n",
    "        if self.mode != 'epoch': \n",
    "            return\n",
    "        kwargs = self.i2kwargs.get(epoch, None)\n",
    "        if kwargs: \n",
    "            trainer.unfreeze(**kwargs, msg_pre=f'Epoch {epoch}: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class SchedulerMixin(TorchCallback):\n",
    "\n",
    "    verbose = False\n",
    "\n",
    "    def on_train_end(self, trainer, *args, **kwargs):\n",
    "        self.plot_lrs(os.path.join(trainer.out_dir, 'lrs.png'))\n",
    "\n",
    "    def update_lr(self, trainer, n):\n",
    "        try:\n",
    "            lr = self.lrs[n]\n",
    "        except IndexError as e:\n",
    "            return\n",
    "\n",
    "        update_optimizer(trainer.optim, lr, lr_mult=self.lr_mult)\n",
    "        if self.verbose:\n",
    "            trainer.logger.info(f'Set learning rate to {lr:.4f}.')\n",
    "\n",
    "    def plot_lrs(self, path=None):\n",
    "        \"\"\"Display learning rate by iteration.\n",
    "\n",
    "        Note: If the plot is not as smooth as expected, this likely\n",
    "        means that there are very few iterations per epoch\n",
    "        (i.e. the batch size is very large, at least in relative terms).\n",
    "        \"\"\"\n",
    "        plt.plot(self.lrs)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        if path:\n",
    "            plt.savefig(path)\n",
    "            plt.close()\n",
    "        else:\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class CosineLRScheduler(SchedulerMixin):\n",
    "    \"\"\"Learning rate scheduler that makes updates each batch.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, warm=0.3, restarts=False, cycle_len=5, cycle_decay=0.0,\n",
    "                 min_lr=None, verbose=False, order=10):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        warm: float\n",
    "            Percent of training run (or cycle length) devoted to the increasing\n",
    "            portion of the schedule. Default 0.3.\n",
    "        restarts: bool\n",
    "            Specifies whether to use restarts, i.e. use a cyclical LR.\n",
    "            True: Version of cosine annealing with restarts. In one\n",
    "                  cycle, LR starts high and gradually decreases.\n",
    "                  At the start of the next cycle, it is\n",
    "                  immediately increased again.\n",
    "            False: Version of cosine annealing where LR increases\n",
    "                   for first 30% of training, then decreases for\n",
    "                   remaining 70%.\n",
    "        cycle_len: int\n",
    "            Number of epochs contained in a single cycle. Only used\n",
    "            when scheduler uses restarts.\n",
    "        cycle_decay: float\n",
    "            Scalar to decay the learning rate at the end of each cycle.\n",
    "            This is only used with restarts, since the regular cosine\n",
    "            annealing already decays the LR over time.\n",
    "            E.g. 1.0 will use no decay.\n",
    "            0.9 means that cycle 2 LRs = cycle 1 LRs * 0.9,\n",
    "            cycle 3 LRs = cycle 1 LRs * .81,\n",
    "            etc.\n",
    "        min_lr: float\n",
    "            Minimum learning rate. If None is specified, it will be set\n",
    "            to max_lr / 10.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.warm = warm\n",
    "        self.cycle_len = cycle_len\n",
    "        self.cycle_decay = cycle_decay\n",
    "        self.restarts = restarts\n",
    "        self.verbose = verbose\n",
    "        self.min_lr = min_lr\n",
    "        self.order = order\n",
    "\n",
    "        # Set in `on_train_begin()`.\n",
    "        self.lrs = None             # Iterable[float]\n",
    "        self.batches_per_e = None   # int\n",
    "        self.batches = None         # int\n",
    "        self.max_lr = None          # float\n",
    "        self.lr_mult = None         # float\n",
    "\n",
    "    def on_train_begin(self, trainer, epochs, lrs, lr_mult, **kwargs):\n",
    "        \"\"\"Wrapper to schedule learning rates depending on chosen method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        restarts: bool\n",
    "            If True, use schedule with restarts. If False, use regular\n",
    "            cosine annealing that spans whole duration of training.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array: LR for each iteration (i.e. output[i] is the LR to use\n",
    "            at iteration i).\n",
    "        \"\"\"\n",
    "        self.batches_per_e = len(trainer._dl_train_curr)\n",
    "        self.batches = epochs * self.batches_per_e\n",
    "        self.max_lr = max(lrs) if isinstance(lrs, Iterable) else lrs\n",
    "        self.lr_mult = lr_mult\n",
    "        if not self.min_lr: self.min_lr = self.max_lr / 10\n",
    "\n",
    "        if self.restarts and self.batches < self.cycle_len:\n",
    "            warnings.warn('Training will be less than 1 full cycle.')\n",
    "\n",
    "        if self.restarts:\n",
    "            self.lrs = self._cosine_restarts_schedule()\n",
    "        else:\n",
    "            self.lrs = self._cosine_schedule()\n",
    "\n",
    "    def on_batch_begin(self, trainer, i, sum_i):\n",
    "        self.update_lr(trainer, sum_i)\n",
    "\n",
    "    @staticmethod\n",
    "    def _cosine_anneal(batches, lr1, lr2):\n",
    "        \"\"\"Helper function for _cosine_schedule().\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        batches: int\n",
    "            Number of batches in segment.\n",
    "        lr1: float\n",
    "            Learning rate at start of segment.\n",
    "        lr2: float\n",
    "            Learning rate at end of segment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "        \"\"\"\n",
    "        i = np.arange(batches)\n",
    "        return lr2 + (lr1 - lr2)*(1 + np.cos(np.pi * i/batches))/2\n",
    "\n",
    "    def _cosine_schedule(self):\n",
    "        \"\"\"Cosine annealing scheduler. Computes learning rates for each\n",
    "        iteration.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array\n",
    "        \"\"\"\n",
    "        seg1 = self._cosine_anneal(int(self.warm * self.batches),\n",
    "                                   self.min_lr, self.max_lr)\n",
    "        seg2 = self._cosine_anneal(int(np.ceil((1 - self.warm) * self.batches)),\n",
    "                                   self.max_lr, self.min_lr)\n",
    "        return np.concatenate((seg1, seg2))\n",
    "\n",
    "    def _cosine_restarts_schedule(self):\n",
    "        \"\"\"Cosine annealing with restarts.\"\"\"\n",
    "        cycles = int(np.ceil(self.batches / (self.cycle_len * self.batches_per_e)))\n",
    "        cycle_batches = self.cycle_len * self.batches_per_e\n",
    "        lrs = [self._cosine_anneal(cycle_batches, self.max_lr, self.min_lr)\n",
    "               / (1 + self.cycle_decay * i) for i in range(cycles)]\n",
    "        return np.concatenate(lrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class AdaptiveSawtoothScheduler(SchedulerMixin):\n",
    "    \"\"\"Learning rate scheduler inspired by the sawtooth pattern often\n",
    "    used to manage TCP flow \n",
    "    (ex: https://witestlab.poly.edu/blog/tcp-congestion-control-basics/).\n",
    "    This uses a strategy called \"additive increase, multiplicative decrease\".\n",
    "    Basically, while the training loss is generally decreasing, we \n",
    "    gradually increase the learning rate. When things show signs of getting\n",
    "    worse, we dramatically decrease the LR and begin slowly climbing again.\n",
    "    The result looks something like a cyclical policy with restarts, \n",
    "    except that in this case the cycle lengths are dependent on training\n",
    "    rather than pre-defined. SGD w/ restarts typically also uses a sharp\n",
    "    increase and a gradual decrease, while this is closer to the opposite.\n",
    "    \n",
    "    Unlike the standard AIMD algorithm, we decay the amount added if the\n",
    "    batch loss increases, even if we're still within the patience window.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, add=1e-4, scale=0.6, patience=5, order=10):\n",
    "        \"\"\"Note: further experimentation is required to determine \n",
    "        sensible defaults for these hyperparameters.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        \n",
    "        \"\"\"\n",
    "        self.add = add\n",
    "        self.scale = scale\n",
    "        self.patience = patience\n",
    "        self.order = order\n",
    "        \n",
    "        # These are reset in `on_train_begin`, but types remain the same.\n",
    "        self.lrs = []\n",
    "        self.since_improve = 0\n",
    "        self.recent_best = float('inf')\n",
    "        self.lr_mult = 1.0\n",
    "\n",
    "    def on_train_begin(self, trainer, epochs, lrs, lr_mult, **kwargs):\n",
    "        \"\"\"Wrapper to schedule learning rates depending on chosen method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        restarts: bool\n",
    "            If True, use schedule with restarts. If False, use regular\n",
    "            cosine annealing that spans whole duration of training.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.array: LR for each iteration (i.e. output[i] is the LR to use\n",
    "            at iteration i).\n",
    "        \"\"\"\n",
    "        self.lrs.clear()\n",
    "        self.since_improve = 0\n",
    "        self.recent_best = float('inf')\n",
    "        self.lr_mult = lr_mult\n",
    "\n",
    "    def on_batch_begin(self, trainer, i, sum_i):\n",
    "        \"\"\"Update LR at the start of every batch.\"\"\"\n",
    "        try:\n",
    "            loss = trainer.stats.get('loss')[-1]\n",
    "        except:\n",
    "            return\n",
    "        \n",
    "        lr = max(p['lr'] for p in trainer.optim.param_groups)\n",
    "        if loss <= self.recent_best:\n",
    "            self.recent_best = loss\n",
    "            self.since_improve = 0\n",
    "            lr += self.add\n",
    "        elif loss > self.recent_best and self.since_improve < self.patience:\n",
    "            self.since_improve += 1\n",
    "            lr += self.add / (self.since_improve+1)\n",
    "        else:\n",
    "            self.since_improve += 1\n",
    "            lr *= self.scale\n",
    "        update_optimizer(trainer.optim, lr, lr_mult=self.lr_mult)\n",
    "        self.lrs.append(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sample training run with an adaptive sawtooth scheduler.\n",
    "<img src=\"adaptive_sawtooth_lrs.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
