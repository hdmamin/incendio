{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Incendio\n",
    "\n",
    "> The basics for building and training models are contained in this module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BasicConfig' from 'incendio.callbacks' (/Users/harrisonmamin/incendio/incendio/callbacks.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-eee8550a7c52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoggerMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaluecheck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhasarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mincendio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasicConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStatsHandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetricPrinter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mincendio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mincendio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquick_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incendio/incendio/callbacks.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0maccio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms3tool\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mS3tool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mauto_repr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaluecheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/incendio/incendio/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhtools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLoggerMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaluecheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBasicConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStatsHandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMetricPrinter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquick_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'BasicConfig' from 'incendio.callbacks' (/Users/harrisonmamin/incendio/incendio/callbacks.py)"
     ]
    }
   ],
   "source": [
    "# export\n",
    "from collections import defaultdict\n",
    "from collections.abc import Iterable\n",
    "from functools import partial\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from htools import load, save, LoggerMixin, valuecheck, hasarg\n",
    "from incendio.callbacks import BasicConfig, StatsHandler, MetricPrinter\n",
    "from incendio.metrics import batch_size\n",
    "from incendio.utils import quick_stats, DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used in notebook but not needed in package.\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from htools import assert_raises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class Trainer(LoggerMixin):\n",
    "    \n",
    "    @valuecheck\n",
    "    def __init__(self, net, ds_train, ds_val, dl_train, dl_val,\n",
    "                 criterion, mode:('binary', 'multiclass', 'regression'),\n",
    "                 out_dir, bucket=None, optim_type=Adam, eps=1e-3, \n",
    "                 last_act=None, threshold=0.5, metrics=None, callbacks=None,\n",
    "                 device=DEVICE):\n",
    "        \"\"\"An object to handle model training. This makes it easy for us to\n",
    "        save model weights, optimizer state, datasets and dataloaders all\n",
    "        at once.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        net: BaseModel (inherits from nn.Module)\n",
    "            A pytorch model. The BaseModel implementation from this library\n",
    "            should be used, since Trainer relies on its `unfreeze` method.\n",
    "        ds_train: torch.utils.data.Dataset\n",
    "            Training dataset.\n",
    "        ds_val: torch.utils.data.Dataset\n",
    "            Validation dataset.\n",
    "        dl_train: torch.utils.data.DataLoader\n",
    "            Training dataloader. Lazily retrieves items from train dataset.\n",
    "        dl_val: torch.utils.data.DataLoader\n",
    "            Validation dataloader. Lazily retrieves items from val dataset.\n",
    "        criterion: callable\n",
    "            Typically a PyTorch loss function, but you could define your\n",
    "            own as long as it accepts the same arguments in the same order.\n",
    "            This can be a function (e.g. F.cross_entropy) or a callable\n",
    "            object (e.g. nn.CrossEntropyLoss(). Notice that this is the\n",
    "            object, not the class.)\n",
    "        mode: str\n",
    "            Specifies the problem type. Multi-label classification is \n",
    "            considered 'binary' as well since each example receives a binary\n",
    "            prediction for each class.\n",
    "        out_dir: str\n",
    "            The path to an output directory where logs, model weights, and\n",
    "            more will be stored. If it doesn't already exist, it will be \n",
    "            created.\n",
    "        bucket: str\n",
    "            S3 bucket to store data. This can be used with the S3Upload\n",
    "            callback to automatically upload all files to S3 when training\n",
    "            completes.\n",
    "        optim_type: torch.optim callable\n",
    "            Callable optimizer. The default is Adam. Notice that this is the \n",
    "            class, not the object.\n",
    "        eps: float\n",
    "            The value of epsilon that will be passed to our optimizer. \n",
    "            We use a larger value than PyTorch's default, which empirically\n",
    "            can cause exploding gradients.\n",
    "        last_act: callable or None\n",
    "            Last activation function to be applied outside the model. \n",
    "            For example, for a binary classification problem, if we choose\n",
    "            to use binary_cross_entropy_with_logits loss but want to compute\n",
    "            some metric using soft predictions, we would pass in torch.sigmoid\n",
    "            for `last_act`. For a multi-class problem using F.cross_entropy \n",
    "            loss, we would need to pass in F.softmax to compute predicted \n",
    "            probabilities.  Remember this is ONLY necessary if all of the \n",
    "            following conditions are met:\n",
    "            1. It is a classification problem.\n",
    "            2. We have excluded the final activation from our model for \n",
    "            numerical stability reasons. (I.E. the loss function has the \n",
    "            the final activation built into it.)\n",
    "            3. We wish to compute 1 or more metrics based on soft predictions,\n",
    "            such as AUC-ROC.\n",
    "        threshold: float or None\n",
    "            For a classification problem, pass in the decision threshold to\n",
    "            use when converting soft predictions to hard predictions. For a\n",
    "            regression problem, pass in None.\n",
    "        metrics: list\n",
    "            A list of callable metrics. These will be computed on both the\n",
    "            train and validation sets during training. To maintain \n",
    "            compatibility with scikit-learn metrics, they should accept \n",
    "            two arguments: y_true, followed by either y_score (for soft\n",
    "            predictions) or y_pred (for hard predictions). The name and\n",
    "            order of these arguments matters. If other arguments are\n",
    "            required, pass in a partial with those values specified.\n",
    "        callbacks: list[TorchCallback]\n",
    "            List of callbacks. These will be evaluated during model training\n",
    "            and can be used to track stats, adjust learning rates, clip\n",
    "            gradients, etc.\n",
    "        device: torch.device\n",
    "            Trainer will place the model and current batch of data on this\n",
    "            device during training. The default value uses a GPU if one is\n",
    "            available, otherwise falls back to a CPU.\n",
    "            \n",
    "        Reference\n",
    "        ---------\n",
    "        Classification Loss Function (k = number of classes)\n",
    "\n",
    "        Loss                               y shape  yhat shape  dtype\n",
    "        --------------------------------------------------------------        \n",
    "        binary_cross_entropy_with_logits   (bs, 1)  (bs, 1)     float\n",
    "        \"\" (multilabel case)               (bs, k)  (bs, k)     float\n",
    "        cross_entropy                      (bs,)    (bs, k)     long\n",
    "        \"\"\"\n",
    "        self.net = net\n",
    "        self.ds_train, self.ds_val = ds_train, ds_val\n",
    "        self.dl_train, self.dl_val = dl_train, dl_val\n",
    "        # Optim created in fit() method. Must be after net is on the GPU.\n",
    "        self.optim_type = optim_type\n",
    "        self.optim = None\n",
    "        self.eps = eps\n",
    "        self.criterion = criterion\n",
    "        self.mode = mode\n",
    "        self.device = DEVICE\n",
    "        self.last_act = last_act\n",
    "        self.thresh = threshold\n",
    "        self._stop_training = False\n",
    "        # For now, only print logs. During training, a file will be created.\n",
    "        self.logger = self.get_logger()\n",
    "    \n",
    "        # Storage options.\n",
    "        self.out_dir = out_dir\n",
    "        self.bucket = bucket\n",
    "        os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "        # Dict makes it easier to adjust callbacks after creating model.\n",
    "        self.callbacks = {}\n",
    "        self.add_callbacks(*[BasicConfig(), StatsHandler(), MetricPrinter()] \n",
    "                           + (callbacks or []))\n",
    "        self.metrics = [batch_size] + (metrics or [])\n",
    "    \n",
    "    def save(self, fname):\n",
    "        \"\"\"Save a Trainer for later use. This includes the model, optimizer,\n",
    "        datasets, and data loaders.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        fname: str\n",
    "            File name to save to (not a full path - the trainer already has\n",
    "            an `out_dir` attribute which will be used). The extension must\n",
    "            be .pkl or .zip, and will determine whether the trainer is \n",
    "            compressed.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        save(self, os.path.join(self.out_dir, fname))\n",
    "        \n",
    "    def load(self, fname):\n",
    "        \"\"\"This lets a trainer load a previously saved state. This is NOT\n",
    "        an in-place operation: the new trainer is simply returned.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        fname: str\n",
    "            Name of file where Trainer object is stored. Must end in either\n",
    "            .zip or .pkl. Do not include the full path. This automatically\n",
    "            checks the output directory.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Trainer\n",
    "        \n",
    "        Examples\n",
    "        --------\n",
    "        trainer = Trainer(...)\n",
    "        trainer.fit(...)\n",
    "        trainer.save('v1')\n",
    "        trainer = trainer.load('v1')\n",
    "        \"\"\"\n",
    "        return load(self, os.path.join(self.out_dir, fname))\n",
    "    \n",
    "    @staticmethod\n",
    "    def from_file(path):\n",
    "        \"\"\"Ths lets us load a previously saved Trainer. Unlike load(), this\n",
    "        does not require us to have a Trainer instance first. The intent is\n",
    "        that load() works well when within a single Jupyter notebook session,\n",
    "        but when returning to work on a different day, we may not have a live\n",
    "        instance of Trainer and a staticmethod lets us load without\n",
    "        remembering all the arguments used at initialization time.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        path: str\n",
    "            Full path to saved file. This differs from load() because here, we\n",
    "            don't have an instance with an out_dir attribute to check.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        Trainer\n",
    "        \"\"\"\n",
    "        return load(path)\n",
    "    \n",
    "    def add_callbacks(self, *callbacks):\n",
    "        \"\"\"Attach additional callbacks to Trainer. Note that callback order \n",
    "        will be determined by their `priority` attribute, not insertion\n",
    "        order.\n",
    "            \n",
    "        Parameters\n",
    "        ----------\n",
    "        callbacks: TorchCallback\n",
    "            One or more callbacks to add. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.callbacks.update({type(cb).__name__: cb for cb in callbacks})\n",
    "        self.callbacks = dict(sorted(self.callbacks.items(),\n",
    "                                     key=lambda x: x[1].priority))\n",
    "    \n",
    "    def add_metrics(self, *metrics):\n",
    "        \"\"\"Add additional metrics to track. See the `metrics` parameter in \n",
    "        the __init__ docstring for more details.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        metrics: callable\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        self.metrics.extend(metrics)\n",
    "    \n",
    "    def fit(self, epochs, lrs=3e-3, lr_mult=1.0, **kwargs): \n",
    "        \"\"\"Train the model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        epochs: int\n",
    "            Number of epochs to train for.\n",
    "        lrs: float or Iterable(float)\n",
    "            Pass in one or more learning rates. If lr_mult < 1, these\n",
    "            will be the max LR(s). If the number of values matches the number\n",
    "            of layer groups in the model, they will be matched accordingly,\n",
    "            with the first layer is assigned the first LR. If 1 LR is passed\n",
    "            in and lr_mult < 1, the multiplier will be used to create an\n",
    "            appropriate number of LRs. Example: for a network with 3 groups,\n",
    "            lrs=3e-3 and lr_mult=0.1 will produce LRs of [3e-5, 3e-4, 3e-3].\n",
    "        lr_mult: float\n",
    "            Multiplier used to compute additional learning rates if needed.\n",
    "            See `update_optimizer()` for details.\n",
    "        kwargs: any\n",
    "            Pass in clean=True to remove existing files in out_dir.\n",
    "        \"\"\"\n",
    "        stats = defaultdict(list)\n",
    "        sum_i = 0\n",
    "        _ = self.decide_stop('on_train_begin', epochs, lrs, lr_mult, **kwargs)\n",
    "        for e in range(epochs):\n",
    "            _ = self.decide_stop('on_epoch_begin', e, stats, None)\n",
    "            for i, batch in enumerate(tqdm(self.dl_train)):\n",
    "                sum_i += 1\n",
    "                *xb, yb = map(lambda x: x.to(self.device), batch)\n",
    "                self.optim.zero_grad()\n",
    "                _ = self.decide_stop('on_batch_begin', i, sum_i, stats)\n",
    "                \n",
    "                # Forward and backward passes.\n",
    "                y_score = self.net(*xb)\n",
    "                loss = self.criterion(y_score, yb)\n",
    "                loss.backward()\n",
    "                self.optim.step()\n",
    "                \n",
    "                # Separate because callbacks are only applied during training.\n",
    "                self._update_stats(stats, loss, yb, y_score.detach())\n",
    "                if self.decide_stop('on_batch_end', i, sum_i, stats): break\n",
    "            \n",
    "            # If on_batch_end callback halts training, else block is skipped.  \n",
    "            else: \n",
    "                val_stats = self.validate()\n",
    "                if self.decide_stop('on_epoch_end', e, stats, val_stats): break\n",
    "                continue\n",
    "            break  \n",
    "        _ = self.decide_stop('on_train_end', e, stats, val_stats)\n",
    "    \n",
    "    def validate(self, dl_val=None):\n",
    "        \"\"\"Evaluate the model on a validation set.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        dl_val: torch.utils.data.DataLoader\n",
    "            Accepting an optional dataloader allows the user to pass in \n",
    "            different loaders after training for evaluation. If None is\n",
    "            passed in, self.dl_val is used.\n",
    "        \"\"\"\n",
    "        dl_val = self.dl_val or dl_val\n",
    "        val_stats = defaultdict(list)\n",
    "        self.net.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in dl_val:\n",
    "                *xb, yb = map(lambda x: x.to(self.device), batch)\n",
    "                y_score = self.net(*xb)\n",
    "                loss = self.criterion(y_score, yb)\n",
    "                self._update_stats(val_stats, loss, yb, y_score)\n",
    "        return val_stats\n",
    "        \n",
    "    def _update_stats(self, stats, loss, yb, y_score):\n",
    "        \"\"\"Update stats in place.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        stats: defaultdict[str, list]\n",
    "        loss: torch.Tensor\n",
    "            Tensor containing single value (mini-batch loss).\n",
    "        yb: torch.Tensor\n",
    "            Mini-batch of labels.\n",
    "        y_score: torch.Tensor\n",
    "            Mini-batch of raw predictions. In the case of \n",
    "            classification, these may still need to be passed\n",
    "            through a sigmoid or softmax.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        # Final activation often excluded from network architecture.\n",
    "        try:\n",
    "            y_score = self.last_act(y_score)\n",
    "        except TypeError:\n",
    "            pass\n",
    "        \n",
    "        # Convert soft predictions to hard predictions.\n",
    "        if self.mode == 'binary':\n",
    "            # In multi-label case, this will have shape (bs, k).\n",
    "            y_pred = (y_score > self.thresh).float()\n",
    "        elif self.mode == 'multiclass':\n",
    "            y_pred = y_score.argmax(-1)\n",
    "        elif self.mode == 'regression':\n",
    "            y_pred = y_score\n",
    "            \n",
    "        stats['loss'].append(loss.detach().cpu().numpy().item())\n",
    "        for m in self.metrics:\n",
    "            yhat = y_pred if hasarg(m, 'y_pred') else y_score\n",
    "            stats[m.__name__.replace('_score', '')].append(m(yb, yhat))\n",
    "        \n",
    "    def decide_stop(self, attr, *args, **kwargs):\n",
    "        \"\"\"Evaluates each of the trainer's callbacks. If any callback\n",
    "        encounters a condition that signals that training should halt,\n",
    "        it will set the attribute trainer._stop_training to True.\n",
    "        This method returns that value. By design, all callbacks will\n",
    "        be called before stopping training.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        attr: str\n",
    "            Determines which method to call for each callback.\n",
    "            One of ('on_train_begin', 'on_train_end', 'on_batch_begin',\n",
    "            'on_batch_end', 'on_epoch_begin', 'on_epoch_end').\n",
    "        args, kwargs: any\n",
    "            Additional arguments to pass to the callbacks.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        bool: If True, halt training.\n",
    "        \"\"\"\n",
    "        self._stop_training = False\n",
    "        # Pass model object as first argument to callbacks.\n",
    "        for cb in self.callbacks.values():\n",
    "            getattr(cb, attr)(self, *args, **kwargs)\n",
    "        return self._stop_training\n",
    "    \n",
    "    def unfreeze(self, n_layers=None, n_groups=None, msg_pre=''):\n",
    "        \"\"\"Pass in either the number of layers or number of groups to \n",
    "        unfreeze. Unfreezing always starts at the end of the network and moves\n",
    "        backward (e.g. n_layers=1 will unfreeze the last 1 layer, or n_groups=2 \n",
    "        will unfreeze the last 2 groups.) Remember than weights and biases are \n",
    "        treated as separate layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_layers: int or None\n",
    "            Number of layers to unfreeze.\n",
    "        n_groups: int or None\n",
    "            Number of layer groups to unfreeze. For this to work, the model\n",
    "            must define an attribute `groups` containing the layer groups.\n",
    "            Each group can be a layer, a nn.Sequential object, or \n",
    "            nn.Module.\n",
    "        msg_pre: str\n",
    "            Optional: add a prefix to the logged message. For example,\n",
    "            this can be used to record the epoch that unfreezing occurred\n",
    "            during.\n",
    "        \"\"\"\n",
    "        mode = 'layers' if n_layers is not None else 'groups'\n",
    "        msg_pre += f'Unfreezing last {n_layers or n_groups} {mode}.'\n",
    "        self.logger.info(msg_pre)\n",
    "        self.net.unfreeze(n_layers, n_groups)\n",
    "            \n",
    "    def freeze(self):\n",
    "        \"\"\"Freeze whole network. Mostly used for testing.\"\"\"\n",
    "        self.logger.info('Freezing whole network.')\n",
    "        self.net.unfreeze(n_layers=0)\n",
    "    \n",
    "    def cleanup(self, sentinel=None, confirmed=False):\n",
    "        \"\"\"Delete output directory. An empty directory with the same name\n",
    "        will be created in its place.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        sentinel: None\n",
    "            Placeholder to force user to pass confirmed as keyword arg.\n",
    "        confirmed: bool\n",
    "            Placeholder variable. This is just intended to force the user\n",
    "            to confirm their desire to delete files before doing it. If\n",
    "            True, the directory will be deleted. (Technically, any truthy\n",
    "            value will work.)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        if not confirmed: \n",
    "            self.logger.info('Missing confirmation, cleanup skipped.')\n",
    "            return\n",
    "        self.logger.info('Removing files from output directory.')\n",
    "        shutil.rmtree(self.out_dir)\n",
    "        os.makedirs(self.out_dir)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        r = (f'Trainer(criterion={repr(self.criterion.__name__)}, '\n",
    "             f'out_dir={repr(self.out_dir)}, bucket={repr(self.bucket)})'\n",
    "             f'\\n\\nDatasets: {len(self.ds_train)} train rows, '\n",
    "             f'{len(self.ds_val)} val rows'\n",
    "             f'\\n\\nOptimizer: {repr(self.optim)}'\n",
    "             f'\\n\\n{repr(self.net)})')\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BaseModel` allows models to freeze/unfreeze layers and provides several methods for weight diagnostics. It should not be instantiated directly, but used as a parent class for a model. Like all PyTorch models, its children will still need to call `super().__init__()` and implement a `forward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class BaseModel(nn.Module):\n",
    "    \n",
    "    def unfreeze(self, n_layers=None, n_groups=None):\n",
    "        \"\"\"Pass in either the number of layers or number of groups to \n",
    "        unfreeze. Unfreezing always starts at the end of the network and moves\n",
    "        backward (e.g. n_layers=1 will unfreeze the last 1 layer, or n_groups=2 \n",
    "        will unfreeze the last 2 groups.) Remember than weights and biases are \n",
    "        treated as separate layers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_layers: int or None\n",
    "            Number of layers to unfreeze.\n",
    "        n_groups: int or None\n",
    "            Number of layer groups to unfreeze. For this to work, the model\n",
    "            must define an attribute `groups` containing the layer groups.\n",
    "            Each group can be a layer, a nn.Sequential object, or \n",
    "            nn.Module.\n",
    "        \"\"\"\n",
    "        if n_groups is not None: \n",
    "            self._unfreeze_by_group(n_groups)\n",
    "            return\n",
    "\n",
    "        length = len(self)\n",
    "        for i, p in enumerate(self.parameters()):\n",
    "            p.requires_grad = i >= length - n_layers\n",
    "            \n",
    "    def freeze(self):\n",
    "        \"\"\"Freeze whole network. Mostly used for testing.\"\"\"\n",
    "        self.unfreeze(n_layers=0)\n",
    "        \n",
    "    def _unfreeze_by_group(self, n_groups):\n",
    "        \"\"\"Helper for unfreeze() method.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_groups: int\n",
    "            Number of groups to unfreeze, starting at the end of the network.\n",
    "        \"\"\"\n",
    "        length = len(self.groups)\n",
    "        for i, group in enumerate(self.groups):\n",
    "            setting = i >= length - n_groups\n",
    "            for p in group.parameters():\n",
    "                p.requires_grad = setting\n",
    "                \n",
    "    def __len__(self):\n",
    "        \"\"\"Number of parameter matrices in model (basically number of layers, \n",
    "        except that biases are counted separately).\n",
    "        \"\"\"\n",
    "        return sum(1 for p in self.parameters())\n",
    "    \n",
    "    def dims(self):\n",
    "        \"\"\"Get shape of each layer's weights.\"\"\"\n",
    "        return [tuple(p.shape) for p in self.parameters()]\n",
    "\n",
    "    def trainable(self):\n",
    "        \"\"\"Check which layers are trainable.\"\"\"\n",
    "        return [(tuple(p.shape), p.requires_grad) for p in self.parameters()]\n",
    "\n",
    "    def weight_stats(self):\n",
    "        \"\"\"Check mean and standard deviation of each layer's weights.\"\"\"\n",
    "        return [quick_stats(p.data, 3) for p in self.parameters()]\n",
    "\n",
    "    def plot_weights(self):\n",
    "        \"\"\"Plot histograms of each layer's weights.\"\"\"\n",
    "        n_layers = len(self.dims())\n",
    "        fig, ax = plt.subplots(n_layers, figsize=(8, n_layers * 1.25))\n",
    "        if not isinstance(ax, Iterable): ax = [ax]\n",
    "        for i, p in enumerate(self.parameters()):\n",
    "            ax[i].hist(p.data.flatten())\n",
    "            ax[i].set_title(\n",
    "                f'Shape: {tuple(p.shape)} Stats: {quick_stats(p.data)}'\n",
    "            )\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(BaseModel):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()  \n",
    "        self.fc1 = nn.Linear(dim, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        return self.fc2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupedModel(BaseModel):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        super().__init__()  \n",
    "        g1 = nn.Sequential(\n",
    "             nn.Linear(dim, 8),\n",
    "             nn.LeakyReLU(),\n",
    "             nn.Linear(8, 4),\n",
    "             nn.LeakyReLU()\n",
    "        )\n",
    "        g2 = nn.Linear(4, 1)\n",
    "        self.groups = nn.ModuleList([g1, g2])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for group in self.groups:\n",
    "            x = group(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfrozen [False, False, False, False]\n",
      "Unfrozen [False, False, False, True]\n",
      "Unfrozen [False, False, True, True]\n",
      "Unfrozen [False, True, True, True]\n",
      "Unfrozen [True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "snet = SimpleModel(2)\n",
    "snet.freeze()\n",
    "for n in range(5):\n",
    "    snet.unfreeze(n_layers=n)\n",
    "    unfrozen = [x[1] for x in snet.trainable()]\n",
    "    print('Unfrozen', unfrozen)\n",
    "    assert sum(unfrozen) == n\n",
    "    assert not any(unfrozen[:-n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected, got AttributeError('SimpleModel' object has no attribute 'groups').\n"
     ]
    }
   ],
   "source": [
    "snet.freeze()\n",
    "with assert_raises(AttributeError) as ar:\n",
    "    for n in range(3):\n",
    "        snet.unfreeze(n_groups=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfrozen [False, False, False, False, False, False]\n",
      "Unfrozen [False, False, False, False, True, True]\n",
      "Unfrozen [True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "gnet = GroupedModel(DIM)\n",
    "gnet.freeze()\n",
    "n_unfrozen = [0, 2, 6]\n",
    "for n, nu in zip(range(3), n_unfrozen):\n",
    "    gnet.unfreeze(n_groups=n)\n",
    "    unfrozen = [x[1] for x in gnet.trainable()]\n",
    "    print('Unfrozen', unfrozen)\n",
    "    assert sum(unfrozen) == nu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfrozen [False, False, False, False, False, False]\n",
      "Unfrozen [False, False, False, False, False, True]\n",
      "Unfrozen [False, False, False, False, True, True]\n",
      "Unfrozen [False, False, False, True, True, True]\n",
      "Unfrozen [False, False, True, True, True, True]\n",
      "Unfrozen [False, True, True, True, True, True]\n",
      "Unfrozen [True, True, True, True, True, True]\n"
     ]
    }
   ],
   "source": [
    "gnet.freeze()\n",
    "for n in range(7):\n",
    "    gnet.unfreeze(n_layers=n)\n",
    "    unfrozen = [x[1] for x in gnet.trainable()]\n",
    "    print('Unfrozen', unfrozen)\n",
    "    assert sum(unfrozen) == n\n",
    "    assert not any(unfrozen[:-n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers\n",
    "\n",
    "Optimizers like Adam or RMSProp can contain multiple \"parameter groups\", each with a different learning rate. (Other hyperparameters can vary as well, but we ignore that for now.) The functions below allow us to get a new optimizer or update an existing one. It allows us to easily use differential learning rate, but that is not required: it can also use the same LR for each parameter group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def variable_lr_optimizer(model, lr=3e-3, lr_mult=1.0, optimizer=Adam, \n",
    "                          eps=1e-3, **kwargs):\n",
    "    \"\"\"Get an optimizer that uses different learning rates for different layer\n",
    "    groups. Additional keyword arguments can be used to alter momentum and/or\n",
    "    weight decay, for example, but for the sake of simplicity these values\n",
    "    will be the same across layer groups.\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    model: nn.Module\n",
    "        A model object. If you intend to use differential learning rates,\n",
    "        the model must have an attribute `groups` containing a ModuleList of\n",
    "        layer groups in the form of Sequential objects. The number of layer\n",
    "        groups must match the number of learning rates passed in.\n",
    "    lr: float, Iterable[float]\n",
    "        A number of list of numbers containing the learning rates to use for\n",
    "        each layer group. There should generally be one LR for each layer group\n",
    "        in the model. If fewer LR's are provided, lr_mult will be used to\n",
    "        compute additional LRs. See `update_optimizer` for details.\n",
    "    lr_mult: float\n",
    "        If you pass in fewer LRs than layer groups, `lr_mult` will be used to\n",
    "        compute additional learning rates from the one that was passed in.\n",
    "    optimizer: torch optimizer\n",
    "        The Torch optimizer to be created (Adam by default).\n",
    "    eps: float\n",
    "        Hyperparameter used by optimizer. The default of 1e-8 can lead to\n",
    "        exploding gradients, so we typically override this.\n",
    "\n",
    "    Examples\n",
    "    ---------\n",
    "    optim = variable_lr_optimizer(model, lrs=[3e-3, 3e-2, 1e-1])\n",
    "    \"\"\"\n",
    "    groups = getattr(model, 'groups', [model])\n",
    "    # Placeholder LR used. We update this afterwards.\n",
    "    data = [{'params': group.parameters(), 'lr': 0} for group in groups]\n",
    "    optim = optimizer(data, eps=eps, **kwargs)\n",
    "    update_optimizer(optim, lr, lr_mult)\n",
    "    return optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def update_optimizer(optim, lrs, lr_mult=1.0):\n",
    "    \"\"\"Pass in 1 or more learning rates, 1 for each layer group, and update the\n",
    "    optimizer accordingly. The optimizer is updated in place so nothing is\n",
    "    returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    optim: torch.optim\n",
    "        Optimizer object.\n",
    "    lrs: float, Iterable[float]\n",
    "        One or more learning rates. If using multiple values, usually the\n",
    "        earlier values will be smaller and later values will be larger. This\n",
    "        can be achieved by passing in a list of LRs that is the same length as\n",
    "        the number of layer groups in the optimizer, or by passing in a single\n",
    "        LR and a value for lr_mult.\n",
    "    lr_mult: float\n",
    "        If you pass in fewer LRs than layer groups, `lr_mult` will be used to\n",
    "        compute additional learning rates from the one that was passed in.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    If optim has 3 layer groups, this will result in LRs of [3e-5, 3e-4, 3e-3]\n",
    "    in that order:\n",
    "    update_optimizer(optim, lrs=3e-3, lr_mult=0.1)\n",
    "\n",
    "    Again, optim has 3 layer groups. We leave the default lr_mult of 1.0 so\n",
    "    each LR will be 3e-3.\n",
    "    update_optimizer(optim, lrs=3e-3)\n",
    "\n",
    "    Again, optim has 3 layer groups. 3 LRs are passed in so lr_mult is unused.\n",
    "    update_optimizer(optim, lrs=[1e-3, 1e-3, 3e-3])\n",
    "    \"\"\"\n",
    "    if not isinstance(lrs, Iterable): lrs = [lrs]\n",
    "    n_missing = len(optim.param_groups) - len(lrs)\n",
    "\n",
    "    if n_missing < 0:\n",
    "        raise ValueError('Received more learning rates than layer groups.')\n",
    "    while n_missing > 0:\n",
    "        lrs.insert(0, lrs[0] * lr_mult)\n",
    "        n_missing -= 1\n",
    "\n",
    "    for group, lr in zip(optim.param_groups, lrs):\n",
    "        group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected, got ValueError(Received more learning rates than layer groups.).\n"
     ]
    }
   ],
   "source": [
    "optim = variable_lr_optimizer(snet, 2e-3)\n",
    "print(optim)\n",
    "\n",
    "with assert_raises(ValueError) as ar:\n",
    "    optim = variable_lr_optimizer(snet, [3e-3, 1e-1])\n",
    "    optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_optimizer(optim, 1e-3, 0.5)\n",
    "assert len(optim.param_groups) == 1\n",
    "assert optim.param_groups[0]['lr'] == 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 0.001\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 0.001\n",
      "    lr: 0.003\n",
      "    weight_decay: 0\n",
      ")\n",
      "[0.0006666666666666666, 0.002]\n"
     ]
    }
   ],
   "source": [
    "lrs = [1e-3, 3e-3]\n",
    "optim = variable_lr_optimizer(gnet, lrs)\n",
    "print(optim)\n",
    "assert [group['lr'] for group in optim.param_groups] == lrs\n",
    "\n",
    "update_optimizer(optim, 2e-3, lr_mult=1/3)\n",
    "print([group['lr'] for group in optim.param_groups])\n",
    "assert np.isclose(optim.param_groups[1]['lr'], optim.param_groups[0]['lr'] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0005, 0.001]\n"
     ]
    }
   ],
   "source": [
    "optim = variable_lr_optimizer(gnet, 1e-3, lr_mult=0.5)\n",
    "print([group['lr'] for group in optim.param_groups])\n",
    "assert np.isclose(optim.param_groups[1]['lr'], optim.param_groups[0]['lr'] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "adam = partial(torch.optim.Adam, eps=1e-3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
