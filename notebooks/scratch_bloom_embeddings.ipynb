{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import mmh3\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from htools import hdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<itertools.chain at 0x1215dc310>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.from_iterable(row.split(' ') for row in sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [\n",
    "    'I walked to the store so I hope it is not closed.',\n",
    "    'The theater is closed today and the sky is grey.',\n",
    "    'His dog is brown while hers is grey.'\n",
    "]\n",
    "labels = [0, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, just convert int to str and take hash. Another option that is meant for ints is Knuth's multiplicative method:\n",
    "\n",
    "hash(i) = i*2654435761 mod 2^32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_int(x, n_buckets, n_hashes=3):\n",
    "    assert isinstance(x, int), 'Input `x` must have type int.'\n",
    "    return [mmh3.hash(str(x), i, signed=False) % n_buckets for i in range(n_hashes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_int_tensor(x_2d, n_buckets, n_hashes=3):\n",
    "    return torch.tensor([[hash_int(x.item(), n_buckets, n_hashes) for x in row]\n",
    "                         for row in x_2d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2,  5,  6,  3,  7,  8,  2,  9, 10,  1])\n",
      "\n",
      "[2, 5, 6, 3, 7, 8, 2, 9, 10, 1]\n",
      "tensor([13, 14,  1, 15, 16, 17,  3, 18,  1,  4])\n",
      "\n",
      "[13, 14, 1, 15, 16, 17, 3, 18, 1, 4]\n"
     ]
    }
   ],
   "source": [
    "for row in x:\n",
    "    print(row, end='\\n\\n')\n",
    "    print([x.item() for x in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8,  2,  7],\n",
       "         [ 2,  8,  1],\n",
       "         [ 6,  6, 10],\n",
       "         [10,  5,  5],\n",
       "         [ 6,  9,  7],\n",
       "         [ 5,  9,  4],\n",
       "         [ 8,  2,  7],\n",
       "         [ 5, 10,  8],\n",
       "         [ 7,  8,  6],\n",
       "         [ 6, 10,  6]],\n",
       "\n",
       "        [[ 2,  5,  1],\n",
       "         [ 9,  8,  8],\n",
       "         [ 6, 10,  6],\n",
       "         [ 8,  1, 10],\n",
       "         [ 6,  3,  9],\n",
       "         [ 2,  6,  9],\n",
       "         [10,  5,  5],\n",
       "         [10,  5,  0],\n",
       "         [ 6, 10,  6],\n",
       "         [ 4,  8,  6]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_int_tensor(x, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[10,  9,  8],\n",
       "         [ 8,  1,  0],\n",
       "         [ 4,  4,  6],\n",
       "         [ 5, 12,  6],\n",
       "         [ 7,  3,  2],\n",
       "         [ 0,  4,  7],\n",
       "         [10,  9,  8],\n",
       "         [ 6,  2,  8],\n",
       "         [ 0,  1,  0],\n",
       "         [11,  9,  0]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_int_tensor(x[0, None], 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  5,  6,  3,  7,  8,  2,  9, 10,  1],\n",
       "        [13, 14,  1, 15, 16, 17,  3, 18,  1,  4]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 6, 3, 7, 8, 2, 9, 10, 1]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[n.item() for n in x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 2, 7],\n",
       " [2, 8, 1],\n",
       " [6, 6, 10],\n",
       " [10, 5, 5],\n",
       " [6, 9, 7],\n",
       " [5, 9, 4],\n",
       " [8, 2, 7],\n",
       " [5, 10, 8],\n",
       " [7, 8, 6],\n",
       " [6, 10, 6]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[hash_int(n.item(), 11) for n in x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 8, 1]\n",
      "[2, 6, 9]\n",
      "[8, 2, 7]\n",
      "[2, 5, 7]\n",
      "[10, 10, 2]\n",
      "[0, 10, 4]\n",
      "[3, 8, 4]\n",
      "[6, 5, 0]\n",
      "[6, 10, 8]\n",
      "[4, 4, 0]\n",
      "[7, 1, 0]\n",
      "[10, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 200, 17):\n",
    "    print(hash_int(i, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    \n",
    "    def __init__(self, sentences, labels, seq_len):\n",
    "        x = [s.split(' ') for s in sentences]\n",
    "        self.w2i = self.make_w2i(x)\n",
    "        self.seq_len = seq_len\n",
    "        self.x = self.encode(x)\n",
    "        self.y = torch.tensor(labels)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def make_w2i(self, tok_rows):\n",
    "        return {k: i for i, (k, v) in \n",
    "                enumerate(Counter(chain(*tok_rows)).most_common(), 1)}\n",
    "    \n",
    "    def encode(self, tok_rows):\n",
    "        enc = np.zeros((len(tok_rows), self.seq_len), dtype=int)\n",
    "        for i, row in enumerate(tok_rows):\n",
    "            trunc = [self.w2i.get(w, 0) for w in row[:self.seq_len]]\n",
    "            enc[i, :len(trunc)] = trunc\n",
    "        return torch.tensor(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([13, 14,  1, 15, 16, 17,  3, 18,  1,  4]), tensor(1))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Data(sents, labels, 10)\n",
    "ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2,  5,  6,  3,  7,  8,  2,  9, 10,  1],\n",
       "         [13, 14,  1, 15, 16, 17,  3, 18,  1,  4]]), tensor([0, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(ds, batch_size=2)\n",
    "x, y = next(iter(dl))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  5,  6,  3,  7,  8,  2,  9, 10,  1],\n",
       "        [13, 14,  1, 15, 16, 17,  3, 18,  1,  4],\n",
       "        [19, 20,  1, 21, 22, 23,  1,  4,  0,  0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 1,\n",
       " 'I': 2,\n",
       " 'the': 3,\n",
       " 'grey.': 4,\n",
       " 'walked': 5,\n",
       " 'to': 6,\n",
       " 'store': 7,\n",
       " 'so': 8,\n",
       " 'hope': 9,\n",
       " 'it': 10,\n",
       " 'not': 11,\n",
       " 'closed.': 12,\n",
       " 'The': 13,\n",
       " 'theater': 14,\n",
       " 'closed': 15,\n",
       " 'today': 16,\n",
       " 'and': 17,\n",
       " 'sky': 18,\n",
       " 'His': 19,\n",
       " 'dog': 20,\n",
       " 'brown': 21,\n",
       " 'while': 22,\n",
       " 'hers': 23}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.w2i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do:\n",
    "- maybe handle padding differently (i.e. just a row of zeros as usual. Should this happen in hash_int, hash_int_tensor, or BloomEmbedding?)\n",
    "- maybe use fastai embedding() instead of nn.Embedding? Check if the weight init method might help for this.\n",
    "- maybe use nn.embeddingbag?\n",
    "- experiment with different numbers of embeddings and hashes. Try to find guidelines for what reasonable choices are to prevent collisions. \n",
    "    - Eventually, maybe better to let user input vocab size and choose prob of collision, then automatically select values for n_emb and n_hashes. \n",
    "- check to make sure indices are working correctly after switching hash_int_tensor to take only 2d tensors. Also consider if this is the preferred way to do this.\n",
    "- should we let user choose between mean and sum? Wonder if mean would be better bc we could try different values of n_hashes while still loading pre-trained embeddings (bc scale is standardized)? But that probably doesn't work bc the hashes will be different anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BloomEmbedding(nn.Module):\n",
    "    \"\"\"Bloom Embedding layer for memory-efficient word representations.\n",
    "    Each word is encoded by a combination of rows of the embedding\n",
    "    matrix. The number of rows can therefore be far lower than the number\n",
    "    of words in our vocabulary while still providing unique representations.\n",
    "    \n",
    "    The reduction in rows allows us to use memory in other ways: \n",
    "    a larger embedding dimension, more or larger layers after the embedding,\n",
    "    or larger batch sizes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_emb=997, emb_dim=100, n_hashes=3):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_emb: int\n",
    "            Number of rows to create in the embedding matrix. A prime\n",
    "            number is recommended. Lower numbers will be more \n",
    "            memory-efficient but increase the chances of collisions.\n",
    "        emb_dim: int\n",
    "            Size of each embedding. If emb_dim=100, each word will\n",
    "            be represented by a 100-dimensional vector.\n",
    "        n_hashes: int\n",
    "            This determines the number of hashes that will be taken\n",
    "            for each word index, and as a result, the number of rows\n",
    "            that will be summed to create each unique representation.\n",
    "            The higher the number, the lower the chances of a collision.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_emb = n_emb\n",
    "        self.emb = nn.Embedding(n_emb, emb_dim)\n",
    "        self.n_hashes = n_hashes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.LongTensor\n",
    "            Input tensor of word indices. (bs x seq_len)\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.FloatTensor: Words encoded with combination of embeddings.\n",
    "            (bs x seq_len x emb_dim)\n",
    "        \"\"\"\n",
    "        # (bs, seq_len, n_hashes)\n",
    "        hashed = hash_int_tensor(x, self.n_emb, self.n_hashes)\n",
    "        # (bs, seq_len, n_hashes, emb_dim) -> (bs, seq_len, emb_dim)\n",
    "        return self.emb(hashed).sum(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.9593,  1.0518,  0.6034, -1.1845],\n",
       "        [ 2.6107,  0.8535,  0.1414,  2.5509],\n",
       "        [ 0.1232, -0.9368, -1.1105, -0.5421],\n",
       "        [-0.8812, -0.6248,  0.1107,  1.3015],\n",
       "        [-0.8906,  0.4898, -0.2640,  0.9300],\n",
       "        [ 0.0312, -0.0197, -0.4688,  0.6815],\n",
       "        [-1.2531, -1.1531,  0.2033,  0.0260],\n",
       "        [-0.7790, -0.6460, -1.5369, -1.4178],\n",
       "        [-0.2901,  0.2190, -1.0544, -0.3004],\n",
       "        [-1.7659,  0.6724, -0.5916, -1.2079],\n",
       "        [-1.1231,  0.2622, -1.1394, -0.7264]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be = BloomEmbedding(11, 4)\n",
    "be.emb.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  5,  6,  3,  7,  8,  2,  9, 10,  1],\n",
       "        [13, 14,  1, 15, 16, 17,  3, 18,  1,  4]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 2, 7]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_int(2, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 8, 1]\n",
      "[6, 10, 6]\n",
      "[8, 2, 7]\n",
      "[10, 5, 5]\n",
      "[4, 8, 6]\n",
      "[2, 8, 1]\n",
      "[6, 6, 10]\n",
      "[6, 9, 7]\n",
      "[5, 9, 4]\n",
      "[5, 10, 8]\n",
      "[7, 8, 6]\n",
      "[7, 9, 2]\n",
      "[1, 7, 3]\n",
      "[2, 5, 1]\n",
      "[9, 8, 8]\n",
      "[8, 1, 10]\n",
      "[6, 3, 9]\n",
      "[2, 6, 9]\n",
      "[10, 5, 0]\n",
      "[2, 10, 6]\n",
      "[1, 7, 8]\n",
      "[2, 7, 1]\n",
      "[9, 4, 8]\n",
      "[5, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "for i in range(24):\n",
    "    print(hash_int(i, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 2, 7]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_int(2, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = be(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9459, -1.3638, -3.7018, -2.2603],\n",
       "        [ 2.4437,  0.1357, -2.0235,  1.7084],\n",
       "        [-3.6293, -2.0441, -0.7328, -0.6743],\n",
       "        [-1.0606,  0.2228, -2.0770,  0.6366],\n",
       "        [-3.7980, -1.1267, -1.9251, -2.5997],\n",
       "        [-2.6253,  1.1425, -1.3244,  0.4036],\n",
       "        [-0.9459, -1.3638, -3.7018, -2.2603],\n",
       "        [-1.3820,  0.4614, -2.6626, -0.3453],\n",
       "        [-2.3222, -1.5802, -2.3880, -1.6921],\n",
       "        [-3.6293, -2.0441, -0.7328, -0.6743]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  5,  6,  3,  7,  8,  2,  9, 10,  1],\n",
       "        [13, 14,  1, 15, 16, 17,  3, 18,  1,  4]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.7651, -0.1030, -1.4379,  2.6902],\n",
       "         [-2.3462,  1.1103, -2.7004, -1.8087],\n",
       "         [-3.6293, -2.0441, -0.7328, -0.6743],\n",
       "         [ 1.1975,  1.3346, -2.0524,  1.5241],\n",
       "         [-3.9003, -1.1055, -0.2776,  0.1196],\n",
       "         [-2.8959, -1.4175, -1.4988, -1.7240],\n",
       "         [-1.0606,  0.2228, -2.0770,  0.6366],\n",
       "         [-0.1325,  1.2943, -1.0048, -1.2294],\n",
       "         [-3.6293, -2.0441, -0.7328, -0.6743],\n",
       "         [-2.4338, -0.4444, -1.1151,  0.6557]]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be([ds[1][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.2530, -1.8277, -2.0466, -1.2425],\n",
       "         [ 1.5416,  0.4265, -2.4499,  0.8327],\n",
       "         [-3.6293, -2.0441, -0.7328, -0.6743],\n",
       "         [ 1.9549, -0.7293, -2.5060,  0.5909],\n",
       "         [-2.9467,  1.3812, -1.9100, -0.5783],\n",
       "         [-1.7311, -1.2693, -0.2474,  3.2845],\n",
       "         [-3.6293, -2.0441, -0.7328, -0.6743],\n",
       "         [-2.4338, -0.4444, -1.1151,  0.6557],\n",
       "         [ 0.5547,  1.7449, -1.5046,  1.0425],\n",
       "         [ 0.5547,  1.7449, -1.5046,  1.0425]]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be([ds[2][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is 1 [[[-3.629335   -2.0440652  -0.73281133 -0.67432237]]]\n",
      "I 2 [[[-0.9459374 -1.3638294 -3.701799  -2.260304 ]]]\n",
      "the 3 [[[-1.0606176   0.22278604 -2.076974    0.6365739 ]]]\n",
      "grey. 4 [[[-2.4338439  -0.44435126 -1.1151254   0.6556832 ]]]\n",
      "walked 5 [[[ 2.4437425  0.1356715 -2.0235345  1.7083521]]]\n",
      "to 6 [[[-3.629335   -2.0440652  -0.73281133 -0.6743224 ]]]\n",
      "store 7 [[[-3.7980285 -1.1267047 -1.925149  -2.599691 ]]]\n",
      "so 8 [[[-2.6252909   1.1425201  -1.3243927   0.40356058]]]\n",
      "hope 9 [[[-1.3819773   0.46144292 -2.6625922  -0.34529456]]]\n",
      "it 10 [[[-2.322223  -1.5801504 -2.3879628 -1.6921202]]]\n",
      "not 11 [[[-2.421743  -0.9103837 -3.2389848 -3.1678748]]]\n",
      "closed. 12 [[[ 0.9505731 -0.4172793 -1.2848289  2.434585 ]]]\n",
      "The 13 [[[ 2.7651021  -0.10298538 -1.4379164   2.6902206 ]]]\n",
      "theater 14 [[[-2.3461998  1.1103309 -2.700363  -1.8086885]]]\n",
      "closed 15 [[[ 1.1975132  1.3346475 -2.0524316  1.5240704]]]\n",
      "today 16 [[[-3.900252   -1.105511   -0.27760884  0.11962378]]]\n",
      "and 17 [[[-2.8959124 -1.4175131 -1.4987998 -1.7240317]]]\n",
      "sky 18 [[[-0.13253772  1.2942821  -1.0048163  -1.2293912 ]]]\n",
      "His 19 [[[-2.2530494 -1.8277442 -2.046647  -1.2425064]]]\n",
      "dog 20 [[[ 1.5416262  0.42648   -2.4498835  0.832693 ]]]\n",
      "brown 21 [[[ 1.9549128  -0.72928154 -2.50602     0.5909294 ]]]\n",
      "while 22 [[[-2.9466505   1.381177   -1.9100108  -0.57830787]]]\n",
      "hers 23 [[[-1.7311409  -1.2692902  -0.24744955  3.2845347 ]]]\n"
     ]
    }
   ],
   "source": [
    "for w, i in ds.w2i.items():\n",
    "    print(w, i, be(torch.tensor([[i]])).detach().numpy())\n",
    "#           .emb.weight[hash_int(i, be.n_emb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[i]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 3]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_int(23, 11,  3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5, 3, 3]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashed = hash_int_tensor([[23]], 11, 3)\n",
    "hashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.8263,  0.3004, -0.2202, -0.6845],\n",
       "          [-0.1281,  1.5147,  0.0252,  0.3402],\n",
       "          [-0.1281,  1.5147,  0.0252,  0.3402]]]], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be.emb.weight[hashed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([1, 1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[4, 8, 6]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashed = hash_int_tensor([[4]], 11, 3)\n",
    "print('shape:', hashed.shape)\n",
    "hashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.7247,  0.6117,  1.5340,  1.1633],\n",
       "          [-1.0695,  0.5586,  0.8308, -1.2400],\n",
       "          [-0.4010,  1.0211,  0.6487,  1.4627]]]], grad_fn=<IndexBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be.emb.weight[hashed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 2, 7]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_int(2, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 4.5428e+00, -1.5926e+00, -1.8052e+00, -4.5093e-01],\n",
       "        [-1.2740e+00,  6.9280e-01, -1.6181e-01,  1.0787e+00],\n",
       "        [-5.7357e-01, -3.9490e-01, -2.5627e-01,  5.3629e-01],\n",
       "        [-4.6838e-02, -3.5406e-01,  6.5238e-01, -1.2767e+00],\n",
       "        [-3.7521e-01,  5.7965e-05, -2.1415e-01,  7.3730e-01],\n",
       "        [-1.0251e+00, -1.0557e+00,  1.0477e-01, -6.6462e-01],\n",
       "        [ 2.6104e-01, -1.6422e+00,  1.5243e+00, -4.5074e-01],\n",
       "        [ 1.7178e+00, -4.8310e-01,  3.5616e-01, -2.4185e+00],\n",
       "        [ 8.9364e-01, -2.7994e+00, -1.5495e+00, -1.0016e+00],\n",
       "        [-2.1734e-01, -6.0759e-01, -1.1346e+00, -6.1147e-01],\n",
       "        [-3.1615e-01, -9.3613e-01,  6.4561e-01,  1.6669e+00]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be.emb.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[8, 2, 7]]])\n",
      "\n",
      "tensor([[[[ 0.8936, -2.7994, -1.5495, -1.0016],\n",
      "          [-0.5736, -0.3949, -0.2563,  0.5363],\n",
      "          [ 1.7178, -0.4831,  0.3562, -2.4185]]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0378, -3.6774, -1.4496, -2.8839]]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be([[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9459, -1.3638, -3.7018, -2.2603],\n",
       "         [ 2.4437,  0.1357, -2.0235,  1.7084]]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be(torch.tensor([[2, 5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9459, -1.3638, -3.7018, -2.2603],\n",
       "         [ 2.4437,  0.1357, -2.0235,  1.7084],\n",
       "         [-3.6293, -2.0441, -0.7328, -0.6743],\n",
       "         [-1.0606,  0.2228, -2.0770,  0.6366],\n",
       "         [-3.7980, -1.1267, -1.9251, -2.5997],\n",
       "         [-2.6253,  1.1425, -1.3244,  0.4036],\n",
       "         [-0.9459, -1.3638, -3.7018, -2.2603],\n",
       "         [-1.3820,  0.4614, -2.6626, -0.3453],\n",
       "         [-2.3222, -1.5802, -2.3880, -1.6921],\n",
       "         [-3.6293, -2.0441, -0.7328, -0.6743]],\n",
       "\n",
       "        [[ 2.7651, -0.1030, -1.4379,  2.6902],\n",
       "         [-2.3462,  1.1103, -2.7004, -1.8087],\n",
       "         [-3.6293, -2.0441, -0.7328, -0.6743],\n",
       "         [ 1.1975,  1.3346, -2.0524,  1.5241],\n",
       "         [-3.9003, -1.1055, -0.2776,  0.1196],\n",
       "         [-2.8959, -1.4175, -1.4988, -1.7240],\n",
       "         [-1.0606,  0.2228, -2.0770,  0.6366],\n",
       "         [-0.1325,  1.2943, -1.0048, -1.2294],\n",
       "         [-3.6293, -2.0441, -0.7328, -0.6743],\n",
       "         [-2.4338, -0.4444, -1.1151,  0.6557]]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
