{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import abc, Counter\n",
    "from functools import lru_cache\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "import mmh3\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from htools import hdir, eprint, assert_raises, flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = [\n",
    "    'I walked to the store so I hope it is not closed.',\n",
    "    'The theater is closed today and the sky is grey.',\n",
    "    'His dog is brown while hers is grey.'\n",
    "]\n",
    "labels = [0, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    \n",
    "    def __init__(self, sentences, labels, seq_len):\n",
    "        x = [s.split(' ') for s in sentences]\n",
    "        self.w2i = self.make_w2i(x)\n",
    "        self.seq_len = seq_len\n",
    "        self.x = self.encode(x)\n",
    "        self.y = torch.tensor(labels)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def make_w2i(self, tok_rows):\n",
    "        return {k: i for i, (k, v) in \n",
    "                enumerate(Counter(chain(*tok_rows)).most_common(), 1)}\n",
    "    \n",
    "    def encode(self, tok_rows):\n",
    "        enc = np.zeros((len(tok_rows), self.seq_len), dtype=int)\n",
    "        for i, row in enumerate(tok_rows):\n",
    "            trunc = [self.w2i.get(w, 0) for w in row[:self.seq_len]]\n",
    "            enc[i, :len(trunc)] = trunc\n",
    "        return torch.tensor(enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([13, 14,  1, 15, 16, 17,  3, 18,  1,  4]), tensor(1))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = Data(sents, labels, 10)\n",
    "ds[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2,  5,  6,  3,  7,  8,  2,  9, 10,  1],\n",
       "         [13, 14,  1, 15, 16, 17,  3, 18,  1,  4],\n",
       "         [19, 20,  1, 21, 22, 23,  1,  4,  0,  0]]), tensor([0, 1, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = DataLoader(ds, batch_size=3)\n",
    "x, y = next(iter(dl))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  5,  6,  3,  7,  8,  2,  9, 10,  1],\n",
       "        [13, 14,  1, 15, 16, 17,  3, 18,  1,  4],\n",
       "        [19, 20,  1, 21, 22, 23,  1,  4,  0,  0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is': 1,\n",
       " 'I': 2,\n",
       " 'the': 3,\n",
       " 'grey.': 4,\n",
       " 'walked': 5,\n",
       " 'to': 6,\n",
       " 'store': 7,\n",
       " 'so': 8,\n",
       " 'hope': 9,\n",
       " 'it': 10,\n",
       " 'not': 11,\n",
       " 'closed.': 12,\n",
       " 'The': 13,\n",
       " 'theater': 14,\n",
       " 'closed': 15,\n",
       " 'today': 16,\n",
       " 'and': 17,\n",
       " 'sky': 18,\n",
       " 'His': 19,\n",
       " 'dog': 20,\n",
       " 'brown': 21,\n",
       " 'while': 22,\n",
       " 'hers': 23}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.w2i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, just convert int to str and take hash. Another option that is meant for ints is Knuth's multiplicative method:\n",
    "\n",
    "hash(i) = i*2654435761 mod 2^32\n",
    "\n",
    "But we'd need to make this dependent on a random seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilistic_hash_item(x, n_buckets, mode=int, n_hashes=3):\n",
    "    \"\"\"Slightly hacky way to probabilistically hash an integer by\n",
    "    first converting it to a string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: int\n",
    "        The integer or string to hash.\n",
    "    n_buckets: int\n",
    "        The number of buckets that items will be mapped to. Typically \n",
    "        this would occur outside the hashing function, but since \n",
    "        the intended use case is so narrow here it makes sense to me \n",
    "        to include it here.\n",
    "    mode: type\n",
    "        The type of input you want to hash. This is user-provided to prevent\n",
    "        accidents where we pass in a different item than intended and hash \n",
    "        the wrong thing. One of (int, str). When using this inside a\n",
    "        BloomEmbedding layer, this must be `int` because there are no\n",
    "        string tensors. When used inside a dataset or as a one-time\n",
    "        pre-processing step, you can choose either as long as you\n",
    "        pass in the appropriate inputs.\n",
    "    n_hashes: int\n",
    "        The number of times to hash x, each time with a different seed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list[int]: A list of integers with length `n_hashes`, where each integer\n",
    "        is in [0, n_buckets).\n",
    "    \"\"\"\n",
    "    # Check type to ensure we don't accidentally hash Tensor(5) instead of 5.\n",
    "    assert isinstance(x, mode), f'Input `x` must have type {mode}.'\n",
    "    return [mmh3.hash(str(x), i, signed=False) % n_buckets \n",
    "            for i in range(n_hashes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilistic_hash_tensor(x_r2, n_buckets, n_hashes=3, pad_idx=0):\n",
    "    \"\"\"Hash a rank 2 LongTensor.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_r2: torch.LongTensor\n",
    "        Rank 2 tensor of integers. Shape: (bs, seq_len)\n",
    "    n_buckets: int\n",
    "        Number of buckets to hash items into (i.e. the number of \n",
    "        rows in the embedding matrix). Typically a moderately large\n",
    "        prime number, like 251 or 997.\n",
    "    n_hashes: int\n",
    "        Number of hashes to take for each input index. This determines\n",
    "        the number of rows of the embedding matrix that will be summed\n",
    "        to get the representation for each word. Typically 2-5.\n",
    "    pad_idx: int or None\n",
    "        If you want to pad sequences with vectors of zeros, pass in an\n",
    "        integer (same as the `padding_idx` argument to nn.Embedding).\n",
    "        If None, no padding index will be used. The sequences must be\n",
    "        padded before passing them into this function.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.LongTensor: Tensor of indices where each row corresponds\n",
    "        to one of the input indices. Shape: (bs, seq_len, n_hashes)\n",
    "    \"\"\"\n",
    "    return torch.tensor(\n",
    "        [[probabilistic_hash_item(x.item(), n_buckets, int, n_hashes) \n",
    "          if x != pad_idx else [pad_idx]*n_hashes for x in row]\n",
    "         for row in x_r2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "tensor([ 2,  5,  6,  3,  7,  8,  2,  9, 10,  1])\n",
      "[2, 5, 6, 3, 7, 8, 2, 9, 10, 1]\n",
      "\n",
      "1\n",
      "tensor([13, 14,  1, 15, 16, 17,  3, 18,  1,  4])\n",
      "[13, 14, 1, 15, 16, 17, 3, 18, 1, 4]\n",
      "\n",
      "2\n",
      "tensor([19, 20,  1, 21, 22, 23,  1,  4,  0,  0])\n",
      "[19, 20, 1, 21, 22, 23, 1, 4, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, row in enumerate(x):\n",
    "    print(i)\n",
    "    print(row)\n",
    "    print([x.item() for x in row], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8,  2,  7],\n",
       "         [ 2,  8,  1],\n",
       "         [ 6,  6, 10],\n",
       "         [10,  5,  5],\n",
       "         [ 6,  9,  7],\n",
       "         [ 5,  9,  4],\n",
       "         [ 8,  2,  7],\n",
       "         [ 5, 10,  8],\n",
       "         [ 7,  8,  6],\n",
       "         [ 6, 10,  6]],\n",
       "\n",
       "        [[ 2,  5,  1],\n",
       "         [ 9,  8,  8],\n",
       "         [ 6, 10,  6],\n",
       "         [ 8,  1, 10],\n",
       "         [ 6,  3,  9],\n",
       "         [ 2,  6,  9],\n",
       "         [10,  5,  5],\n",
       "         [10,  5,  0],\n",
       "         [ 6, 10,  6],\n",
       "         [ 4,  8,  6]],\n",
       "\n",
       "        [[ 2, 10,  6],\n",
       "         [ 1,  7,  8],\n",
       "         [ 6, 10,  6],\n",
       "         [ 2,  7,  1],\n",
       "         [ 9,  4,  8],\n",
       "         [ 5,  3,  3],\n",
       "         [ 6, 10,  6],\n",
       "         [ 4,  8,  6],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilistic_hash_tensor(x, 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8,  2,  7],\n",
       "         [ 2,  8,  1],\n",
       "         [ 6,  6, 10],\n",
       "         [10,  5,  5],\n",
       "         [ 6,  9,  7],\n",
       "         [ 5,  9,  4],\n",
       "         [ 8,  2,  7],\n",
       "         [ 5, 10,  8],\n",
       "         [ 7,  8,  6],\n",
       "         [ 6, 10,  6]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilistic_hash_tensor(x[0, None], 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2, 10,  6],\n",
       "         [ 1,  7,  8],\n",
       "         [ 6, 10,  6],\n",
       "         [ 2,  7,  1],\n",
       "         [ 9,  4,  8],\n",
       "         [ 5,  3,  3],\n",
       "         [ 6, 10,  6],\n",
       "         [ 4,  8,  6],\n",
       "         [ 0,  0,  0],\n",
       "         [ 0,  0,  0]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilistic_hash_tensor(x[2, None], 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  5,  6,  3,  7,  8,  2,  9, 10,  1],\n",
       "        [13, 14,  1, 15, 16, 17,  3, 18,  1,  4],\n",
       "        [19, 20,  1, 21, 22, 23,  1,  4,  0,  0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[8, 2, 7],\n",
       " [2, 8, 1],\n",
       " [6, 6, 10],\n",
       " [10, 5, 5],\n",
       " [6, 9, 7],\n",
       " [5, 9, 4],\n",
       " [8, 2, 7],\n",
       " [5, 10, 8],\n",
       " [7, 8, 6],\n",
       " [6, 10, 6]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[probabilistic_hash_item(n.item(), 11) for n in x[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 8, 1]\n",
      "[2, 6, 9]\n",
      "[8, 2, 7]\n",
      "[2, 5, 7]\n",
      "[10, 10, 2]\n",
      "[0, 10, 4]\n",
      "[3, 8, 4]\n",
      "[6, 5, 0]\n",
      "[6, 10, 8]\n",
      "[4, 4, 0]\n",
      "[7, 1, 0]\n",
      "[10, 4, 6]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 200, 17):\n",
    "    print(probabilistic_hash_item(i, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: ('I', [0, 5, 5])\n",
      " 1: ('walked', [2, 5, 1])\n",
      " 2: ('to', [10, 4, 6])\n",
      " 3: ('the', [4, 1, 4])\n",
      " 4: ('store', [4, 6, 3])\n",
      " 5: ('so', [7, 8, 8])\n",
      " 6: ('I', [0, 5, 5])\n",
      " 7: ('hope', [1, 2, 7])\n",
      " 8: ('it', [3, 9, 0])\n",
      " 9: ('is', [3, 1, 3])\n",
      "10: ('not', [6, 10, 4])\n",
      "11: ('closed.', [3, 6, 10])\n",
      "\n",
      " 0: ('The', [1, 6, 9])\n",
      " 1: ('theater', [8, 10, 2])\n",
      " 2: ('is', [3, 1, 3])\n",
      " 3: ('closed', [5, 5, 0])\n",
      " 4: ('today', [3, 10, 8])\n",
      " 5: ('and', [7, 2, 4])\n",
      " 6: ('the', [4, 1, 4])\n",
      " 7: ('sky', [1, 2, 9])\n",
      " 8: ('is', [3, 1, 3])\n",
      " 9: ('grey.', [7, 6, 7])\n",
      "\n",
      " 0: ('His', [0, 10, 3])\n",
      " 1: ('dog', [8, 6, 6])\n",
      " 2: ('is', [3, 1, 3])\n",
      " 3: ('brown', [9, 8, 9])\n",
      " 4: ('while', [9, 2, 8])\n",
      " 5: ('hers', [0, 5, 4])\n",
      " 6: ('is', [3, 1, 3])\n",
      " 7: ('grey.', [7, 6, 7])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for row in [s.split(' ') for s in sents]:\n",
    "    eprint(list(zip(row, (probabilistic_hash_item(word, 11, str) for word in row))))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As expected, got AssertionError(torch tensor).\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(np.array(sents), (list, np.ndarray)), 'np array'\n",
    "assert isinstance(sents, (list, np.ndarray)), 'list'\n",
    "with assert_raises(AssertionError):\n",
    "    assert isinstance(x, (list, np.ndarray)), 'torch tensor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterable tensor True\n",
      "Iterable list True\n",
      "Iterable array True\n",
      "\n",
      "Collection tensor True\n",
      "Collection list True\n",
      "Collection array True\n",
      "\n",
      "Container tensor True\n",
      "Container list True\n",
      "Container array True\n",
      "\n",
      "Sequence tensor False\n",
      "Sequence list True\n",
      "Sequence array False\n",
      "\n",
      "MutableSequence tensor False\n",
      "MutableSequence list True\n",
      "MutableSequence array False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in (abc.Iterable, abc.Collection, abc.Container, abc.Sequence, abc.MutableSequence):\n",
    "    tname = t.__name__\n",
    "    print(tname, 'tensor', isinstance(x, t))\n",
    "    print(tname, 'list', isinstance(sents, t))\n",
    "    print(tname, 'array', isinstance(np.array(sents), t))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do:\n",
    "- [x] maybe handle padding differently (i.e. just a row of zeros as usual. Should this happen in hash_int, hash_int_tensor, or BloomEmbedding?)\n",
    "- maybe use fastai embedding() instead of nn.Embedding? Check if the weight init method might help for this.\n",
    "- maybe use nn.embeddingbag? (note: people report this is much slower on CPU, and possibly slower on GPU even though it's supposed to be faster)\n",
    "- experiment with different numbers of embeddings and hashes. Try to find guidelines for what reasonable choices are to prevent collisions. \n",
    "    - Eventually, maybe better to let user input vocab size and choose prob of collision, then automatically select values for n_emb and n_hashes. \n",
    "- check to make sure indices are working correctly after switching hash_int_tensor to take only 2d tensors. Also consider if this is the preferred way to do this.\n",
    "- should we let user choose between mean and sum? Wonder if mean would be better bc we could try different values of n_hashes while still loading pre-trained embeddings (bc scale is standardized)? But that probably doesn't work bc the hashes will be different anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.5849, -0.1108, -0.1991, -1.1982],\n",
       "        [-0.5891,  1.5808,  1.4062, -1.5162],\n",
       "        [-1.0055,  0.1712, -0.5717, -0.2766],\n",
       "        [ 1.6280, -0.1770, -1.4850, -2.0490],\n",
       "        [ 0.8724,  0.5324,  0.1913, -0.9653]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = nn.EmbeddingBag(5, 4, mode='sum')\n",
    "emb.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.1837,  3.3329,  2.2408, -3.3091],\n",
       "        [ 1.2073, -0.1165, -2.2557, -3.5238]], grad_fn=<EmbeddingBagBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.Tensor([[1, 2, 1],\n",
    "                  [3, 2, 0]]).long()\n",
    "emb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BloomEmbedding(nn.Module):\n",
    "    \"\"\"Bloom Embedding layer for memory-efficient word representations.\n",
    "    Each word is encoded by a combination of rows of the embedding\n",
    "    matrix. The number of rows can therefore be far lower than the number\n",
    "    of words in our vocabulary while still providing unique representations.\n",
    "    \n",
    "    The reduction in rows allows us to use memory in other ways: \n",
    "    a larger embedding dimension, more or larger layers after the embedding,\n",
    "    or larger batch sizes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_emb=251, emb_dim=100, n_hashes=4, padding_idx=0,\n",
    "                 pre_hashed=False):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_emb: int\n",
    "            Number of rows to create in the embedding matrix. A prime\n",
    "            number is recommended. Lower numbers will be more \n",
    "            memory-efficient but increase the chances of collisions.\n",
    "        emb_dim: int\n",
    "            Size of each embedding. If emb_dim=100, each word will\n",
    "            be represented by a 100-dimensional vector.\n",
    "        n_hashes: int\n",
    "            This determines the number of hashes that will be taken\n",
    "            for each word index, and as a result, the number of rows\n",
    "            that will be summed to create each unique representation.\n",
    "            The higher the number, the lower the chances of a collision.\n",
    "        padding_idx: int or None\n",
    "            If an integer is provided, this will set aside the corresponding\n",
    "            row in the embedding matrix as a vector of zeros. If None, no\n",
    "            padding vector will be allocated.\n",
    "        pre_hashed: bool\n",
    "            Pass in True if the input tensor will already be hashed by the time \n",
    "            it enters this layer (you may prefer pre-compute the hashes in the\n",
    "            Dataset to save computation time during training). In this\n",
    "            scenario, the layer is a simple embedding bag with mode \"sum\". \n",
    "            Pass in False if the inputs will be word indices that have not yet\n",
    "            been hashed. In this case, hashing will be done inside the `forward`\n",
    "            call.\n",
    "            \n",
    "        Suggested values for a vocab size of ~30,000:\n",
    "        \n",
    "        | n_emb | n_hashes | unique combos |\n",
    "        |-------|----------|---------------|\n",
    "        | 127   | 5        | 29,998        |\n",
    "        | 251   | 4        | 29,996        |\n",
    "        | 997   | 3        | 29,997        |\n",
    "        | 5,003 | 2        | 29,969        |\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_emb = n_emb\n",
    "        self.emb = nn.Embedding(n_emb, emb_dim, padding_idx=padding_idx)\n",
    "        self.n_hashes = n_hashes\n",
    "        self.pad_idx = padding_idx\n",
    "        self.pre_hashed = pre_hashed\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.LongTensor\n",
    "            Input tensor of word indices (bs x seq_len) if pre_hashed is False.\n",
    "            Hashed indices (bs x seq_len x n_hashes) if pre_hashed is False.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        torch.FloatTensor: Words encoded with combination of embeddings.\n",
    "            (bs x seq_len x emb_dim)\n",
    "        \"\"\"\n",
    "        if not self.pre_hashed:\n",
    "            # (bs, seq_len) -> hash -> (bs, seq_len, n_hashes)\n",
    "            hashed = probabilistic_hash_tensor(x, \n",
    "                                               self.n_emb,\n",
    "                                               self.n_hashes,\n",
    "                                               self.pad_idx)\n",
    "        # (bs, seq_len, n_hashes, emb_dim) -> sum -> (bs, seq_len, emb_dim)\n",
    "        return self.emb(hashed).sum(-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 2,  5,  6,  3,  7,  8,  2,  9, 10,  1],\n",
       "         [13, 14,  1, 15, 16, 17,  3, 18,  1,  4],\n",
       "         [19, 20,  1, 21, 22, 23,  1,  4,  0,  0]]), tensor([0, 1, 1]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter(dl))\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0890,  2.8199,  1.8165, -1.9656],\n",
       "        [ 1.2061,  0.3585, -0.5961, -1.2669],\n",
       "        [-1.5584, -2.4416,  1.1176, -0.4356],\n",
       "        [ 0.1442, -0.2951,  0.8433, -0.1157],\n",
       "        [-0.2319,  0.6422, -0.1494,  0.2830],\n",
       "        [ 0.7345,  2.0709,  1.4002,  0.0562],\n",
       "        [-0.3707,  0.5033,  0.6533, -1.4163],\n",
       "        [ 0.7275,  0.1103,  1.3854,  0.0924],\n",
       "        [ 0.3850,  0.0692,  0.1639,  0.3658],\n",
       "        [-1.2039, -1.9414, -0.2005, -0.4414]], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be = BloomEmbedding(11, 4)\n",
    "be.emb.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2,  5,  6,  3,  7,  8,  2,  9, 10,  1],\n",
       "        [13, 14,  1, 15, 16, 17,  3, 18,  1,  4],\n",
       "        [19, 20,  1, 21, 22, 23,  1,  4,  0,  0]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 8, 1]\n",
      "[6, 10, 6]\n",
      "[8, 2, 7]\n",
      "[10, 5, 5]\n",
      "[4, 8, 6]\n",
      "[2, 8, 1]\n",
      "[6, 6, 10]\n",
      "[6, 9, 7]\n",
      "[5, 9, 4]\n",
      "[5, 10, 8]\n",
      "[7, 8, 6]\n",
      "[7, 9, 2]\n",
      "[1, 7, 3]\n",
      "[2, 5, 1]\n",
      "[9, 8, 8]\n",
      "[8, 1, 10]\n",
      "[6, 3, 9]\n",
      "[2, 6, 9]\n",
      "[10, 5, 0]\n",
      "[2, 10, 6]\n",
      "[1, 7, 8]\n",
      "[2, 7, 1]\n",
      "[9, 4, 8]\n",
      "[5, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "for i in range(24):\n",
    "    print(probabilistic_hash_item(i, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 10, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (bs x seq_len) -> (bs -> seq_len -> emb_size)\n",
    "y = be(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2904,  1.0825,  2.8281, -2.4985],\n",
       "        [ 3.2287,  3.6471,  2.0098, -4.4070],\n",
       "        [-0.9388,  0.2589,  2.3995, -0.7704],\n",
       "        [-1.8995, -0.0147, -0.6485,  0.4076],\n",
       "        [ 1.9549,  3.0019,  1.6213, -2.2612],\n",
       "        [ 0.2973,  0.4164,  0.8578,  0.5331],\n",
       "        [ 2.2904,  1.0825,  2.8281, -2.4985],\n",
       "        [-0.3233, -1.1196,  1.1995,  0.2997],\n",
       "        [ 2.2974,  3.0430,  2.8428, -2.5347],\n",
       "        [ 0.2651,  2.2004,  2.6000, -0.3290]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.2693,  4.1790,  0.4750, -4.2163],\n",
       "        [ 2.5744,  2.3608,  4.3350,  0.6067],\n",
       "        [ 0.2651,  2.2004,  2.6000, -0.3290],\n",
       "        [-1.9458, -1.4528,  4.1191, -2.7502],\n",
       "        [-0.4390, -0.3015,  2.6817, -0.0136],\n",
       "        [ 2.3256,  2.4986,  0.9680, -0.8449],\n",
       "        [-1.8995, -0.0147, -0.6485,  0.4076],\n",
       "        [-0.2297, -0.9407, -0.9459, -1.4253],\n",
       "        [ 0.2651,  2.2004,  2.6000, -0.3290],\n",
       "        [ 2.8123,  2.2446,  3.0328, -1.2341]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4671, -1.4535,  0.4032, -2.0935],\n",
       "        [ 1.1803,  5.5044,  5.2554, -3.2333],\n",
       "        [ 0.2651,  2.2004,  2.6000, -0.3290],\n",
       "        [ 1.0686,  3.3866,  2.7170, -4.7645],\n",
       "        [ 1.9912,  1.9554,  3.7928,  0.3986],\n",
       "        [-2.6212, -4.1307,  3.4713, -0.4958],\n",
       "        [ 0.2651,  2.2004,  2.6000, -0.3290],\n",
       "        [ 2.8123,  2.2446,  3.0328, -1.2341],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  0.0000]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is 1 [ 0.26507235  2.2003503   2.5999665  -0.32898915]\n",
      "I 2 [ 2.2903948  1.0824531  2.828073  -2.498523 ]\n",
      "the 3 [-1.8995289  -0.01470172 -0.6485326   0.40761688]\n",
      "grey. 4 [ 2.812286   2.24457    3.0328324 -1.2340608]\n",
      "walked 5 [ 3.2286701  3.6471388  2.009758  -4.4069753]\n",
      "to 6 [-0.93877876  0.2589332   2.3995147  -0.7704108 ]\n",
      "store 7 [ 1.9548693  3.0019045  1.6212814 -2.2612271]\n",
      "so 8 [0.29734743 0.41637933 0.85779124 0.5330522 ]\n",
      "hope 9 [-0.3232504  -1.1195885   1.1994998   0.29972154]\n",
      "it 10 [ 2.2973628  3.042996   2.8428445 -2.534657 ]\n",
      "not 11 [ 0.8497246  1.4343383  0.8743619 -3.7337801]\n",
      "closed. 12 [-3.043953  -1.0598472  3.3869708 -4.2589087]\n",
      "The 13 [ 2.2692842  4.179036   0.4749601 -4.2163124]\n",
      "theater 14 [2.5744486  2.3608148  4.334958   0.60669684]\n",
      "closed 15 [-1.9457765 -1.452824   4.1191187 -2.7502215]\n",
      "today 16 [-0.43895298 -0.30150518  2.6817129  -0.01360181]\n",
      "and 17 [ 2.325552    2.498587    0.9679917  -0.84489024]\n",
      "sky 18 [-0.22965312 -0.9407249  -0.9459034  -1.4252954 ]\n",
      "His 19 [-0.4671499  -1.4534968   0.40321434 -2.0935135 ]\n",
      "dog 20 [ 1.1802679  5.5044327  5.2554398 -3.2333224]\n",
      "brown 21 [ 1.0686436  3.3865533  2.716979  -4.764516 ]\n",
      "while 22 [1.9911952  1.9553655  3.792798   0.39860588]\n",
      "hers 23 [-2.621228   -4.1306973   3.471336   -0.49583316]\n"
     ]
    }
   ],
   "source": [
    "for w, i in ds.w2i.items():\n",
    "    print(w, i, be(torch.tensor([[i]])).detach().numpy().squeeze())\n",
    "#           .emb.weight[hash_int(i, be.n_emb)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[5, 3, 3, 8]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashed = probabilistic_hash_tensor(torch.tensor([23]).unsqueeze(0), 11, 4)\n",
    "hashed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.6212, -4.1307,  3.4713, -0.4958]]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "be.emb.weight[hashed].sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_combos(tups):\n",
    "    return len(set(tuple(sorted(x)) for x in tups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_all_idx(vocab_size, n_buckets, n_hashes):\n",
    "    return [probabilistic_hash_item(i, n_buckets, int, n_hashes) \n",
    "            for i in range(vocab_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buckets: 127 Hashes: 5 Unique combos: 29998 % unique: 0.99993\n",
      "Buckets: 251 Hashes: 4 Unique combos: 29996 % unique: 0.99987\n",
      "Buckets: 997 Hashes: 3 Unique combos: 29997 % unique: 0.9999\n",
      "Buckets: 5003 Hashes: 2 Unique combos: 29969 % unique: 0.99897\n"
     ]
    }
   ],
   "source": [
    "buckets2hashes = {127: 5,\n",
    "                  251: 4,\n",
    "                  997: 3,\n",
    "                  5_003: 2}\n",
    "for b, h in buckets2hashes.items():\n",
    "    tups = hash_all_idx(30_000, b,  h)\n",
    "    unique = unique_combos(tups)\n",
    "    print('Buckets:', b, 'Hashes:', h, 'Unique combos:', unique,\n",
    "          '% unique:', round(unique/30_000, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_n_buckets(vocab_size, hash_sizes, bucket_sizes):\n",
    "    for bs in bucket_sizes:\n",
    "        for hs in hash_sizes:\n",
    "            tups = hash_all_idx(vocab_size, bs, hs)\n",
    "            unique = unique_combos(tups)\n",
    "            print('buckets:', bs, \n",
    "                  'hashes:', hs, \n",
    "                  'unique:', round(unique/vocab_size, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buckets: 5 hashes: 2 unique: 0.1875\n",
      "buckets: 5 hashes: 3 unique: 0.3625\n",
      "buckets: 5 hashes: 4 unique: 0.5375\n",
      "buckets: 5 hashes: 5 unique: 0.625\n",
      "buckets: 11 hashes: 2 unique: 0.625\n",
      "buckets: 11 hashes: 3 unique: 0.875\n",
      "buckets: 11 hashes: 4 unique: 0.975\n",
      "buckets: 11 hashes: 5 unique: 1.0\n",
      "buckets: 13 hashes: 2 unique: 0.675\n",
      "buckets: 13 hashes: 3 unique: 0.9\n",
      "buckets: 13 hashes: 4 unique: 1.0\n",
      "buckets: 13 hashes: 5 unique: 1.0\n",
      "buckets: 19 hashes: 2 unique: 0.85\n",
      "buckets: 19 hashes: 3 unique: 0.975\n",
      "buckets: 19 hashes: 4 unique: 1.0\n",
      "buckets: 19 hashes: 5 unique: 1.0\n",
      "buckets: 29 hashes: 2 unique: 0.9375\n",
      "buckets: 29 hashes: 3 unique: 0.9875\n",
      "buckets: 29 hashes: 4 unique: 1.0\n",
      "buckets: 29 hashes: 5 unique: 1.0\n",
      "buckets: 37 hashes: 2 unique: 0.925\n",
      "buckets: 37 hashes: 3 unique: 1.0\n",
      "buckets: 37 hashes: 4 unique: 1.0\n",
      "buckets: 37 hashes: 5 unique: 1.0\n"
     ]
    }
   ],
   "source": [
    "eval_n_buckets(80, range(2, 6), [5, 11, 13, 19, 29, 37])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 500])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(0, 30_000, (64, 500))\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339 ms ± 2.82 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "hashed = probabilistic_hash_tensor(x, 127, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343 ms ± 2.82 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "hashed = probabilistic_hash_tensor(x, 251, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352 ms ± 4.19 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "hashed = probabilistic_hash_tensor(x, 997, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346 ms ± 3.62 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "hashed = probabilistic_hash_tensor(x, 5_003, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318 ms ± 1.54 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "hashed = probabilistic_hash_tensor(x, 251, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "303 ms ± 3.05 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "hashed = probabilistic_hash_tensor(x, 997, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284 ms ± 2.39 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "hashed = probabilistic_hash_tensor(x, 5_003, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(range(2, 6), [284, 303, 318, 339], s=[127, 251, 997, 5_003])\n",
    "plt.xlabel('# of Hashes')\n",
    "plt.ylabel('Time (ms) \\nto encode \\na batch with \\n32,000 words',\n",
    "           rotation=0, labelpad=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try memoizing hash func.\n",
    "\n",
    "Caching results does save computation but it increases memory usage, which is one of the benefits of Bloom Embeddings. However, this is probably still more memory efficient since each word in the cache is represented by only a few indices rather than a large embedding. A large embedding matrix also forces us to devote a lot of memory to unused gradients during training (if the layer is not frozen). Still, the fact that `probabilistic_hash_tensor` is not vectorized means that it may be best to pre-compute these and load the indices in the Dataset. Rather than caching, it may be easiest to create a w2hash dict similar to a standard w2index dict. Leaving this implementation lets the user determine what tradeoff they want between speed and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=30_000)\n",
    "def probabilistic_hash_item(x, n_buckets, mode=int, n_hashes=3):\n",
    "    \"\"\"Slightly hacky way to probabilistically hash an integer by\n",
    "    first converting it to a string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: int\n",
    "        The integer or string to hash.\n",
    "    n_buckets: int\n",
    "        The number of buckets that items will be mapped to. Typically \n",
    "        this would occur outside the hashing function, but since \n",
    "        the intended use case is so narrow here it makes sense to me \n",
    "        to include it here.\n",
    "    mode: type\n",
    "        The type of input you want to hash. This is user-provided to prevent\n",
    "        accidents where we pass in a different item than intended and hash \n",
    "        the wrong thing. One of (int, str). When using this inside a\n",
    "        BloomEmbedding layer, this must be `int` because there are no\n",
    "        string tensors. When used inside a dataset or as a one-time\n",
    "        pre-processing step, you can choose either as long as you\n",
    "        pass in the appropriate inputs.\n",
    "    n_hashes: int\n",
    "        The number of times to hash x, each time with a different seed.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    list[int]: A list of integers with length `n_hashes`, where each integer\n",
    "        is in [0, n_buckets).\n",
    "    \"\"\"\n",
    "    # Check type to ensure we don't accidentally hash Tensor(5) instead of 5.\n",
    "    assert isinstance(x, mode), f'Input `x` must have type {mode}.'\n",
    "    return [mmh3.hash(str(x), i, signed=False) % n_buckets \n",
    "            for i in range(n_hashes)]\n",
    "\n",
    "\n",
    "def probabilistic_hash_tensor(x_r2, n_buckets, n_hashes=3, pad_idx=0):\n",
    "    \"\"\"Hash a rank 2 LongTensor.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x_r2: torch.LongTensor\n",
    "        Rank 2 tensor of integers. Shape: (bs, seq_len)\n",
    "    n_buckets: int\n",
    "        Number of buckets to hash items into (i.e. the number of \n",
    "        rows in the embedding matrix). Typically a moderately large\n",
    "        prime number, like 251 or 997.\n",
    "    n_hashes: int\n",
    "        Number of hashes to take for each input index. This determines\n",
    "        the number of rows of the embedding matrix that will be summed\n",
    "        to get the representation for each word. Typically 2-5.\n",
    "    pad_idx: int or None\n",
    "        If you want to pad sequences with vectors of zeros, pass in an\n",
    "        integer (same as the `padding_idx` argument to nn.Embedding).\n",
    "        If None, no padding index will be used. The sequences must be\n",
    "        padded before passing them into this function.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.LongTensor: Tensor of indices where each row corresponds\n",
    "        to one of the input indices. Shape: (bs, seq_len, n_hashes)\n",
    "    \"\"\"\n",
    "    return torch.tensor(\n",
    "        [[probabilistic_hash_item(x.item(), n_buckets, int, n_hashes) \n",
    "          if x != pad_idx else [pad_idx]*n_hashes for x in row]\n",
    "         for row in x_r2]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191 ms ± 12 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "hashed = probabilistic_hash_tensor(x, 5_003, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 ms ± 8.76 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "hashed = probabilistic_hash_tensor(x, 5_003, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195 ms ± 5.1 ms per loop (mean ± std. dev. of 5 runs, 5 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 5 -r 5\n",
    "hashed = probabilistic_hash_tensor(x, 5_003, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CacheInfo(hits=780342, misses=19633, maxsize=30000, currsize=19633)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilistic_hash_item.cache_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19634"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(flatten(x.numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_hash_item.cache_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CacheInfo(hits=0, misses=0, maxsize=30000, currsize=0)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilistic_hash_item.cache_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
