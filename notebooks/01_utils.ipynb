{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils\n",
    "\n",
    "> Basic utilities with few dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import inspect\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At training time, we will typically want to put the model and the current mini batch on the GPU. When developing on a CPU, a GPU isn't available, so we define a variable that will automatically find the right device. This goes in utils rather than core to avoid circular imports with the callbacks module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def hasarg(func, arg):\n",
    "    \"\"\"Checks if a function has a given argument.\n",
    "    Works with args and kwargs as well if you exclude the\n",
    "    stars. See example below.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    func: function\n",
    "    arg: str\n",
    "        Name of argument to look for.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    def foo(a, b=6, *args):\n",
    "        return\n",
    "\n",
    "    >>> hasarg(foo, 'b')\n",
    "    True\n",
    "\n",
    "    >>> hasarg(foo, 'args')\n",
    "    True\n",
    "\n",
    "    >>> hasarg(foo, 'c')\n",
    "    False\n",
    "    \"\"\"\n",
    "    return arg in inspect.signature(func).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def quick_stats(x, digits=3):\n",
    "    \"\"\"Quick wrapper to get mean and standard deviation of a tensor.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: torch.Tensor\n",
    "    digits: int\n",
    "        Number of digits to round mean and standard deviation to.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[float]\n",
    "    \"\"\"\n",
    "    return round(x.mean().item(), digits), round(x.std().item(), digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def concat(*args, dim=-1):\n",
    "    \"\"\"Wrapper to torch.cat which accepts tensors as non-keyword\n",
    "    arguments rather than requiring them to be wrapped in a list.\n",
    "    This can be useful if we've built some generalized functionality\n",
    "    where parameters must be passed in a consistent manner.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    args: torch.tensor\n",
    "        Multiple tensors to concatenate.\n",
    "    dim: int\n",
    "        Dimension to concatenate on (last dimension by default).\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.tensor\n",
    "    \"\"\"\n",
    "    return torch.cat(args, dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def weighted_avg(*args, weights):\n",
    "    \"\"\"Compute a weighted average of multiple tensors.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    args: torch.tensor\n",
    "        Multiple tensors with the same dtype and shape that you want to average.\n",
    "    weights: list\n",
    "        Ints or floats to weight each input tensor. The length of this list must\n",
    "        match the number of tensors passed in: the first weight will be multiplied \n",
    "        by the first tensor, the second weight by the second tensor, etc. If your\n",
    "        weights don't sum to 1, they will be normalized automatically.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    torch.tensor: Same dtype and shape as each of the input tensors.\n",
    "    \"\"\"\n",
    "    weights = torch.tensor(weights)\n",
    "    total = weights.sum().float()\n",
    "    if total != 1: weights = weights / total\n",
    "    res = torch.stack(args)\n",
    "    weights_shape = [-1 if i == 0 else 1 for i, _ in enumerate(range(res.ndim))]\n",
    "    return torch.mean(res * weights.view(*weights_shape), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def identity(x):\n",
    "    \"\"\"Temporarily copied from htools.\n",
    "    \n",
    "    Returns the input argument. Sometimes it is convenient to have this if\n",
    "    we sometimes apply a function to an item: rather than defining a None\n",
    "    variable, sometimes setting it to a function, then checking if it's None\n",
    "    every time we're about to call it, we can set the default as identity and\n",
    "    safely call it without checking.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x: any\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    x: Unchanged input.\n",
    "    \"\"\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def tensor_dict_diffs(d1, d2):\n",
    "    \"\"\"Compare two dictionaries of tensors. The two dicts must have the \n",
    "    same keys.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    d1: dict[any: torch.Tensor]\n",
    "    d2: dict[any: torch.Tensor]\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    list: Returns the keys where tensors differ for d1 and d2. \n",
    "    \"\"\"\n",
    "    assert d1.keys() == d2.keys()\n",
    "    res = []\n",
    "    for k, v in d1.items():\n",
    "        if not torch.eq(v, d2[k]).all():\n",
    "            res.append(k)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def find_tensors(gpu_only=True):\n",
    "    \"\"\"Prints a list of the Tensors being tracked by the garbage collector.\n",
    "    From\n",
    "    https://forums.fast.ai/t/gpu-memory-not-being-freed-after-training-is-over/10265/8\n",
    "    with some minor reformatting.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gpu_only: bool\n",
    "        If True, only find tensors that are on the GPU.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    None: Output is printed to stdout.\n",
    "    \"\"\"\n",
    "    def pretty_size(size):\n",
    "        \"\"\"Pretty prints a torch.Size object.\"\"\"\n",
    "        assert(isinstance(size, torch.Size))\n",
    "        return \" × \".join(map(str, size))\n",
    "\n",
    "    total_size = 0\n",
    "    for obj in gc.get_objects():\n",
    "        try:\n",
    "            if torch.is_tensor(obj) and (not gpu_only or obj.is_cuda):\n",
    "                print(f'{type(obj).__name__}: {\"GPU\" if obj.is_cuda else \"\"}',\n",
    "                      \"pinned\" if obj.is_pinned else \"\",\n",
    "                      pretty_size(obj.size()))\n",
    "                total_size += obj.numel()\n",
    "            elif hasattr(obj, \"data\") and torch.is_tensor(obj.data):\n",
    "                if not gpu_only or obj.is_cuda:\n",
    "                    print(\n",
    "                        f'{type(obj).__name__} → {type(obj.data).__name__}',\n",
    "                        'GPU' if obj.is_cuda else '',\n",
    "                        'pinned' if obj.data.is_pinned else '',\n",
    "                        'grad' if obj.requires_grad else '',\n",
    "                        'volatile' if obj.volatile else '',\n",
    "                        pretty_size(obj.data.size())\n",
    "                    )\n",
    "                    total_size += obj.data.numel()\n",
    "        except Exception:\n",
    "            pass\n",
    "    print(\"Total size:\", total_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
