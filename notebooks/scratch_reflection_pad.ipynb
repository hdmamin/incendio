{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "from incendio.core import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(BaseModel):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.pad = nn.ReflectionPad2d(2)\n",
    "        self.conv1 = nn.Conv2d(3, 8, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5)\n",
    "        self.adapt = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(self.pad(x))\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv2(self.pad(x))\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.adapt(x)\n",
    "        x = self.fc(x.squeeze())\n",
    "        return torch.sigmoid(x)\n",
    "    \n",
    "    \n",
    "class Model2(BaseModel):\n",
    "\n",
    "    def __init__(self, c_in, c_outs):\n",
    "        super().__init__()\n",
    "        dims = [c_in] + c_outs\n",
    "        self.enc = nn.Sequential(*[nn.Sequential(nn.ReflectionPad2d(2),\n",
    "                                   nn.Conv2d(*(c_in, c_out), kernel_size=5),\n",
    "                                   nn.LeakyReLU())\n",
    "                                   for c_in, c_out in zip(dims, dims[1:])])\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(c_outs[-1], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc(x)\n",
    "        x = self.pool(x)\n",
    "        print(x.shape)\n",
    "        x = x.squeeze()\n",
    "        print(x.shape)\n",
    "        return torch.sigmoid(self.fc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model2(\n",
       "  (enc): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): ReflectionPad2d((2, 2, 2, 2))\n",
       "      (1): Conv2d(3, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): ReflectionPad2d((2, 2, 2, 2))\n",
       "      (1): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): ReflectionPad2d((2, 2, 2, 2))\n",
       "      (1): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "      (2): LeakyReLU(negative_slope=0.01)\n",
       "    )\n",
       "  )\n",
       "  (pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2 = Model2(3, [8, 16, 32])\n",
    "m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (pad): ReflectionPad2d((2, 2, 2, 2))\n",
       "  (conv1): Conv2d(3, 8, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(8, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (adapt): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1 = Model()\n",
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[242., 170., 160., 117.],\n",
       "          [154.,   2., 186., 243.],\n",
       "          [151.,  47.,  33., 201.],\n",
       "          [163., 199., 114., 197.]],\n",
       "\n",
       "         [[213., 203., 234., 193.],\n",
       "          [ 22., 209., 192.,  71.],\n",
       "          [ 39.,  24., 200., 219.],\n",
       "          [172., 170., 102., 136.]],\n",
       "\n",
       "         [[160.,  23.,  98.,  46.],\n",
       "          [  5.,  63.,  40., 184.],\n",
       "          [206., 126., 126.,  49.],\n",
       "          [236., 196., 218.,  43.]]],\n",
       "\n",
       "\n",
       "        [[[ 20.,  74., 165., 111.],\n",
       "          [203.,  71.,  98., 210.],\n",
       "          [167., 178.,  25.,  93.],\n",
       "          [207., 109., 228.,   9.]],\n",
       "\n",
       "         [[ 22., 150., 237., 171.],\n",
       "          [188.,  34., 214.,  87.],\n",
       "          [ 92., 200., 179., 242.],\n",
       "          [137.,  76., 213., 157.]],\n",
       "\n",
       "         [[222., 102., 126.,  55.],\n",
       "          [207., 195., 234.,  98.],\n",
       "          [222.,  90.,  40., 117.],\n",
       "          [ 30., 211., 191., 221.]]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(255, (2, 3, 4, 4)).float()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(img):\n",
    "    plt.imshow(img.permute(1, 2, 0) / 255)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQcAAAD8CAYAAAB6iWHJAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAANJ0lEQVR4nO3dfaxkdX3H8fdHQGoEeVoaNsvKQ6CkBitP2WJJWgrSAjXQREzhDwUDXWukYlMTpU1oa9KINdFGMRoCpGCMQMHSraGabcCqsVCum2WFpeiWxLArkUd32fBgFr79Yw70evldFnfOnJnlvl/JZM+Z89v5fidsPsw9Z+75pqqQpIXeMO0GJM0mw0FSk+EgqclwkNRkOEhqMhwkNY0VDkkOTLI2yY+7Pw9YZN0LSdZ3jzXj1JQ0jIzzPYck/wA8WVVXJvkEcEBVfbyxbntV7TNGn5IGNm44PAicWlWPJFkOfLuqjmmsMxyk3cy44fDzqtq/2w7w1Ev7C9btANYDO4Arq+q2RV5vNbAa4M1v2vvE3zhy+S73Nqu2vrh52i1MzItv3G/aLUzEs889M+0WJuZnDz77eFUd3Dq2587+cpL/AA5pHPrr+TtVVUkWS5rDqmpLkiOBO5L8sKr+d+GiqroauBrghGOPqP+8+W931t5u55vPvOKnrteNZw77o2m3MBH3PzA37RYm5jO/t+Enix3baThU1bsWO5bkZ0mWz/ux4tFFXmNL9+dDSb4NHA+8IhwkzY5xL2WuAS7sti8E/nXhgiQHJNm7214GnAJsHLOupAkbNxyuBM5I8mPgXd0+SU5Kck235jeBuST3AncyOudgOEgzbqc/VryaqnoCOL3x/BxwSbf9feDt49SRNDy/ISmpyXCQ1GQ4SGoyHCQ1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKnJcJDU1Es4JDkzyYNJNnWTrxYe3zvJTd3xu5Mc3kddSZMzdjgk2QP4InAW8DbggiRvW7DsYkYDb44CPgd8ety6kiarj08Oq4BNVfVQVf0CuBE4d8Gac4Hru+1bgNO7CVmSZlQf4bACeHje/ubuueaaqtoBbAUO6qG2pAmZqROSSVYnmUsy9/iTT0+7HWlJ6yMctgAr5+0f2j3XXJNkT2A/4ImFL1RVV1fVSVV10rID9+2hNUm7qo9wuAc4OskRSd4InM9oTN5888fmnQfcUeOM95Y0cWNNvILROYQklwLfAvYArquq+5N8EpirqjXAtcBXkmwCnmQUIJJm2NjhAFBVtwO3L3juinnbzwHv7aOWpGHM1AlJSbPDcJDUZDhIajIcJDUZDpKaDAdJTYaDpCbDQVKT4SCpyXCQ1GQ4SGoyHCQ1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqGmpW5kVJHkuyvntc0kddSZMz9g1m583KPIPRtKt7kqypqo0Llt5UVZeOW0/SMPq4+/TLszIBkrw0K3NhOPxKnnzwJ9z4+3/aQ3uz5c/W/s60W5iYb278/rRbmIj3fOofp93CxHyGMxc9NtSsTID3JNmQ5JYkKxvHf2kc3tMvOvNGmqahTkj+G3B4Vf0WsJb/n7j9S+aPw9v3DQ7hlqZpkFmZVfVEVT3f7V4DnNhDXUkTNMiszCTL5+2eAzzQQ11JEzTUrMyPJDkH2MFoVuZF49aVNFlDzcq8HLi8j1qShuE3JCU1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKnJcJDUZDhIajIcJDUZDpKa+hqHd12SR5Pct8jxJPl8Ny5vQ5IT+qgraXL6+uTwT/Aqo3PgLODo7rEa+FJPdSVNSC/hUFXfYXRX6cWcC9xQI3cB+y+4Xb2kGTPUOYfXNDLPcXjS7JipE5KOw5Nmx1DhsNOReZJmy1DhsAZ4f3fV4mRga1U9MlBtSbugl4lXSb4GnAosS7IZ+BtgL4Cq+jKjaVhnA5uAZ4AP9FFX0uT0NQ7vgp0cL+DDfdSSNIyZOiEpaXYYDpKaDAdJTYaDpCbDQVKT4SCpyXCQ1GQ4SGoyHCQ1GQ6SmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpqGGod3apKtSdZ3jyv6qCtpcnq5hySjcXhXATe8yprvVtW7e6onacKGGocnaTfT1yeH1+KdSe4Ffgp8rKruX7ggyWpGg3Y5cM9DeOGA2wdsbxhv/8Pbpt3CxHzwdfrWbrzq8mm3MDlHL35oqBOS64DDquodwBeA5j+j+ePw9tnjgIFak9QySDhU1baq2t5t3w7slWTZELUl7ZpBwiHJIUnSba/q6j4xRG1Ju2aocXjnAR9KsgN4Fji/m4IlaUYNNQ7vKkaXOiXtJvyGpKQmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKnJcJDUZDhIajIcJDUZDpKaDAdJTYaDpCbDQVLT2OGQZGWSO5NsTHJ/kssaa5Lk80k2JdmQ5IRx60qarD7uIbkD+MuqWpdkX+AHSdZW1cZ5a85iND7jaOC3gS91f0qaUWN/cqiqR6pqXbf9NPAAsGLBsnOBG2rkLmD/JMvHrS1pcno955DkcOB44O4Fh1YAD8/b38wrA4Qkq5PMJZnb/sJTfbYm6VfUWzgk2Qe4FfhoVW3blddwHJ40O3oJhyR7MQqGr1bV1xtLtgAr5+0f2j0naUb1cbUiwLXAA1X12UWWrQHe3121OBnYWlWPjFtb0uT0cbXiFOB9wA+TrO+e+yvgrfDyOLzbgbOBTcAzwAd6qCtpgsYOh6r6HpCdrCngw+PWkjQcvyEpqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKnJcJDUZDhIajIcJDUZDpKaDAdJTYaDpCbDQVKT4SCpyXCQ1DTUOLxTk2xNsr57XDFuXUmTNdQ4PIDvVtW7e6gnaQBDjcOTtJvp45PDy15lHB7AO5PcC/wU+FhV3d/4+6uB1QAHLTuUff7+rX22NxO+/OIfTLuFifnn63592i1MxI4jb512C1Mx1Di8dcBhVfUO4AvAba3XmD8O7y1vOaiv1iTtgkHG4VXVtqra3m3fDuyVZFkftSVNxiDj8JIc0q0jyaqu7hPj1pY0OUONwzsP+FCSHcCzwPndFCxJM2qocXhXAVeNW0vScPyGpKQmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKnJcJDUZDhIajIcJDUZDpKaDAdJTYaDpCbDQVJTHzeY/bUk/53k3m4c3t811uyd5KYkm5Lc3c23kDTD+vjk8DxwWjeT4jjgzCQnL1hzMfBUVR0FfA74dA91JU1QH+Pw6qWZFMBe3WPhnaXPBa7vtm8BTn/pVvWSZlNfQ2326G5L/yiwtqoWjsNbATwMUFU7gK2AI62kGdZLOFTVC1V1HHAosCrJsbvyOklWJ5lLMrdtmzNvpGnq9WpFVf0cuBM4c8GhLcBKgCR7AvvRmHjlrExpdvRxteLgJPt3228CzgD+Z8GyNcCF3fZ5wB1OvJJmWx/j8JYD1yfZg1HY3FxV30jySWCuqtYwmqX5lSSbgCeB83uoK2mC+hiHtwE4vvH8FfO2nwPeO24tScPxG5KSmgwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhqMhwkNRkOkpoMB0lNhoOkJsNBUpPhIKnJcJDUZDhIajIcJDUZDpKaDAdJTYaDpKahZmVelOSxJOu7xyXj1pU0WX3cffqlWZnbk+wFfC/Jv1fVXQvW3VRVl/ZQT9IA+rj7dAE7m5UpaTeTPmbLdDMrfgAcBXyxqj6+4PhFwKeAx4AfAX9RVQ83Xmc1sLrbPQZ4cOzmXrtlwOMD1huK72v3M+R7O6yqDm4d6CUcXn6x0eSrfwH+vKrum/f8QcD2qno+yQeBP6mq03or3IMkc1V10rT76Jvva/czK+9tkFmZVfVEVT3f7V4DnNhnXUn9G2RWZpLl83bPAR4Yt66kyRpqVuZHkpwD7GA0K/OiHur27eppNzAhvq/dz0y8t17POUh6/fAbkpKaDAdJTUs+HJKcmeTBJJuSfGLa/fQlyXVJHk1y385X7z6SrExyZ5KN3df1L5t2T314Lb+GMHhPS/mcQ3cS9UeMrrBsBu4BLqiqjVNtrAdJfpfRN1dvqKpjp91PX7orX8ural2SfRl9+e6Pd/f/ZkkCvHn+ryEAlzV+DWEwS/2TwypgU1U9VFW/AG4Ezp1yT72oqu8wujL0ulJVj1TVum77aUaXxVdMt6vx1chM/RrCUg+HFcD8r3Fv5nXwD22pSHI4cDxw93Q76UeSPZKsBx4F1lbVVN/XUg8H7aaS7APcCny0qrZNu58+VNULVXUccCiwKslUfxxc6uGwBVg5b//Q7jnNsO5n8luBr1bV16fdT98W+zWEoS31cLgHODrJEUneCJwPrJlyT3oV3Ym7a4EHquqz0+6nL6/l1xCGtqTDoap2AJcC32J0Yuvmqrp/ul31I8nXgP8CjkmyOcnF0+6pJ6cA7wNOm3dnsbOn3VQPlgN3JtnA6H9aa6vqG9NsaElfypS0uCX9yUHS4gwHSU2Gg6Qmw0FSk+EgqclwkNRkOEhq+j8Lkijpm+yHrQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_img(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 8, 8])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad = nn.ReflectionPad2d(2)\n",
    "x_pad = pad(x)\n",
    "x_pad.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_img(x_pad[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectionPaddedConv2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, out_channels, padding=1, \n",
    "                 kernel_size=3, stride=1, bias=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.reflect = nn.ReflectionPad2d(padding)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                              stride, padding=0, bias=bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.reflect(x)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rc = ReflectionPaddedConv2d(in_channels=3, out_channels=3)\n",
    "nn.init.constant_(rc.conv.weight, 1)\n",
    "nn.init.constant_(rc.conv.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = nn.Conv2d(3, 3, kernel_size=3, padding=1, padding_mode='zeros', bias=True)\n",
    "nn.init.constant_(r.weight, 1)\n",
    "nn.init.constant_(r.bias, 0)\n",
    "x_p = r(x)\n",
    "x_p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False, False, False, False],\n",
       "         [False,  True,  True, False],\n",
       "         [False,  True,  True, False],\n",
       "         [False, False, False, False]],\n",
       "\n",
       "        [[False, False, False, False],\n",
       "         [False,  True,  True, False],\n",
       "         [False,  True,  True, False],\n",
       "         [False, False, False, False]],\n",
       "\n",
       "        [[False, False, False, False],\n",
       "         [False,  True,  True, False],\n",
       "         [False,  True,  True, False],\n",
       "         [False, False, False, False]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0] == x_p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Applies a 2D convolution over an input signal composed of several input\\n    planes.\\n\\n    In the simplest case, the output value of the layer with input size\\n    :math:`(N, C_{\\\\text{in}}, H, W)` and output :math:`(N, C_{\\\\text{out}}, H_{\\\\text{out}}, W_{\\\\text{out}})`\\n    can be precisely described as:\\n\\n    .. math::\\n        \\\\text{out}(N_i, C_{\\\\text{out}_j}) = \\\\text{bias}(C_{\\\\text{out}_j}) +\\n        \\\\sum_{k = 0}^{C_{\\\\text{in}} - 1} \\\\text{weight}(C_{\\\\text{out}_j}, k) \\\\star \\\\text{input}(N_i, k)\\n\\n\\n    where :math:`\\\\star` is the valid 2D `cross-correlation`_ operator,\\n    :math:`N` is a batch size, :math:`C` denotes a number of channels,\\n    :math:`H` is a height of input planes in pixels, and :math:`W` is\\n    width in pixels.\\n\\n    * :attr:`stride` controls the stride for the cross-correlation, a single\\n      number or a tuple.\\n\\n    * :attr:`padding` controls the amount of implicit zero-paddings on both\\n      sides for :attr:`padding` number of points for each dimension.\\n\\n    * :attr:`dilation` controls the spacing between the kernel points; also\\n      known as the Ã  trous algorithm. It is harder to describe, but this `link`_\\n      has a nice visualization of what :attr:`dilation` does.\\n\\n    * :attr:`groups` controls the connections between inputs and outputs.\\n      :attr:`in_channels` and :attr:`out_channels` must both be divisible by\\n      :attr:`groups`. For example,\\n\\n        * At groups=1, all inputs are convolved to all outputs.\\n        * At groups=2, the operation becomes equivalent to having two conv\\n          layers side by side, each seeing half the input channels,\\n          and producing half the output channels, and both subsequently\\n          concatenated.\\n        * At groups= :attr:`in_channels`, each input channel is convolved with\\n          its own set of filters, of size:\\n          :math:`\\\\left\\\\lfloor\\\\frac{out\\\\_channels}{in\\\\_channels}\\\\right\\\\rfloor`.\\n\\n    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:\\n\\n        - a single ``int`` -- in which case the same value is used for the height and width dimension\\n        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,\\n          and the second `int` for the width dimension\\n\\n    .. note::\\n\\n         Depending of the size of your kernel, several (of the last)\\n         columns of the input might be lost, because it is a valid `cross-correlation`_,\\n         and not a full `cross-correlation`_.\\n         It is up to the user to add proper padding.\\n\\n    .. note::\\n\\n        When `groups == in_channels` and `out_channels == K * in_channels`,\\n        where `K` is a positive integer, this operation is also termed in\\n        literature as depthwise convolution.\\n\\n        In other words, for an input of size :math:`(N, C_{in}, H_{in}, W_{in})`,\\n        a depthwise convolution with a depthwise multiplier `K`, can be constructed by arguments\\n        :math:`(in\\\\_channels=C_{in}, out\\\\_channels=C_{in} \\\\times K, ..., groups=C_{in})`.\\n\\n    .. include:: cudnn_deterministic.rst\\n\\n    Args:\\n        in_channels (int): Number of channels in the input image\\n        out_channels (int): Number of channels produced by the convolution\\n        kernel_size (int or tuple): Size of the convolving kernel\\n        stride (int or tuple, optional): Stride of the convolution. Default: 1\\n        padding (int or tuple, optional): Zero-padding added to both sides of the input. Default: 0\\n        padding_mode (string, optional). Accepted values `zeros` and `circular` Default: `zeros`\\n        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1\\n        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1\\n        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``\\n\\n    Shape:\\n        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`\\n        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where\\n\\n          .. math::\\n              H_{out} = \\\\left\\\\lfloor\\\\frac{H_{in}  + 2 \\\\times \\\\text{padding}[0] - \\\\text{dilation}[0]\\n                        \\\\times (\\\\text{kernel\\\\_size}[0] - 1) - 1}{\\\\text{stride}[0]} + 1\\\\right\\\\rfloor\\n\\n          .. math::\\n              W_{out} = \\\\left\\\\lfloor\\\\frac{W_{in}  + 2 \\\\times \\\\text{padding}[1] - \\\\text{dilation}[1]\\n                        \\\\times (\\\\text{kernel\\\\_size}[1] - 1) - 1}{\\\\text{stride}[1]} + 1\\\\right\\\\rfloor\\n\\n    Attributes:\\n        weight (Tensor): the learnable weights of the module of shape\\n                         :math:`(\\\\text{out\\\\_channels}, \\\\frac{\\\\text{in\\\\_channels}}{\\\\text{groups}},`\\n                         :math:`\\\\text{kernel\\\\_size[0]}, \\\\text{kernel\\\\_size[1]})`.\\n                         The values of these weights are sampled from\\n                         :math:`\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})` where\\n                         :math:`k = \\\\frac{1}{C_\\\\text{in} * \\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}`\\n        bias (Tensor):   the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``,\\n                         then the values of these weights are\\n                         sampled from :math:`\\\\mathcal{U}(-\\\\sqrt{k}, \\\\sqrt{k})` where\\n                         :math:`k = \\\\frac{1}{C_\\\\text{in} * \\\\prod_{i=0}^{1}\\\\text{kernel\\\\_size}[i]}`\\n\\n    Examples::\\n\\n        >>> # With square kernels and equal stride\\n        >>> m = nn.Conv2d(16, 33, 3, stride=2)\\n        >>> # non-square kernels and unequal stride and with padding\\n        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))\\n        >>> # non-square kernels and unequal stride and with padding and dilation\\n        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))\\n        >>> input = torch.randn(20, 16, 50, 100)\\n        >>> output = m(input)\\n\\n    .. _cross-correlation:\\n        https://en.wikipedia.org/wiki/Cross-correlation\\n\\n    .. _link:\\n        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\\n    '"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Conv2d.__doc__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
