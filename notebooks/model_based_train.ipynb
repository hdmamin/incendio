{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections.abc import Iterable\n",
    "import inspect\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from operator import gt, lt, add, sub\n",
    "import os\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from sklearn.metrics import (accuracy_score, dcg_score, roc_auc_score, \n",
    "                             precision_score, recall_score)\n",
    "from textblob import TextBlob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "\n",
    "from accio.s3tool import S3tool\n",
    "from htools import hdir, LoggerMixin, eprint\n",
    "from ml_htools.torch_utils import ModelMixin, variable_lr_optimizer, DEVICE, stats, adam\n",
    "from spellotape.utils import stop_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducible testing.\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To Do:\n",
    "\n",
    "- Maybe get different logger and make new folder and/or file for each training run?\n",
    "- Finish + test csvlogger (decide whether to comebine with statshandler)\n",
    "- Build + add + test LRScheduler\n",
    "- Test regression\n",
    "- Saving and loading (locally and/or S3)\n",
    "- s3 upload callback\n",
    "- refactor w/ trainer? (want to save optimizer state, but should we register this w/ the model itself? Also, defining metrics and callbacks in model definition is kind of weird. Might be good to save datasets/dataloaders but again, grouping w/ model kind of weird.)\n",
    "- Handle case when softmax needed on outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    \n",
    "    def __init__(self, n=64, dim=2):\n",
    "        self.x = torch.rand(n, dim).float()\n",
    "        self.y = torch.clamp(\n",
    "            (self.x[:, 0]*.75 + self.x[:, 1]*.25).round(), 0, 1\n",
    "        ).abs().unsqueeze(-1)\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return self.x[i], self.y[i]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Trainer(LoggerMixin):\n",
    "    \n",
    "#     def __init__(self, net, ds_train, ds_val, dl_train, dl_val,\n",
    "#                  criterion, out_dir, last_act=None, bucket=None,\n",
    "#                  optim=Adam, metrics=None, callbacks=None, device=DEVICE, \n",
    "#                  eps=1e-3, classify=True):\n",
    "#         \"\"\"\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         last_act: callable or None\n",
    "#             Last activation function to be applied outside the model. \n",
    "#             For example, for a binary classification problem, if we choose\n",
    "#             to use binary_cross_entropy_with_logits loss but want to compute\n",
    "#             some metric using soft predictions, we would pass in torch.sigmoid\n",
    "#             for last act. For a multi-class problem using F.cross_entropy loss,\n",
    "#             we would need to pass in F.softmax to compute predicted \n",
    "#             probabilities.  Remember this is ONLY necessary if all of the \n",
    "#             following conditions are met:\n",
    "#             1. It is a classification problem.\n",
    "#             2. We have excluded the final activation from our model for \n",
    "#             numerical stability reasons. (I.E. the loss function has the \n",
    "#             the final activation built into it.)\n",
    "#             3. We wish to compute 1 or more metrics based on soft predictions,\n",
    "#             such as AUC-ROC.\n",
    "#         optim: torch.optim callable\n",
    "#             Callable optimizer. The default is Adam.\n",
    "#         classify: bool\n",
    "#             Specifies whether this is a classification problem. If False,\n",
    "#             we assume it's regression.\n",
    "#         \"\"\"\n",
    "#         self.net = net\n",
    "#         self.ds_train, self.ds_val = ds_train, ds_val\n",
    "#         self.dl_train, self.dl_val = dl_train, dl_val\n",
    "#         self.optim = optim\n",
    "#         self.criterion = criterion\n",
    "#         self.device = DEVICE\n",
    "#         self.last_act = last_act\n",
    "#         self.optim = variable_lr_optimizer(net, optimizer=optim, eps=eps)\n",
    "#         self.classify = classify\n",
    "#         self._stop_training = False\n",
    "#         self.logger = None\n",
    "\n",
    "#         # Storage options.\n",
    "#         self.out_dir = out_dir\n",
    "#         self.bucket = bucket\n",
    "#         os.makedirs(out_dir, exist_ok=True)\n",
    "        \n",
    "#         # Dict makes it easier to adjust callbacks after creating model.\n",
    "#         callbacks = [ModelHandler(), StatsHandler(), MetricPrinter()] \\\n",
    "#                     + (callbacks or [])\n",
    "#         self.callbacks = {type(cb).__name__: cb for cb in callbacks}\n",
    "#         self.metrics = [batch_size] + (metrics or [])\n",
    "    \n",
    "#     def save(self, fname):\n",
    "#         save(self, os.path.join(self.out_dir, fname))\n",
    "    \n",
    "#     @classmethod\n",
    "#     def from_file(path):\n",
    "#         load(self, path)\n",
    "    \n",
    "#     def add_callbacks(self, *callbacks):\n",
    "#         self.callbacks.update({type(cb).__name__: cb for cb in callbacks})\n",
    "    \n",
    "#     def add_metrics(self, *metrics):\n",
    "#         self.metrics.extend(metrics)\n",
    "    \n",
    "#     def fit(self, epochs, lrs=3e-3, optim=Adam, eps=1e-3): \n",
    "# #     def fit(self, classify=True, logit=True, thresh=.5):\n",
    "#         _ = self.decide_stop_training('on_train_begin', lrs, optim, eps)\n",
    "#         for e in range(1, epochs+1):\n",
    "#             _ = self.decide_stop_training('on_epoch_begin', e, stats)\n",
    "#             for i, batch in enumerate(self.train_dl, 1):\n",
    "#                 *xb, yb = map(lambda x: x.to(device), batch)\n",
    "#                 self.optim.zero_grad()\n",
    "#                 _ = self.decide_stop_training('on_batch_begin')\n",
    "                \n",
    "#                 # Forward and backward passes.\n",
    "#                 y_score = self(*xb)\n",
    "#                 loss = self.criterion(y_score, yb)\n",
    "#                 loss.backward()\n",
    "#                 self.optim.step()\n",
    "                \n",
    "#                 # Separate because callbacks are only applied during training.\n",
    "#                 self._update_stats(stats, loss, yb, y_score.detach())\n",
    "#                 if self.decide_stop_training('on_batch_end', stats): break\n",
    "            \n",
    "#             # If on_batch_end callback halts training, else block is skipped.  \n",
    "#             else: \n",
    "#                 val_stats = self.validate(val_dl, classify, logit, thresh)\n",
    "#                 if self.decide_stop_training('on_epoch_end', e, stats, val_stats):\n",
    "#                     break\n",
    "#                 continue\n",
    "#             break      \n",
    "\n",
    "#         self.stop_training('on_train_end', stats, val_stats)\n",
    "        \n",
    "#     def _update_stats(self, stats, loss, yb, y_score):\n",
    "#         \"\"\"Update stats in place.\n",
    "        \n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         stats: defaultdict[str, list]\n",
    "#         loss: torch.Tensor\n",
    "#             Tensor containing single value (mini-batch loss).\n",
    "#         yb: torch.Tensor\n",
    "#             Mini-batch of labels.\n",
    "#         y_pred: torch.Tensor\n",
    "#             Mini-batch of predictions.\n",
    "            \n",
    "#         Returns\n",
    "#         -------\n",
    "#         None\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             y_score = self.last_act(y_score)\n",
    "#         except TypeError:\n",
    "#             pass\n",
    "#         y_pred = (y_score > thresh).float() if self.classify else y_score\n",
    "            \n",
    "#         stats['loss'].append(loss.detach().cpu().numpy().item())\n",
    "#         for m in self.metrics:\n",
    "#             yhat = y_pred if hasarg(m, 'y_pred') else y_score\n",
    "#             stats[m.__name__.replace('_score', '')].append(m(yb, yhat))\n",
    "        \n",
    "#     def decide_stop_training(self, attr, *args, **kwargs):\n",
    "#         self._stop_training = False\n",
    "#         # Pass model object as first argument to callbacks.\n",
    "#         for cb in self.callbacks.values():\n",
    "#             getattr(cb, attr)(self, *args, **kwargs)\n",
    "#         return self._stop_training\n",
    "    \n",
    "#     def __repr__(self):\n",
    "#         r = (f'Trainer(criterion={repr(self.criterion.__name__)}, '\n",
    "#              f'out_dir={repr(self.out_dir)}, bucket={repr(self.bucket)})'\n",
    "#              f'\\n\\nDatasets: {len(self.ds_train)} train rows, '\n",
    "#              f'{len(self.ds_val)} val rows'\n",
    "#              f'\\n\\n{repr(self.net)})')\n",
    "#         return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = Trainer(net3, train, val, dl_train, dl_val, F.binary_cross_entropy_with_logits, \n",
    "#             '../data/v1', 'datascience-delphi-dev',\n",
    "#             torch.optim.RMSprop, metrics, callbacks)\n",
    "# t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module, LoggerMixin):\n",
    "    \n",
    "    def __init__(self, dim, criterion, path=os.path.join('..', 'data'),\n",
    "                 callbacks=None, metrics=None):\n",
    "        super().__init__()\n",
    "        # Dictionary makes it easier to adjust callbacks after creating model.\n",
    "        callbacks = [ModelHandler(), StatsHandler(), MetricPrinter()] \\\n",
    "                    + (callbacks or [])\n",
    "        self.callbacks = {type(cb).__name__: cb for cb in callbacks}\n",
    "        self.metrics = [batch_size] + (metrics or [])\n",
    "        self.logger = self.get_logger(os.path.join(path, 'train.log'), \n",
    "                                      fmt='\\n%(asctime)s\\n %(message)s')\n",
    "        self.criterion = criterion    \n",
    "            \n",
    "        # Specific to this model.\n",
    "        self.fc1 = nn.Linear(dim, 2)\n",
    "        self.fc2 = nn.Linear(2, 1)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "    \n",
    "    def fit(self, epochs, loaders, lr=3e-3, optim=None, callbacks=None, \n",
    "            metrics=None, classify=True, logit=True, thresh=.5,\n",
    "            device=DEVICE):\n",
    "        # Initialize stats, data loaders, optimizer, and callbacks.\n",
    "        stats = defaultdict(list)\n",
    "        train_dl, val_dl = loaders\n",
    "        optim = optim or variable_lr_optimizer(self, lr=lr)\n",
    "        _ = self.stop_training('on_train_begin', callbacks, metrics)\n",
    "            \n",
    "        # Train.\n",
    "        for epoch in range(1, epochs+1):\n",
    "            _ = self.stop_training('on_epoch_begin', epoch, stats)\n",
    "            for i, batch in enumerate(train_dl, 1):\n",
    "                *xb, yb = map(lambda x: x.to(device), batch)\n",
    "                optim.zero_grad()\n",
    "                _ = self.stop_training('on_batch_begin')\n",
    "                \n",
    "                # Forward and backward passes.\n",
    "                y_score = self(*xb)\n",
    "                loss = self.criterion(y_score, yb)\n",
    "                loss.backward()\n",
    "                optim.step()\n",
    "                \n",
    "                # Separate because callbacks are only applied during training.\n",
    "                self._update_stats(stats, loss, yb, y_score.detach(),\n",
    "                                   classify, logit, thresh)\n",
    "                if self.stop_training('on_batch_end', stats): break\n",
    "            \n",
    "            # If on_batch_end callback halts training, else block is skipped.  \n",
    "            else: \n",
    "                val_stats = self.validate(val_dl, classify, logit, thresh)\n",
    "                if self.stop_training('on_epoch_end', epoch, stats, val_stats):\n",
    "                    break\n",
    "                continue\n",
    "            break      \n",
    "\n",
    "        self.stop_training('on_train_end', stats, val_stats)\n",
    "            \n",
    "    def validate(self, val_dl, classify, logit, thresh):\n",
    "        val_stats = defaultdict(list)\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_dl:\n",
    "                y_score = self(xb)\n",
    "                loss = self.criterion(y_score, yb)\n",
    "                self._update_stats(val_stats, loss, yb, y_score, classify,\n",
    "                                   logit, thresh)\n",
    "        return val_stats\n",
    "    \n",
    "    def _update_stats(self, stats, loss, yb, y_score, classify, logit, thresh):\n",
    "        \"\"\"Update stats in place.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        stats: defaultdict[str, list]\n",
    "        loss: torch.Tensor\n",
    "            Tensor containing single value (mini-batch loss).\n",
    "        yb: torch.Tensor\n",
    "            Mini-batch of labels.\n",
    "        y_pred: torch.Tensor\n",
    "            Mini-batch of predictions.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        None\n",
    "        \"\"\"\n",
    "        if classify:\n",
    "            if logit: y_score = torch.sigmoid(y_score)\n",
    "            y_pred = (y_score > thresh).float()\n",
    "            \n",
    "        stats['loss'].append(loss.detach().cpu().numpy().item())\n",
    "        for m in self.metrics:\n",
    "            yhat = y_pred if hasarg(m, 'y_pred') else y_score\n",
    "            stats[m.__name__.replace('_score', '')].append(m(yb, yhat))\n",
    "    \n",
    "    def stop_training(self, attr, *args, **kwargs):\n",
    "        self._stop_training = False\n",
    "        # Pass model object as first argument to callbacks.\n",
    "        for cb in self.callbacks.values():\n",
    "            getattr(cb, attr)(self, *args, **kwargs)\n",
    "        return self._stop_training\n",
    "    \n",
    "    def unfreeze(self, n):\n",
    "        pass\n",
    "        \n",
    "    def dims(self):\n",
    "        \"\"\"Get shape of each layer's weights.\"\"\"\n",
    "        return [tuple(p.shape) for p in self.parameters()]\n",
    "\n",
    "    def trainable(self):\n",
    "        \"\"\"Check which layers are trainable.\"\"\"\n",
    "        return [(tuple(p.shape), p.requires_grad) for p in self.parameters()]\n",
    "\n",
    "    def weight_stats(self):\n",
    "        \"\"\"Check mean and standard deviation of each layer's weights.\"\"\"\n",
    "        return [stats(p.data, 3) for p in self.parameters()]\n",
    "\n",
    "    def plot_weights(self):\n",
    "        \"\"\"Plot histograms of each layer's weights.\"\"\"\n",
    "        n_layers = len(self.dims())\n",
    "        fig, ax = plt.subplots(n_layers, figsize=(8, n_layers * 1.25))\n",
    "        if not isinstance(ax, Iterable): ax = [ax]\n",
    "        for i, p in enumerate(self.parameters()):\n",
    "            ax[i].hist(p.data.flatten())\n",
    "            ax[i].set_title(f'Shape: {tuple(p.shape)} Stats: {stats(p.data)}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchCallback:\n",
    "    \n",
    "    def on_train_begin(self, model, callbacks, metrics):\n",
    "        pass\n",
    "    \n",
    "    def on_train_end(self, model, stats, val_stats):\n",
    "        pass\n",
    "    \n",
    "    def on_epoch_begin(self, model, epoch, stats):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, model, epoch, stats, val_stats):\n",
    "        pass\n",
    "    \n",
    "    def on_batch_begin(self, model):\n",
    "        pass\n",
    "    \n",
    "    def on_batch_end(self, model, stats):\n",
    "        pass\n",
    "\n",
    "# # Trainer version\n",
    "# class TorchCallback:\n",
    "    \n",
    "#     def on_train_begin(self, model, lrs, optim, eps):\n",
    "#         pass\n",
    "    \n",
    "#     def on_train_end(self, model, stats, val_stats):\n",
    "#         pass\n",
    "    \n",
    "#     def on_epoch_begin(self, model, epoch, stats):\n",
    "#         pass\n",
    "\n",
    "#     def on_epoch_end(self, model, epoch, stats, val_stats):\n",
    "#         pass\n",
    "    \n",
    "#     def on_batch_begin(self, model):\n",
    "#         pass\n",
    "    \n",
    "#     def on_batch_end(self, model, stats):\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper(TorchCallback):\n",
    "    \n",
    "    def __init__(self, goal, stat='loss', min_improvement=0.0, patience=3):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        goal: str\n",
    "            Indicates what we want to do to the metric in question.\n",
    "            Either 'min' or 'max'. E.g. metric 'loss' should have goal 'min'\n",
    "            while metric 'precision' should have goal 'max'.\n",
    "        stat: str\n",
    "            Quantity to monitor. This will always be computed on the \n",
    "            validation set.\n",
    "        min_improvement: float\n",
    "            Amount of change needed to qualify as improvement. For example,\n",
    "            min_improvement of 0.0 means any improvement is sufficient. With\n",
    "            a min_improvent of 0.2, we will stop training even if the\n",
    "            quantity improves by, for example, 0.1.\n",
    "        patience: int\n",
    "            Number of acceptable epochs without improvement. E.g. patience=0 \n",
    "            means the metric must improve every epoch for training to continue.            \n",
    "        \"\"\"\n",
    "        # Will use op like: self.op(new_val, current_best)\n",
    "        if goal == 'min':\n",
    "            self.init_stat = self.best_stat = float('inf')\n",
    "            self.op = lt\n",
    "            self.op_best = sub\n",
    "        elif goal == 'max':\n",
    "            self.init_stat = self.best_stat = float('-inf')\n",
    "            self.op = gt\n",
    "            self.op_best = add\n",
    "        else:\n",
    "            raise ValueError('Goal must be \"min\" or \"max\".')\n",
    "            \n",
    "        self.stat = stat\n",
    "        self.min_improvement = min_improvement\n",
    "        self.patience = patience\n",
    "        self.since_improvement = 0\n",
    "        \n",
    "    def on_train_begin(self, model, callbacks, metrics):\n",
    "        \"\"\"Resets tracked variables at start of training.\"\"\"\n",
    "        self.best_stat = self.init_stat\n",
    "        self.since_improvement = 0\n",
    "    \n",
    "    def on_epoch_end(self, model, epoch, stats, val_stats):\n",
    "        new_val = val_stats.get(self.stat, None)\n",
    "        if new_val is None:\n",
    "            model.logger.info(f'EarlyStopper could not find {self.stat}.'\n",
    "                              f'Callback behavior may not be enforced.')\n",
    "            \n",
    "        if self.op(new_val, self.op_best(self.best_stat, self.min_improvement)):\n",
    "            self.best_stat = new_val\n",
    "            self.since_improvement = 0\n",
    "        else:\n",
    "            self.since_improvement += 1\n",
    "            if self.since_improvement > self.patience:\n",
    "                model.logger.info(\n",
    "                    f'EarlyStopper halting training: validation {self.stat} '\n",
    "                    f'has not improved enough in {self.since_improvement} epochs.'\n",
    "                )\n",
    "                model._stop_training = True\n",
    "\n",
    "# Trainer version\n",
    "# class EarlyStopper(TorchCallback):\n",
    "    \n",
    "#     def __init__(self, goal, stat='loss', min_improvement=0.0, patience=3):\n",
    "#         \"\"\"\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         goal: str\n",
    "#             Indicates what we want to do to the metric in question.\n",
    "#             Either 'min' or 'max'. E.g. metric 'loss' should have goal 'min'\n",
    "#             while metric 'precision' should have goal 'max'.\n",
    "#         stat: str\n",
    "#             Quantity to monitor. This will always be computed on the \n",
    "#             validation set.\n",
    "#         min_improvement: float\n",
    "#             Amount of change needed to qualify as improvement. For example,\n",
    "#             min_improvement of 0.0 means any improvement is sufficient. With\n",
    "#             a min_improvent of 0.2, we will stop training even if the\n",
    "#             quantity improves by, for example, 0.1.\n",
    "#         patience: int\n",
    "#             Number of acceptable epochs without improvement. E.g. patience=0 \n",
    "#             means the metric must improve every epoch for training to continue.            \n",
    "#         \"\"\"\n",
    "#         # Will use op like: self.op(new_val, current_best)\n",
    "#         if goal == 'min':\n",
    "#             self.init_stat = self.best_stat = float('inf')\n",
    "#             self.op = lt\n",
    "#             self.op_best = sub\n",
    "#         elif goal == 'max':\n",
    "#             self.init_stat = self.best_stat = float('-inf')\n",
    "#             self.op = gt\n",
    "#             self.op_best = add\n",
    "#         else:\n",
    "#             raise ValueError('Goal must be \"min\" or \"max\".')\n",
    "            \n",
    "#         self.stat = stat\n",
    "#         self.min_improvement = min_improvement\n",
    "#         self.patience = patience\n",
    "#         self.since_improvement = 0\n",
    "        \n",
    "#     def on_train_begin(self, model, *args, **kwargs):\n",
    "#         \"\"\"Resets tracked variables at start of training.\"\"\"\n",
    "#         self.best_stat = self.init_stat\n",
    "#         self.since_improvement = 0\n",
    "    \n",
    "#     def on_epoch_end(self, trainer, epoch, stats, val_stats):\n",
    "#         new_val = val_stats.get(self.stat, None)\n",
    "#         if new_val is None:\n",
    "#             model.logger.info(f'EarlyStopper could not find {self.stat}.'\n",
    "#                               f'Callback behavior may not be enforced.')\n",
    "            \n",
    "#         if self.op(new_val, self.op_best(self.best_stat, self.min_improvement)):\n",
    "#             self.best_stat = new_val\n",
    "#             self.since_improvement = 0\n",
    "#         else:\n",
    "#             self.since_improvement += 1\n",
    "#             if self.since_improvement > self.patience:\n",
    "#                 model.logger.info(\n",
    "#                     f'EarlyStopper halting training: validation {self.stat} '\n",
    "#                     f'has not improved enough in {self.since_improvement} epochs.'\n",
    "#                 )\n",
    "#                 trainer._stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceThreshold(TorchCallback):\n",
    "    \n",
    "    def __init__(self, metric, goal, threshold, split='val'):\n",
    "        assert split in ('train', 'val'), 'Split must be \"train\" or \"val\".'\n",
    "        assert goal in ('min', 'max'), 'Goal must be \"min\" or \"max\"'\n",
    "        \n",
    "        self.metric = metric\n",
    "        self.threshold = threshold\n",
    "        self.split = split\n",
    "        self.op = gt if goal == 'min' else lt\n",
    "        \n",
    "    def on_epoch_end(self, model, epoch, stats, val_stats):\n",
    "        data = val_stats if self.split == 'val' else stats\n",
    "        new_val = data.get(self.metric, None)\n",
    "        if new_val is None:\n",
    "            model.logger.info(f'{self.metric} not found in metrics.'\n",
    "                              'PerformanceThreshold may not be enforced.')\n",
    "            return\n",
    "        \n",
    "        if self.op(new_val, self.threshold):\n",
    "            model.logger.info(\n",
    "                f'PerformanceThreshold halting training: {self.metric} '\n",
    "                f'of {new_val:.4f} did not meet threshold.'\n",
    "            )\n",
    "            model._stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricPrinter(TorchCallback):\n",
    "    \"\"\"Prints metrics at the end of each epoch. This is one of the \n",
    "    default callbacks provided in BaseModel - it does not need to\n",
    "    be passed in explicitly.\n",
    "    \"\"\"\n",
    "    \n",
    "    def on_epoch_end(self, model, epoch, stats, val_stats):\n",
    "        data = [[k, v, val_stats[k]] for k, v in stats.items()]\n",
    "        table = tabulate(data, headers=['Metric', 'Train', 'Validation'], \n",
    "                         tablefmt='github', floatfmt='.4f')\n",
    "        model.logger.info(f'Epoch {epoch}\\n\\n{table}\\n\\n{\"=\"*9}')\n",
    "\n",
    "# Trainer version\n",
    "# class MetricPrinter(TorchCallback):\n",
    "#     \"\"\"Prints metrics at the end of each epoch. This is one of the \n",
    "#     default callbacks provided in BaseModel - it does not need to\n",
    "#     be passed in explicitly.\n",
    "#     \"\"\"\n",
    "#     def on_train_begin(self, trainer, *args, **kwargs):\n",
    "#         trainer.logger = trainer.get_logger(\n",
    "#             os.path.join(self.out_dir, 'train.log'),\n",
    "#             fmt='\\n%(asctime)s\\n %(message)s'\n",
    "#         )\n",
    "    \n",
    "#     def on_epoch_end(self, trainer, epoch, stats, val_stats):\n",
    "#         data = [[k, v, val_stats[k]] for k, v in stats.items()]\n",
    "#         table = tabulate(data, headers=['Metric', 'Train', 'Validation'], \n",
    "#                          tablefmt='github', floatfmt='.4f')\n",
    "#         trainer.logger.info(f'Epoch {epoch}\\n\\n{table}\\n\\n{\"=\"*9}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHandler(TorchCallback):\n",
    "    \"\"\"Handles basic model tasks like putting the model on the GPU\n",
    "    and switching between train and eval modes.\n",
    "    \"\"\"\n",
    "    \n",
    "    def on_train_begin(self, model, callbacks, metrics):\n",
    "        model.to(DEVICE)\n",
    "        if callbacks: model.callbacks.update(\n",
    "            {type(cb).__name__: cb for cb in callbacks}\n",
    "        )\n",
    "        if metrics: model.metrics.extend(metrics)\n",
    "        \n",
    "    def on_epoch_begin(self, model, epoch, stats):\n",
    "        model.train()\n",
    "        \n",
    "    def on_train_end(self, model, stats, val_stats):\n",
    "        model.logger.info('Training complete. Model in eval mode.')\n",
    "        model.eval()\n",
    "\n",
    "# # Trainer version\n",
    "# class ModelHandler(TorchCallback):\n",
    "#     \"\"\"Handles basic model tasks like putting the model on the GPU\n",
    "#     and switching between train and eval modes.\n",
    "#     \"\"\"\n",
    "        \n",
    "#     def on_epoch_begin(self, trainer, epoch, stats):\n",
    "#         trainer.model.train()\n",
    "        \n",
    "#     def on_train_end(self, trainer, stats, val_stats):\n",
    "#         trainer.logger.info('Training complete. Model in eval mode.')\n",
    "#         trainer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class S3Uploader(TorchCallback):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def on_train_end(self, model, stats, val_stats):\n",
    "        s3 = S3tool()\n",
    "        s3.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsHandler(TorchCallback):\n",
    "    \"\"\"This updates metrics at the end of each epoch to account for\n",
    "    potentially varying batch sizes.\n",
    "    \"\"\"\n",
    "        \n",
    "    def on_epoch_begin(self, model, epoch, stats):\n",
    "        \"\"\"Resets stats at the start of each epoch.\"\"\"\n",
    "        stats.clear()\n",
    "        \n",
    "    def on_epoch_end(self, model, epoch, stats, val_stats):\n",
    "        \"\"\"Computes (possibly weighted) averages of mini-batch stats\n",
    "        at the end of each epoch.\n",
    "        \"\"\"\n",
    "        for group in (stats, val_stats):\n",
    "            for k, v in group.items():\n",
    "                if k == 'batch_size': continue\n",
    "                group[k] = np.average(v, weights=group['batch_size'])\n",
    "            group.pop('batch_size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSVLogger(TorchCallback):\n",
    "    \"\"\"Separate from StatsHandler in case we don't want to log outputs.\"\"\"\n",
    "    \n",
    "    def __init__(self, mode='epoch', file_fmt='{}_stats.csv'):\n",
    "        assert mode in ('epoch', 'batch'), \\\n",
    "            'Mode must be \"epoch\" or \"batch\".'\n",
    "        self.mode = mode\n",
    "        self.history = defaultdict(list)\n",
    "        self.fname = file_fmt.format(mode)\n",
    "        \n",
    "    def on_train_begin(self, model, callbacks, metrics):\n",
    "        pass\n",
    "        \n",
    "    def on_batch_end(self, model, stats):\n",
    "        if self.mode != 'batch':\n",
    "            pass\n",
    "        pass\n",
    "        \n",
    "    def on_epoch_end(self, model, epoch, stats, val_stats):\n",
    "        if self.mode != 'epoch':\n",
    "            pass\n",
    "        \n",
    "    def write_csv(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EC2Closer(TorchCallback):\n",
    "    \n",
    "    def on_train_end(self, model, stats, val_stats):\n",
    "        stop_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_translate(text, to, from_lang='en'):\n",
    "    return TextBlob(text)\\\n",
    "        .translate(to=to)\\\n",
    "        .translate(from_lang=to, to=from_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Visit ESPN to get up-to-the-minute sports news coverage, scores, highlights and commentary for NFL, MLB, NBA, College Football, NCAA Basketball and more.\n",
    "\"\"\"\n",
    "# back_translate(text, 'es')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Keep sklearn pattern with y_true as first argument.\n",
    "\n",
    "For classification problems, round probabilities once instead of in every metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasarg(func, arg):\n",
    "    return arg in inspect.signature(func).parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_positive(y_true, y_pred):\n",
    "    return (y_pred == 1).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_soft_prediction(y_true, y_score):\n",
    "    return y_score.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_size(y_true, y_pred):\n",
    "    return y_true.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, False]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[hasarg(roc_auc_score, val) for val in ('y_score', 'y_pred')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, True]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[hasarg(precision_score, val) for val in ('y_score', 'y_pred')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 2\n",
    "metrics = [accuracy_score, \n",
    "           precision_score, \n",
    "           recall_score, \n",
    "           percent_positive,\n",
    "           mean_soft_prediction\n",
    "          ]\n",
    "callbacks = [EarlyStopper('max', 'accuracy', patience=3),\n",
    "             PerformanceThreshold('recall', 'max', 0.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = Data(n=34, dim=DIM)\n",
    "val = Data(n=30, dim=DIM)\n",
    "\n",
    "dl_train = DataLoader(train, batch_size=8, shuffle=True)\n",
    "dl_val = DataLoader(val, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc1): Linear(in_features=2, out_features=2, bias=True)\n",
       "  (fc2): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net = Model(DIM, F.binary_cross_entropy_with_logits, callbacks=callbacks,\n",
    "            metrics=metrics)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = Model(DIM, F.binary_cross_entropy_with_logits, callbacks=callbacks,\n",
    "             metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2020-02-12 12:27:51,307\n",
      " Epoch 1\n",
      "\n",
      "| Metric               |   Train |   Validation |\n",
      "|----------------------|---------|--------------|\n",
      "| loss                 |  0.7207 |       0.6854 |\n",
      "| accuracy             |  0.4706 |       0.5667 |\n",
      "| precision            |  0.4118 |       0.0000 |\n",
      "| recall               |  0.9412 |       0.0000 |\n",
      "| percent_positive     |  0.9412 |       0.0000 |\n",
      "| mean_soft_prediction |  0.5432 |       0.4141 |\n",
      "\n",
      "=========\n",
      "\n",
      "2020-02-12 12:27:51,308\n",
      " PerformanceThreshold halting training: recall of 0.0000 did not meet threshold.\n",
      "\n",
      "2020-02-12 12:27:51,308\n",
      " Training complete. Model in eval mode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrisonmamin/.pyenv/versions/3.7.4/envs/main/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/harrisonmamin/.pyenv/versions/3.7.4/envs/main/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "net.fit(10, [dl_train, dl_val], [.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htools import save, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to ../data/net.zip.\n"
     ]
    }
   ],
   "source": [
    "save(net, '../data/net.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object loaded from ../data/net.zip.\n"
     ]
    }
   ],
   "source": [
    "net3 = load('../data/net.zip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
