{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T20:52:48.980744Z",
     "start_time": "2020-09-27T20:52:48.954758Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T20:55:37.008316Z",
     "start_time": "2020-09-27T20:55:36.864363Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from htools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T20:59:02.813211Z",
     "start_time": "2020-09-27T20:59:02.737591Z"
    }
   },
   "outputs": [],
   "source": [
    "# From my img_wang project.\n",
    "class SmoothSoftmaxBase(nn.Module):\n",
    "    \"\"\"Parent class of SmoothSoftmax and SmoothLogSoftmax (softmax or log\n",
    "    softmax with temperature baked in). There shouldn't be a need to\n",
    "    instantiate this class directly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, log=False, temperature='auto', dim=-1):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        log: bool\n",
    "            If True, use log softmax (if this is the last activation in a\n",
    "            network, it can be followed by nn.NLLLoss). If False, use softmax\n",
    "            (this is more useful if you're doing something attention-related:\n",
    "            no standard torch loss functions expect softmax outputs). This\n",
    "            argument is usually passed implicitly by the higher level interface\n",
    "            provided by the child classes.\n",
    "        temperature: float or str\n",
    "            If a float, this is the temperature to divide activations by before\n",
    "            applying the softmax. Values larger than 1 soften the distribution\n",
    "            while values between 0 and 1 sharpen it. If str ('auto'), this will\n",
    "            compute the square root of the last dimension of x's shape the\n",
    "            first time the forward method is called and use that for subsequent\n",
    "            calls.\n",
    "        dim: int\n",
    "            The dimension to compute the softmax over.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.temperature = None if temperature == 'auto' else temperature\n",
    "        self.act = nn.LogSoftmax(dim=dim) if log else nn.Softmax(dim=dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        torch.float: Same shape as x.\n",
    "        \"\"\"\n",
    "        # Kind of silly but this is called every mini batch so removing an\n",
    "        # extra dot attribute access saves a little time.\n",
    "        while True:\n",
    "            try:\n",
    "                return self.act(x.div(self.temperature))\n",
    "            except TypeError:\n",
    "                self.temperature = np.sqrt(x.shape[-1])\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "\n",
    "\n",
    "class SmoothSoftmax(SmoothSoftmaxBase):\n",
    "\n",
    "    def __init__(self, temperature='auto', dim=-1):\n",
    "        super().__init__(log=False, temperature=temperature, dim=dim)\n",
    "\n",
    "\n",
    "class SmoothLogSoftmax(SmoothSoftmaxBase):\n",
    "\n",
    "    def __init__(self, temperature='auto', dim=-1):\n",
    "        super().__init__(log=True, temperature=temperature, dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:38:12.497376Z",
     "start_time": "2020-09-28T00:38:12.444073Z"
    }
   },
   "outputs": [],
   "source": [
    "class SpatialSoftmax(nn.Module):\n",
    "    \n",
    "    def __init__(self, temperature='auto', log=False):\n",
    "        super().__init__()\n",
    "        cls = SmoothLogSoftmax if log else SmoothSoftmax\n",
    "        self.act = cls(temperature)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Should work on any tensor with shape (bs, ..., h, w).\n",
    "        flattened = self.act(x.view(*x.shape[:-2], -1))\n",
    "        return flattened.view(*x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:38:19.748315Z",
     "start_time": "2020-09-28T00:38:19.635830Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 2\n",
    "c = 3\n",
    "h = 4\n",
    "w = 4\n",
    "\n",
    "x = torch.randn(bs, c, h, w)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:38:21.077674Z",
     "start_time": "2020-09-28T00:38:21.021946Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-2.3441, -0.7909,  1.0298,  0.4039],\n",
       "          [-1.6188,  0.2625,  0.3535, -0.9783],\n",
       "          [-0.7640, -1.2406, -1.1361, -2.3369],\n",
       "          [-2.7585,  0.0651,  1.5677,  0.1620]],\n",
       "\n",
       "         [[-0.5695, -1.0143,  0.2400, -1.4993],\n",
       "          [ 0.8825, -0.3705, -0.1127, -1.2692],\n",
       "          [ 0.2704, -1.9788, -0.6400,  0.8381],\n",
       "          [-1.4564, -0.3531, -1.5069, -2.1211]],\n",
       "\n",
       "         [[ 0.0988, -0.4415,  0.7668,  1.9533],\n",
       "          [ 0.4844, -0.5616, -0.7259,  1.7374],\n",
       "          [-0.2316, -0.3654, -1.1872,  0.2421],\n",
       "          [ 0.3454,  0.5115,  0.1213, -0.5517]]],\n",
       "\n",
       "\n",
       "        [[[-0.1965,  1.2759,  1.5981,  0.7768],\n",
       "          [ 1.5828,  1.6654,  1.1962,  2.7620],\n",
       "          [-0.9039, -0.5277,  0.6310, -0.4405],\n",
       "          [-0.9645, -0.3029,  0.5913,  0.2449]],\n",
       "\n",
       "         [[ 1.0945,  0.4701, -0.0326, -1.1616],\n",
       "          [ 3.1092, -1.2894, -0.6499,  2.6702],\n",
       "          [ 1.2273,  0.5223, -0.3593,  1.1957],\n",
       "          [ 0.5903, -0.3326,  1.0712,  0.8777]],\n",
       "\n",
       "         [[-0.8878, -0.0735, -0.4978, -0.3571],\n",
       "          [ 0.8847, -0.4557,  0.5479, -0.8211],\n",
       "          [-1.5795,  1.4599, -1.3009, -0.2769],\n",
       "          [-1.8096,  2.3363, -0.3242,  1.3311]]]])"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:38:21.987624Z",
     "start_time": "2020-09-28T00:38:21.946271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:38:22.507675Z",
     "start_time": "2020-09-28T00:38:22.458557Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.3441, -0.7909,  1.0298,  0.4039, -1.6188,  0.2625,  0.3535,\n",
       "          -0.9783, -0.7640, -1.2406, -1.1361, -2.3369, -2.7585,  0.0651,\n",
       "           1.5677,  0.1620],\n",
       "         [-0.5695, -1.0143,  0.2400, -1.4993,  0.8825, -0.3705, -0.1127,\n",
       "          -1.2692,  0.2704, -1.9788, -0.6400,  0.8381, -1.4564, -0.3531,\n",
       "          -1.5069, -2.1211],\n",
       "         [ 0.0988, -0.4415,  0.7668,  1.9533,  0.4844, -0.5616, -0.7259,\n",
       "           1.7374, -0.2316, -0.3654, -1.1872,  0.2421,  0.3454,  0.5115,\n",
       "           0.1213, -0.5517]],\n",
       "\n",
       "        [[-0.1965,  1.2759,  1.5981,  0.7768,  1.5828,  1.6654,  1.1962,\n",
       "           2.7620, -0.9039, -0.5277,  0.6310, -0.4405, -0.9645, -0.3029,\n",
       "           0.5913,  0.2449],\n",
       "         [ 1.0945,  0.4701, -0.0326, -1.1616,  3.1092, -1.2894, -0.6499,\n",
       "           2.6702,  1.2273,  0.5223, -0.3593,  1.1957,  0.5903, -0.3326,\n",
       "           1.0712,  0.8777],\n",
       "         [-0.8878, -0.0735, -0.4978, -0.3571,  0.8847, -0.4557,  0.5479,\n",
       "          -0.8211, -1.5795,  1.4599, -1.3009, -0.2769, -1.8096,  2.3363,\n",
       "          -0.3242,  1.3311]]])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(*x.shape[:2], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:38:25.524502Z",
     "start_time": "2020-09-28T00:38:25.465703Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpatialSoftmax(\n",
       "  (act): SmoothSoftmax(\n",
       "    (act): Softmax(dim=-1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = SpatialSoftmax(1.0)\n",
    "sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:38:26.254874Z",
     "start_time": "2020-09-28T00:38:26.216106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0058, 0.0276, 0.1705, 0.0912],\n",
       "          [0.0121, 0.0792, 0.0867, 0.0229],\n",
       "          [0.0284, 0.0176, 0.0196, 0.0059],\n",
       "          [0.0039, 0.0650, 0.2921, 0.0716]],\n",
       "\n",
       "         [[0.0461, 0.0296, 0.1036, 0.0182],\n",
       "          [0.1970, 0.0563, 0.0728, 0.0229],\n",
       "          [0.1068, 0.0113, 0.0430, 0.1884],\n",
       "          [0.0190, 0.0573, 0.0181, 0.0098]],\n",
       "\n",
       "         [[0.0406, 0.0237, 0.0793, 0.2596],\n",
       "          [0.0598, 0.0210, 0.0178, 0.2092],\n",
       "          [0.0292, 0.0255, 0.0112, 0.0469],\n",
       "          [0.0520, 0.0614, 0.0416, 0.0212]]],\n",
       "\n",
       "\n",
       "        [[[0.0169, 0.0738, 0.1018, 0.0448],\n",
       "          [0.1003, 0.1089, 0.0681, 0.3262],\n",
       "          [0.0083, 0.0122, 0.0387, 0.0133],\n",
       "          [0.0079, 0.0152, 0.0372, 0.0263]],\n",
       "\n",
       "         [[0.0494, 0.0265, 0.0160, 0.0052],\n",
       "          [0.3706, 0.0046, 0.0086, 0.2389],\n",
       "          [0.0564, 0.0279, 0.0115, 0.0547],\n",
       "          [0.0298, 0.0119, 0.0483, 0.0398]],\n",
       "\n",
       "         [[0.0145, 0.0327, 0.0214, 0.0246],\n",
       "          [0.0852, 0.0223, 0.0608, 0.0155],\n",
       "          [0.0072, 0.1514, 0.0096, 0.0267],\n",
       "          [0.0058, 0.3638, 0.0254, 0.1331]]]])"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:38:53.141923Z",
     "start_time": "2020-09-28T00:38:53.075579Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0389, 0.0574, 0.0905, 0.0774],\n",
       "          [0.0467, 0.0747, 0.0764, 0.0548],\n",
       "          [0.0578, 0.0513, 0.0527, 0.0390],\n",
       "          [0.0351, 0.0711, 0.1035, 0.0728]],\n",
       "\n",
       "         [[0.0624, 0.0558, 0.0764, 0.0495],\n",
       "          [0.0897, 0.0656, 0.0699, 0.0524],\n",
       "          [0.0770, 0.0439, 0.0613, 0.0887],\n",
       "          [0.0500, 0.0659, 0.0494, 0.0423]],\n",
       "\n",
       "         [[0.0606, 0.0529, 0.0716, 0.0963],\n",
       "          [0.0667, 0.0513, 0.0493, 0.0912],\n",
       "          [0.0558, 0.0539, 0.0439, 0.0628],\n",
       "          [0.0644, 0.0671, 0.0609, 0.0515]]],\n",
       "\n",
       "\n",
       "        [[[0.0500, 0.0722, 0.0783, 0.0637],\n",
       "          [0.0780, 0.0796, 0.0708, 0.1047],\n",
       "          [0.0419, 0.0460, 0.0614, 0.0470],\n",
       "          [0.0412, 0.0487, 0.0608, 0.0558]],\n",
       "\n",
       "         [[0.0682, 0.0584, 0.0515, 0.0388],\n",
       "          [0.1129, 0.0376, 0.0441, 0.1012],\n",
       "          [0.0705, 0.0591, 0.0474, 0.0700],\n",
       "          [0.0601, 0.0478, 0.0678, 0.0646]],\n",
       "\n",
       "         [[0.0494, 0.0606, 0.0545, 0.0565],\n",
       "          [0.0770, 0.0551, 0.0708, 0.0503],\n",
       "          [0.0416, 0.0889, 0.0446, 0.0576],\n",
       "          [0.0393, 0.1107, 0.0569, 0.0861]]]])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm = SpatialSoftmax('auto')\n",
    "sm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:39:59.974165Z",
     "start_time": "2020-09-28T00:39:59.918019Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adapted from my annotated GPT notebook.\n",
    "class ConvolutionalProjector(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, spaces=3):\n",
    "        super().__init__()\n",
    "        self.c_in = c_in\n",
    "        self.spaces = spaces\n",
    "        self.conv = nn.Conv2d(c_in, c_in * spaces, 1, groups=1, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Project input tensor into n subspaces. We use 3 by default to\n",
    "        create query, key, and value vectors.\n",
    "        \"\"\"\n",
    "        bs, c, h, w = x.shape\n",
    "        z = self.conv(x)\n",
    "        return z.view(bs, c, self.spaces, h, w).transpose(0, 1)\n",
    "    \n",
    "    # Old way: a similar approach might be more useful if we try to generalize\n",
    "    # nD inputs, but for now we're just using conv2d so probably better to be\n",
    "    # explicit and name the dimensions as we do above. Above also avoids \n",
    "    # calling shape multiple times.\n",
    "#         z = self.conv(x)\n",
    "#         return z.view(z.shape[0], self.c_in, self.spaces, *z.shape[-2:])\\\n",
    "#                 .transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:01.388524Z",
     "start_time": "2020-09-28T00:40:01.346126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolutionalProjector(\n",
       "  (conv): Conv2d(3, 9, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       ")"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj = ConvolutionalProjector(c)\n",
    "proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:03.202028Z",
     "start_time": "2020-09-28T00:40:03.148551Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 3, 1, 1])"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proj.conv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:04.099967Z",
     "start_time": "2020-09-28T00:40:04.036556Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 3, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 3, 4, 4]), torch.Size([2, 3, 4, 4]), torch.Size([2, 3, 4, 4])]"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a single tensor but we can easily assign it to 3 different vars.\n",
    "res = proj(x)\n",
    "q, k, v = proj(x)\n",
    "\n",
    "print(res.shape)\n",
    "smap(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:05.561461Z",
     "start_time": "2020-09-28T00:40:05.504108Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-1.6037, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q[0, 0, 0, :] * k[0, 0, :, 0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:06.832059Z",
     "start_time": "2020-09-28T00:40:06.781014Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6749, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q[-1, -1, -1, :] * k[-1, -1, :, -1]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:07.547305Z",
     "start_time": "2020-09-28T00:40:07.498364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.6037, -0.0939, -0.2546, -2.0012],\n",
       "          [-1.3959, -0.5026, -0.5371, -0.1470],\n",
       "          [-1.2164, -0.0037,  0.5243, -1.4530],\n",
       "          [-1.1913,  0.1414, -0.5223, -2.5689]],\n",
       "\n",
       "         [[ 0.4544, -0.3331,  0.0215,  0.0728],\n",
       "          [ 0.0412,  0.0757, -0.4914, -0.6715],\n",
       "          [ 0.8307, -0.9926, -1.0531,  0.9694],\n",
       "          [ 0.1037, -0.7941, -0.3201, -0.0301]],\n",
       "\n",
       "         [[-0.4950, -0.0913, -0.9914, -0.9311],\n",
       "          [ 0.0516,  0.3831, -0.6121, -0.3686],\n",
       "          [ 1.0549,  0.4504,  0.5480,  1.0619],\n",
       "          [-0.3888, -1.1192, -1.3545, -0.5644]]],\n",
       "\n",
       "\n",
       "        [[[-1.4427,  0.6771, -1.0180, -1.0829],\n",
       "          [-3.3068,  3.1099, -1.5567,  0.6423],\n",
       "          [-1.3335,  1.1716, -1.2045,  0.7099],\n",
       "          [-0.7767,  0.6920, -1.0570,  1.0162]],\n",
       "\n",
       "         [[-0.7798,  0.5073,  0.5074,  0.4309],\n",
       "          [ 1.7568, -2.3141,  0.5051, -0.5665],\n",
       "          [ 0.0459, -0.2779, -0.0075, -0.2615],\n",
       "          [ 0.7350, -1.3004,  0.4445, -0.6756]],\n",
       "\n",
       "         [[ 0.0742,  0.2712,  0.0885,  1.3086],\n",
       "          [ 1.6956, -0.4648, -0.5074,  1.5843],\n",
       "          [-0.3646, -0.3570, -0.2991, -0.0604],\n",
       "          [ 0.4201, -0.6340,  0.1055,  0.6749]]]],\n",
       "       grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = q @ k\n",
    "assert torch.allclose(a1, torch.matmul(q, k))\n",
    "print(a1.shape)\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T01:16:52.793865Z",
     "start_time": "2020-09-28T01:16:52.690639Z"
    }
   },
   "outputs": [],
   "source": [
    "class SpatialAttention2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, temperature='auto', output_attentions=False):\n",
    "        super().__init__()\n",
    "        # TODO: not sure if projector kwargs work w/ other values.\n",
    "        self.projector = ConvolutionalProjector(c_in=c_in, spaces=3)\n",
    "        self.softmax = SpatialSoftmax(temperature)\n",
    "        self.output_attentions = output_attentions\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Noticed my annotated gpt2 notebook uses matrix multiply for\n",
    "        # v and attn weights, while here I'm doing element-wise mult. \n",
    "        # Make sure this is okay. I think it's because with text, we project\n",
    "        # into a larger dimension (spaces*hidden_dim) whereas here we don't\n",
    "        # (but maybe we should? Trying to think what that would look like).\n",
    "        \n",
    "#         q, k, v = self.projector(x)\n",
    "#         attn = self.softmax(q @ k)\n",
    "#         res = (attn * v, )\n",
    "#         if self.output_attentions: res = (*res, attn)\n",
    "#         return res\n",
    "    \n",
    "        # TODO: above implementation only does self attention. \n",
    "        # I want to use between-channel info.\n",
    "        q, k, v = self.projector(x)\n",
    "        attn = q.unsqueeze(1) @ k.unsqueeze(2)\n",
    "        res = attn * v.unsqueeze(1)\n",
    "        \n",
    "        # TODO: test start. Considering what we want to output. This should\n",
    "        # stack along the channel dimension, so we get (bs, c_in*3, h, w).\n",
    "        # Seems like a nice feature to keep h and w the same (potential for\n",
    "        # skip connections) which is why I chose the channel dimension.\n",
    "        # Still need to check this a little more carefully to ensure it's \n",
    "        # doing what I think it is.\n",
    "        \n",
    "        shape = torch.tensor(res.shape)\n",
    "        res = res.view(shape[0], torch.prod(shape[1:3]), *shape[-2:])\n",
    "        # TODO: test end\n",
    "        \n",
    "        return (res, attn) if self.output_attentions else (res,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T01:04:07.436739Z",
     "start_time": "2020-09-28T01:04:07.384322Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpatialAttention2d(\n",
       "  (projector): ConvolutionalProjector(\n",
       "    (conv): Conv2d(3, 9, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (softmax): SpatialSoftmax(\n",
       "    (act): SmoothSoftmax(\n",
       "      (act): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = SpatialAttention2d(3)\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T01:04:07.920356Z",
     "start_time": "2020-09-28T01:04:07.853506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9, 4, 4])"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z, = attn(x)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:57:32.082803Z",
     "start_time": "2020-09-28T00:57:32.015601Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2445,  0.0338],\n",
       "          [ 0.0727,  0.0566]],\n",
       "\n",
       "         [[-0.1141, -0.0486],\n",
       "          [ 0.1168,  0.0052]],\n",
       "\n",
       "         [[-0.2818, -0.0043],\n",
       "          [ 0.0176,  0.0643]]],\n",
       "\n",
       "\n",
       "        [[[-0.2578, -0.0271],\n",
       "          [-0.1115, -0.0273]],\n",
       "\n",
       "         [[ 0.1289,  0.0392],\n",
       "          [-0.1250, -0.0031]],\n",
       "\n",
       "         [[ 0.3163,  0.0034],\n",
       "          [-0.0190, -0.0385]]],\n",
       "\n",
       "\n",
       "        [[[-0.1025,  0.0117],\n",
       "          [-0.0574, -0.0657]],\n",
       "\n",
       "         [[-0.0786, -0.0354],\n",
       "          [ 0.0895, -0.0013]],\n",
       "\n",
       "         [[-0.1683, -0.0053],\n",
       "          [ 0.0125, -0.0072]]]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[0, ..., :2, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T01:00:25.658007Z",
     "start_time": "2020-09-28T01:00:25.569223Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9, 4, 4])"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = torch.tensor(z.shape)\n",
    "z.view(shape[0], torch.prod(shape[1:3]), *shape[-2:]).shape#[0, ..., :2, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:23.309238Z",
     "start_time": "2020-09-28T00:40:23.260300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpatialAttention2d(\n",
       "  (projector): ConvolutionalProjector(\n",
       "    (conv): Conv2d(3, 9, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (softmax): SpatialSoftmax(\n",
       "    (act): SmoothSoftmax(\n",
       "      (act): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 406,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn = SpatialAttention2d(3, output_attentions=True)\n",
    "attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:28.762425Z",
     "start_time": "2020-09-28T00:40:28.722759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 3, 4, 4]), torch.Size([2, 3, 4, 4])]"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z, a = attn(x)\n",
    "smap(z, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:30.841670Z",
     "start_time": "2020-09-28T00:40:30.797360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 1.0000, 1.0000],\n",
       "        [1.0000, 1.0000, 1.0000]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum((-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:33.032735Z",
     "start_time": "2020-09-28T00:40:32.987533Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.0453, -0.0281,  0.0070, -0.0649],\n",
       "          [-0.0225,  0.0104,  0.0217, -0.0999],\n",
       "          [-0.0037, -0.0718, -0.0089, -0.0288],\n",
       "          [-0.0573, -0.0138, -0.0091, -0.0232]],\n",
       "\n",
       "         [[ 0.0548,  0.0309, -0.0473, -0.0782],\n",
       "          [ 0.0188,  0.0110,  0.0153, -0.0352],\n",
       "          [ 0.0191,  0.0373,  0.0621,  0.0375],\n",
       "          [ 0.0569, -0.0190, -0.0397,  0.0178]],\n",
       "\n",
       "         [[-0.0624, -0.0512,  0.0618,  0.0449],\n",
       "          [-0.0097, -0.0162, -0.0149,  0.0023],\n",
       "          [-0.0170, -0.0686, -0.0712, -0.0326],\n",
       "          [-0.0687,  0.0095,  0.0184, -0.0554]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0473,  0.0253,  0.0443, -0.0045],\n",
       "          [ 0.0948,  0.0115, -0.0045,  0.2593],\n",
       "          [ 0.0362, -0.0316,  0.0271,  0.0221],\n",
       "          [ 0.0320, -0.0754,  0.0348, -0.0065]],\n",
       "\n",
       "         [[ 0.0342, -0.0257, -0.0199, -0.0040],\n",
       "          [-0.0549, -0.0222, -0.0459, -0.0360],\n",
       "          [ 0.0607, -0.0432,  0.0308,  0.0184],\n",
       "          [ 0.0645, -0.0754, -0.0037, -0.0435]],\n",
       "\n",
       "         [[-0.0100,  0.0388,  0.0257, -0.0115],\n",
       "          [ 0.1497,  0.0040,  0.0300,  0.0836],\n",
       "          [-0.0445,  0.0415, -0.0297,  0.0030],\n",
       "          [-0.0688,  0.0549,  0.0263,  0.0558]]]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:34.231326Z",
     "start_time": "2020-09-28T00:40:34.187201Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0481, 0.0631, 0.0666, 0.0554],\n",
       "          [0.0677, 0.0689, 0.0647, 0.0707],\n",
       "          [0.0981, 0.0744, 0.0699, 0.0614],\n",
       "          [0.0386, 0.0454, 0.0635, 0.0435]],\n",
       "\n",
       "         [[0.0699, 0.0590, 0.0622, 0.0700],\n",
       "          [0.0616, 0.0545, 0.0611, 0.0671],\n",
       "          [0.0499, 0.0565, 0.0609, 0.0550],\n",
       "          [0.0695, 0.0680, 0.0677, 0.0672]],\n",
       "\n",
       "         [[0.0592, 0.0630, 0.0744, 0.0728],\n",
       "          [0.0588, 0.0620, 0.0662, 0.0639],\n",
       "          [0.0513, 0.0553, 0.0602, 0.0569],\n",
       "          [0.0503, 0.0604, 0.0768, 0.0686]]],\n",
       "\n",
       "\n",
       "        [[[0.0693, 0.0438, 0.0685, 0.0660],\n",
       "          [0.0735, 0.0626, 0.0477, 0.1225],\n",
       "          [0.0462, 0.0620, 0.0501, 0.0527],\n",
       "          [0.0521, 0.0691, 0.0501, 0.0638]],\n",
       "\n",
       "         [[0.0678, 0.0612, 0.0650, 0.0578],\n",
       "          [0.0515, 0.0676, 0.0665, 0.0602],\n",
       "          [0.0549, 0.0753, 0.0680, 0.0667],\n",
       "          [0.0513, 0.0694, 0.0615, 0.0554]],\n",
       "\n",
       "         [[0.0605, 0.0635, 0.0645, 0.0594],\n",
       "          [0.0770, 0.0516, 0.0558, 0.0556],\n",
       "          [0.0614, 0.0679, 0.0661, 0.0624],\n",
       "          [0.0662, 0.0645, 0.0657, 0.0579]]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "- [ ] Test what kinds of values auto temperature gives for different size images. Not sure what the expected input size is: depends how early in the network we apply this. Could plausibly have multiple layers of both regular conv and spatial attention, maybe with different temperatures as we get deeper into the net and height and width decrease. Or if we only use spatial attention, maybe height and width don't need to change and we could therefore keep the same temperature.\n",
    "- [ ] Think about whether this is doing something useful. Found something else called spatial attention which does something different and probably more useful: https://paperswithcode.com/method/spatial-attention-module . That's not to say that I should match what they did necessarily, but it does highlight a weakness in my version. If we're deep in a network, a single channel probably contains a couple \"activated\" regions, representing features like \"has shape of dog ear\" or \"is furry\". Right now, my attention module is learning about how one \"is furry\" area compares to another \"is furry area\". But ideally, we probably want to learn where one \"is furry\" area compares to one \"has shape of dog ear\" area.\n",
    "\n",
    "Fuzzy idea: maybe we can multiply each channel's K by every OTHER channel's Q to get attention weights rather than by its own Q. Options:\n",
    "- Maybe I can just flatten each channel and use matmul.\n",
    "- torch.einsum\n",
    "See spiral notebook: right now attn is (bs, c, h, w) but I want (bs, c, c, h, w) or (bs, c^2, h, w).\n",
    "\n",
    "UPDATES:\n",
    "I updated class to compute attention weights over all combinations of channels, but I'm still not sure if this is useful. In text, we know it can be useful to represent words as a combination of other words near it (see: training word embeddings). In images, I'm not sure if representing a feature as a combination of other features is as useful. On the other hand, images can also be viewed as graphs and graph neural networks definitely do this, so maybe it's still reasonable. \n",
    "\n",
    "More thoughts:\n",
    "- Could we concat the input X along the channel dimension so we have both the feature maps and the \"weighted combo of features\" maps?\n",
    "- On 2nd thought, convolutions already share information between channels. Maybe all this is completely unnecessary. BUT I don't think they account for relationships between $P_{b0, c0, h0, w0}$ and $P_{b0, c64, h128, w128}$ (i.e. different channels AND different spatial locations). I could imagine this would be useful: e.g. \"this image has a shiny thing (eye) in the middle and a hairy thing (hair) at the top)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:43.401262Z",
     "start_time": "2020-09-28T00:40:43.360610Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q, k, v = attn.projector(x)\n",
    "(q @ k).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:44.862329Z",
     "start_time": "2020-09-28T00:40:44.805794Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2648, -0.1779,  0.0391, -0.7008],\n",
       "         [ 0.1021,  0.1710, -0.0809,  0.2738],\n",
       "         [ 1.5865,  0.4821,  0.2279, -0.2885],\n",
       "         [-2.1455, -1.5005, -0.1557, -1.6630]],\n",
       "\n",
       "        [[ 0.6571, -0.0211,  0.1897,  0.6626],\n",
       "         [ 0.1537, -0.3381,  0.1181,  0.4916],\n",
       "         [-0.6913, -0.1907,  0.1066, -0.3036],\n",
       "         [ 0.6338,  0.5477,  0.5314,  0.4989]],\n",
       "\n",
       "        [[-0.3232, -0.0751,  0.5865,  0.5024],\n",
       "         [-0.3542, -0.1384,  0.1200, -0.0216],\n",
       "         [-0.9016, -0.5958, -0.2570, -0.4870],\n",
       "         [-0.9752, -0.2457,  0.7185,  0.2630]]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q @ k)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:46.533811Z",
     "start_time": "2020-09-28T00:40:46.490346Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 3, 4, 4])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:47.012268Z",
     "start_time": "2020-09-28T00:40:46.964310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1, 4, 4])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:51.540628Z",
     "start_time": "2020-09-28T00:40:51.494673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3, 4, 4])"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Right shape but need to check if this is actually doing that I want.\n",
    "res = torch.matmul(q.unsqueeze(1), k.unsqueeze(2))\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:52.707608Z",
     "start_time": "2020-09-28T00:40:52.662224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3940, -0.0655,  0.0942,  0.3738],\n",
       "        [-0.1032, -0.2228,  0.3847,  0.4522],\n",
       "        [-0.6221,  0.1176,  0.3759, -0.2803],\n",
       "        [ 0.8638,  0.5567,  0.4673,  0.6127]], grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q[0][0] @ k[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:40:53.438971Z",
     "start_time": "2020-09-28T00:40:53.391962Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3940, -0.0655,  0.0942,  0.3738],\n",
       "        [-0.1032, -0.2228,  0.3847,  0.4522],\n",
       "        [-0.6221,  0.1176,  0.3759, -0.2803],\n",
       "        [ 0.8638,  0.5567,  0.4673,  0.6127]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For b, c, h, w: \n",
    "\n",
    "# When h_i=w_j, this is the same as q@k (i.e. self attention).\n",
    "# res[0][0][0] = qk[0][0]\n",
    "# res[0][1][1] = qk[0][1]\n",
    "# res[0][2][2] = qk[0][2]\n",
    "\n",
    "# res[1][0][0] = qk[1][0]\n",
    "# res[1][1][1] = qk[1][1]\n",
    "# res[1][2][2] = qk[1][2]\n",
    "\n",
    "##################################\n",
    "# res[0][0][1] = q[0][1] @ k[0][0]\n",
    "# res[0][0][2] = q[0][2] @ k[0][0]\n",
    "# res[0][1][0] = q[0][0] @ k[0][1]\n",
    "\n",
    "# res[b, c1, c2, ...] = q[b, c2, ...] @ k[b, c1, ...]\n",
    "res[0][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:41:36.318965Z",
     "start_time": "2020-09-28T00:41:36.262742Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000]],\n",
       "\n",
       "        [[1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000],\n",
       "         [1.0000, 1.0000, 1.0000]]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = attn.softmax(res)\n",
    "a.sum((-1, -2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-28T00:42:06.294790Z",
     "start_time": "2020-09-28T00:42:06.213135Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3, 4, 4])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(a * v.unsqueeze(1)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different kind of Spatial Attention from paperswithcode\n",
    "\n",
    "Still trying to figure out why this is useful. Seems like the maxpool\n",
    "should be high in many places because most areas of the image \n",
    "surely activate SOME filter (or maybe not - maybe if we choose a \n",
    "reasonable number of some areas of the image are largely ignored).\n",
    "\n",
    "Maybe it's using these as a sort of hack to learn attention weights? I.E.\n",
    "a high value doesn't tell us what's in that area but it does say \"there's\n",
    "something interesting here\".\n",
    "\n",
    "UPDATES: Noticed they apply this directly to the inputs (no high level features yet). They use this in a \"SpatialGate\" where they pool first, then pass that through a conv-bn-relu layer, then pass that output through a sigmoid. The sigmoid outputs are then used to scale the original inputs (hence the Gate name. Seems reminiscent of lstm/gru). There's actually no softmax at all here.\n",
    "\n",
    "Note: paper says their CBAM module is a dropin for a conv block, not a whole model, and is meant to take in intermediate feature maps. So the fact that the channelpool operates directly on inputs isn't as extreme as it sounded initially. \n",
    "\n",
    "The core of the CBAM block is actually something called ChannelGate which DOESN'T use channel pooling. Need to spend more time understanding what that does. It uses the SpatialGate as an optional second step on the ChannelGate's outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T23:01:20.612607Z",
     "start_time": "2020-09-27T23:01:20.552473Z"
    }
   },
   "outputs": [],
   "source": [
    "class ChannelPool(nn.Module):\n",
    "    \"\"\"From \n",
    "    https://github.com/Jongchan/attention-module/blob/5d3a54af0f6688bedca3f179593dff8da63e8274/MODELS/cbam.py#L72\n",
    "    \"\"\"\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        torch.tensor: Shape (bs, 2, h, w) where all dimensions except the \n",
    "        channel dimension are unchanged from the input dimensions.\n",
    "        \"\"\"\n",
    "        return torch.cat(\n",
    "            [x.max(1)[0].unsqueeze(1), x.mean(1).unsqueeze(1)], dim=1\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T22:49:24.896197Z",
     "start_time": "2020-09-27T22:49:24.793583Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4, 4])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.randn(bs, c, 4, 4)\n",
    "x2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T23:50:48.987301Z",
     "start_time": "2020-09-27T23:50:48.939147Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 4, 4])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = ChannelPool()\n",
    "pool(x2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T22:50:39.349163Z",
     "start_time": "2020-09-27T22:50:39.274331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.9192,  0.9164,  0.9607,  2.4313],\n",
       "          [ 1.7686,  0.9094,  1.0064,  1.5090],\n",
       "          [ 0.9665,  0.8555,  1.1865,  1.9622],\n",
       "          [ 1.9539,  0.0046,  1.6981,  1.0057]]],\n",
       "\n",
       "\n",
       "        [[[ 0.8377,  0.3892,  0.9254,  1.0246],\n",
       "          [-0.9388,  1.1602,  1.6493,  0.6775],\n",
       "          [ 0.3136,  1.5351,  1.2207,  1.2812],\n",
       "          [ 0.4229,  1.1303,  2.7650,  0.6992]]]])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.max(1)[0].unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T22:50:29.289856Z",
     "start_time": "2020-09-27T22:50:29.166458Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2255,  0.2981,  0.3671,  0.6192],\n",
       "          [ 0.9503,  0.2977, -0.4940,  1.0133],\n",
       "          [ 0.2477, -0.4888,  0.3105,  0.9771],\n",
       "          [ 0.0368, -0.1245,  0.2561, -0.4880]]],\n",
       "\n",
       "\n",
       "        [[[-0.0326, -0.2258,  0.0792, -0.1351],\n",
       "          [-1.0789,  0.9497,  0.6635,  0.2583],\n",
       "          [-0.6534,  0.5303,  0.0463,  0.3939],\n",
       "          [-0.2778,  0.6453,  1.0496, -0.1302]]]])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2.mean(1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T22:44:55.730567Z",
     "start_time": "2020-09-27T22:44:55.675996Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5338, 2.8634, 2.2469],\n",
       "        [2.4798, 2.9073, 2.0179]])"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.MaxPool2d(kernel_size=x2.shape[-1], stride=1)(x2).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T21:24:27.151976Z",
     "start_time": "2020-09-27T21:24:27.033337Z"
    }
   },
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(c, c*3, 1, groups=1, bias=False)\n",
    "conv2 = nn.Conv2d(c, c*3, 1, groups=3, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T21:24:36.594555Z",
     "start_time": "2020-09-27T21:24:36.513519Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.8149e-01, -9.8219e-01,  7.0230e-02, -8.1756e-01],\n",
       "          [-5.5817e-01, -2.1600e+00,  7.2470e-01,  6.8560e-01],\n",
       "          [ 5.4090e-01, -6.9522e-01, -3.8275e-01,  6.7900e-01],\n",
       "          [ 2.3532e-01, -8.1782e-01,  2.3031e+00,  7.5006e-02]],\n",
       "\n",
       "         [[-1.2837e-01, -4.3930e-01,  1.5289e-01, -5.9028e-01],\n",
       "          [-4.9126e-01, -1.5850e+00,  3.7685e-01,  2.3783e-01],\n",
       "          [ 3.8888e-01, -2.8695e-01,  4.1176e-01,  2.7987e-01],\n",
       "          [-1.5640e-01, -4.6738e-01,  2.1840e+00,  5.4696e-01]],\n",
       "\n",
       "         [[-1.2380e-01, -1.7929e-01, -4.8263e-02, -2.5773e-01],\n",
       "          [-4.8813e-02, -5.5613e-01,  2.5324e-01,  1.8768e-01],\n",
       "          [ 1.4011e-01, -2.2262e-01, -3.7227e-01,  2.0429e-01],\n",
       "          [ 1.2380e-01, -1.6675e-01,  4.0662e-01, -1.0374e-01]],\n",
       "\n",
       "         [[-2.0736e-01,  7.7723e-01, -2.2901e-01,  1.5783e-01],\n",
       "          [ 5.3646e-01,  8.6285e-01, -1.0946e-01, -3.4214e-01],\n",
       "          [-2.1630e-01,  2.0996e-01, -5.6337e-01, -2.5327e-01],\n",
       "          [ 1.8572e-03,  5.4152e-01, -1.3984e+00, -2.7770e-01]],\n",
       "\n",
       "         [[-4.2009e-01, -2.0410e-01, -1.5220e-01, -7.2710e-01],\n",
       "          [-5.1679e-02, -1.4483e+00,  6.5465e-01,  3.4613e-01],\n",
       "          [ 3.6121e-01, -5.0441e-01, -9.1623e-01,  4.4106e-01],\n",
       "          [ 2.0988e-01, -2.9841e-01,  1.1085e+00, -1.3922e-01]],\n",
       "\n",
       "         [[ 1.3352e-01, -2.5534e-03, -4.7062e-02,  3.0906e-01],\n",
       "          [ 1.6167e-01,  7.1327e-01, -1.6768e-01,  3.1505e-04],\n",
       "          [-1.7226e-01,  7.3418e-02, -2.1834e-01, -5.8078e-02],\n",
       "          [ 1.5365e-01,  1.0668e-01, -1.0154e+00, -3.4175e-01]],\n",
       "\n",
       "         [[ 2.7948e-01,  2.4820e-01,  1.6727e-01,  4.5584e-01],\n",
       "          [-4.4246e-02,  8.6647e-01, -4.9663e-01, -3.5265e-01],\n",
       "          [-2.2068e-01,  4.3562e-01,  1.0094e+00, -3.9044e-01],\n",
       "          [-3.2253e-01,  2.3498e-01, -3.3715e-01,  3.8719e-01]],\n",
       "\n",
       "         [[ 6.3865e-01, -7.2582e-01,  3.4513e-01,  5.7002e-01],\n",
       "          [-4.6542e-01,  5.7658e-01, -4.8019e-01,  1.2406e-01],\n",
       "          [-1.3900e-01,  1.9256e-01,  1.1962e+00, -8.5649e-02],\n",
       "          [-6.4530e-02, -3.1826e-01,  9.6764e-02,  2.0390e-01]],\n",
       "\n",
       "         [[ 6.2601e-01, -9.7860e-01,  3.5146e-01,  4.3845e-01],\n",
       "          [-5.6806e-01,  2.0256e-01, -3.3682e-01,  2.9995e-01],\n",
       "          [-4.3734e-02,  3.2894e-02,  1.0438e+00,  7.3766e-02],\n",
       "          [ 3.4416e-02, -5.0123e-01,  4.3179e-01,  1.3725e-01]]],\n",
       "\n",
       "\n",
       "        [[[-3.0672e-01, -1.2153e-01,  1.8354e-01,  1.8482e-01],\n",
       "          [-5.0897e-02,  9.3339e-01,  3.5599e-01,  4.4426e-01],\n",
       "          [-2.9250e-01,  5.8372e-01, -1.3028e-01, -1.5769e-02],\n",
       "          [ 1.4318e+00,  1.0288e+00, -1.3316e+00, -7.3583e-01]],\n",
       "\n",
       "         [[-7.5482e-02, -5.5534e-01,  4.6207e-01, -2.3967e-03],\n",
       "          [ 4.4778e-02,  4.5290e-01,  2.6991e-01,  1.5071e-01],\n",
       "          [ 2.2322e-02,  4.6748e-01, -2.1430e-02,  3.9321e-01],\n",
       "          [ 1.0302e+00,  6.9784e-01, -1.3139e+00, -7.8576e-01]],\n",
       "\n",
       "         [[-9.9534e-02,  9.2024e-02, -5.3609e-02,  1.0704e-01],\n",
       "          [-1.6434e-01,  1.8397e-01,  1.6225e-01,  6.4235e-02],\n",
       "          [-2.0528e-01,  8.7043e-02, -4.1101e-02, -1.7807e-01],\n",
       "          [ 3.2410e-01,  2.9909e-01, -5.3417e-02, -2.1160e-01]],\n",
       "\n",
       "         [[ 1.0884e-01,  3.0786e-01, -3.0759e-01,  8.6835e-02],\n",
       "          [-4.9899e-01, -6.7436e-01,  1.1952e-01, -4.3295e-01],\n",
       "          [-2.6104e-01, -4.4839e-01,  5.4897e-02, -4.6362e-01],\n",
       "          [-7.4334e-01, -3.0938e-01,  1.4520e+00,  1.1189e-01]],\n",
       "\n",
       "         [[-1.9739e-01,  1.2433e-01, -7.7228e-02,  2.7453e-01],\n",
       "          [-5.7666e-01,  2.6807e-01,  5.1978e-01, -2.0268e-03],\n",
       "          [-5.6377e-01,  1.6735e-01, -7.2949e-02, -4.5091e-01],\n",
       "          [ 7.7372e-01,  7.8967e-01,  3.9028e-02, -7.2560e-01]],\n",
       "\n",
       "         [[-1.2143e-02,  3.3373e-01, -2.5239e-01,  2.7056e-03],\n",
       "          [ 9.5998e-02, -4.3086e-02, -1.9671e-01,  6.1258e-02],\n",
       "          [ 1.5367e-02, -1.6395e-01, -1.5884e-02, -1.8208e-01],\n",
       "          [-4.0938e-01, -3.2308e-01,  4.4929e-01,  4.8591e-01]],\n",
       "\n",
       "         [[ 2.0492e-01, -3.6459e-01,  2.5475e-01, -2.5778e-01],\n",
       "          [ 4.3910e-01, -2.6577e-01, -3.2975e-01, -7.5891e-02],\n",
       "          [ 5.0713e-01, -5.8197e-02,  8.5816e-02,  5.4397e-01],\n",
       "          [-4.5909e-01, -5.1432e-01, -2.9946e-01,  3.1000e-01]],\n",
       "\n",
       "         [[ 1.9310e-02, -2.3177e-01,  2.4710e-01, -3.0537e-01],\n",
       "          [ 1.0640e+00,  5.3189e-01, -6.5507e-01,  5.3019e-01],\n",
       "          [ 7.3393e-01,  2.7634e-01, -1.6679e-02,  7.5238e-01],\n",
       "          [-6.4349e-03, -4.5500e-01, -1.3884e+00,  7.3276e-01]],\n",
       "\n",
       "         [[-6.2365e-02, -1.7908e-01,  2.3079e-01, -2.5745e-01],\n",
       "          [ 1.0756e+00,  7.6059e-01, -6.1361e-01,  6.5974e-01],\n",
       "          [ 6.6316e-01,  3.8487e-01, -5.4080e-02,  7.0289e-01],\n",
       "          [ 2.5763e-01, -2.7275e-01, -1.6174e+00,  6.6670e-01]]]],\n",
       "       grad_fn=<MkldnnConvolutionBackward>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T21:38:30.431875Z",
     "start_time": "2020-09-27T21:38:30.370312Z"
    }
   },
   "outputs": [],
   "source": [
    "shape = conv2.weight.shape\n",
    "with torch.no_grad():\n",
    "    conv2.weight.data = torch.arange(shape[0])\\\n",
    "                             .view(shape[0], *[1]*(len(shape)-1)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T21:38:31.158795Z",
     "start_time": "2020-09-27T21:38:31.097106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.1026e+00, -1.3052e+00,  3.2674e-01,  1.2588e+00],\n",
       "         [-3.4807e-01,  1.8726e+00, -7.8483e-01,  4.2860e-01],\n",
       "         [-4.4428e-01,  1.7124e-01,  7.2784e-01, -7.9687e-04],\n",
       "         [ 4.4602e-01, -4.2235e-01, -1.8255e+00, -6.7862e-01]],\n",
       "\n",
       "        [[-6.5279e-01,  1.4068e+00, -5.6674e-01, -8.4416e-02],\n",
       "         [ 1.0725e+00,  9.4541e-01,  1.6971e-01, -4.5089e-01],\n",
       "         [-2.3649e-01,  1.0384e-01, -1.7775e+00, -2.2743e-01],\n",
       "         [ 1.8197e-01,  9.0580e-01, -2.3105e+00, -7.3189e-01]],\n",
       "\n",
       "        [[-1.8626e-01,  1.9660e+00,  4.4586e-02,  3.6956e-01],\n",
       "         [ 4.6333e-01,  1.4049e+00, -7.5455e-01, -1.3606e+00],\n",
       "         [-3.7741e-01,  1.0789e+00,  1.6159e+00, -1.1100e+00],\n",
       "         [-1.0816e+00,  1.1959e+00, -4.8594e-01,  1.2200e+00]]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T21:39:17.413097Z",
     "start_time": "2020-09-27T21:39:17.365869Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 9, 4, 4])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = conv2(x)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T21:39:41.793855Z",
     "start_time": "2020-09-27T21:39:41.737707Z"
    }
   },
   "outputs": [],
   "source": [
    "q, k, v = z.view(z.shape[0], c, c, *z.shape[-2:]).transpose(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T21:39:48.439700Z",
     "start_time": "2020-09-27T21:39:48.387891Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 3, 4, 4]), torch.Size([2, 3, 4, 4]), torch.Size([2, 3, 4, 4])]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smap(q, k, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was experimenting with einsum as a way to do my q k broadcasted matrix multiplication, but fortunately I found a different (simpler) way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T23:56:16.788848Z",
     "start_time": "2020-09-27T23:56:16.742759Z"
    }
   },
   "outputs": [],
   "source": [
    "x3 = torch.randn(2, 3, 4, 4)\n",
    "x4 = torch.randn(2, 3, 4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T23:58:02.345222Z",
     "start_time": "2020-09-27T23:58:02.250964Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[ 1.2403e+00, -1.0589e+00, -2.6218e+00, -2.1962e-01],\n",
       "           [ 1.5456e+00, -1.3448e+00, -1.5296e+00, -1.4196e+00],\n",
       "           [-7.1985e-01,  1.1622e+00, -8.5005e-01,  1.0664e+00],\n",
       "           [ 3.0225e+00, -2.6262e+00, -5.9555e+00, -8.4185e-01]],\n",
       "\n",
       "          [[ 1.2202e+00,  1.9351e-01, -1.0199e+00, -4.8780e-01],\n",
       "           [ 2.0950e-01, -8.8629e-01, -7.4623e-03,  2.1819e+00],\n",
       "           [-4.0034e-01,  1.5097e-01, -7.1156e-02, -4.0540e-01],\n",
       "           [ 2.8856e+00, -8.0512e-01, -1.6272e+00,  1.5047e+00]],\n",
       "\n",
       "          [[ 6.4828e-01, -1.3674e-01,  5.0591e-01, -1.0983e+00],\n",
       "           [-1.0043e+00, -4.7152e-03, -4.1933e+00,  1.7472e+00],\n",
       "           [ 1.4408e+00, -1.5071e-01,  4.6706e+00, -2.3493e+00],\n",
       "           [ 1.6439e+00, -3.7970e-01,  4.8559e-01, -1.8974e+00]]],\n",
       "\n",
       "\n",
       "         [[[-1.0202e+00,  5.2712e-02,  1.8747e+00,  1.2337e+00],\n",
       "           [ 2.9561e+00, -1.4705e+00, -6.5982e+00, -1.5241e+00],\n",
       "           [-1.2448e+00,  1.0389e+00,  3.0559e+00, -4.5393e-02],\n",
       "           [-1.6221e-01,  1.8713e+00, -2.5982e+00,  1.1323e-01]],\n",
       "\n",
       "          [[ 1.8284e+00, -7.2990e-02,  1.5571e-01, -6.6986e-01],\n",
       "           [ 6.1989e-02, -1.1879e+00, -9.3350e-01,  3.1549e+00],\n",
       "           [-1.4802e+00, -5.4434e-01,  1.3635e+00,  1.3063e+00],\n",
       "           [-3.1917e+00,  9.2504e-01, -3.6160e-01, -1.0202e+00]],\n",
       "\n",
       "          [[ 1.7246e+00, -8.0637e-02,  3.6115e+00, -1.4376e+00],\n",
       "           [ 2.5786e-01, -3.0844e-01, -1.3541e+00, -7.2380e-01],\n",
       "           [-1.0201e+00,  1.6828e-01, -1.5942e+00,  1.8305e+00],\n",
       "           [-5.6118e-01, -1.4103e-02,  1.7793e+00, -1.2530e+00]]],\n",
       "\n",
       "\n",
       "         [[[-5.6911e-02, -7.2367e-01,  1.2611e+00,  1.1722e-01],\n",
       "           [ 2.3861e+00, -5.3156e-01, -6.5073e+00, -1.0947e+00],\n",
       "           [ 1.1314e-01, -7.6143e-01, -1.4681e+00,  1.4783e+00],\n",
       "           [ 2.4709e+00, -1.6508e+00, -5.8525e+00, -6.1746e-01]],\n",
       "\n",
       "          [[ 1.3764e+00,  1.5737e-01, -1.3165e-01, -7.5088e-01],\n",
       "           [-1.1455e+00,  3.5251e-01, -1.5096e+00,  1.6890e-01],\n",
       "           [ 3.2714e+00,  2.2577e-01, -1.0930e+00, -1.4897e+00],\n",
       "           [ 1.6941e+00, -1.0768e+00, -1.1294e+00,  2.2915e+00]],\n",
       "\n",
       "          [[ 1.6118e-01,  4.2508e-02, -5.1003e-01,  3.1936e-01],\n",
       "           [-2.8438e-01, -1.9471e-01, -4.7525e-01, -1.3202e+00],\n",
       "           [ 3.1881e+00, -3.0869e-01,  6.3065e+00, -3.7818e+00],\n",
       "           [ 1.6939e+00, -3.9658e-01,  1.5650e+00, -2.1394e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[-9.1499e-01, -2.4445e+00,  1.0061e-01, -7.7175e-01],\n",
       "           [ 1.9895e+00, -5.5236e-01, -7.2024e-01, -5.1337e-01],\n",
       "           [ 3.7976e+00, -2.8874e+00,  3.4040e+00, -4.5561e+00],\n",
       "           [ 5.1491e+00, -2.4342e+00, -3.6956e+00,  8.8746e-02]],\n",
       "\n",
       "          [[-1.5503e+00,  1.6820e+00,  3.4523e+00, -4.7760e-01],\n",
       "           [ 9.4349e-01, -1.3098e+00, -1.3343e+00, -1.2083e+00],\n",
       "           [-3.2575e-01, -3.3757e-01,  4.1809e+00, -4.9389e+00],\n",
       "           [ 2.0317e+00, -1.3324e+00, -4.5910e+00, -6.1330e-01]],\n",
       "\n",
       "          [[ 2.6794e+00, -2.7634e+00,  1.5482e+00, -3.2433e+00],\n",
       "           [-5.6589e-01, -1.7328e+00,  5.9881e-01,  1.4312e-01],\n",
       "           [ 6.1499e+00, -4.0057e+00, -5.3387e+00, -2.6308e+00],\n",
       "           [-2.8739e+00, -5.3159e+00,  4.4045e+00, -2.2693e+00]]],\n",
       "\n",
       "\n",
       "         [[[-6.0352e+00, -2.1707e+00, -1.1799e+00,  1.4613e+00],\n",
       "           [ 5.1556e+00,  1.6843e+00, -1.6378e+00,  1.8897e+00],\n",
       "           [-6.9149e-01, -9.0410e-01, -2.8298e+00,  1.4771e+00],\n",
       "           [ 5.1734e+00,  7.5223e-01,  8.7446e-01, -6.6700e-01]],\n",
       "\n",
       "          [[-3.3392e+00,  3.4029e+00,  5.4240e+00,  2.1191e+00],\n",
       "           [ 2.4798e+00,  1.0591e+00, -8.0850e+00,  3.9314e+00],\n",
       "           [ 7.2535e-02, -8.8153e-02, -1.2606e+00,  1.0485e+00],\n",
       "           [ 1.8094e+00,  2.5123e-01, -4.1538e+00,  5.8148e-01]],\n",
       "\n",
       "          [[ 1.9181e+00, -1.5039e+00,  5.8478e+00, -3.5252e+00],\n",
       "           [-5.1844e+00,  2.5211e+00, -1.9616e+00, -5.0107e-01],\n",
       "           [-2.1832e+00, -1.9445e+00,  5.2343e+00, -7.9600e-01],\n",
       "           [-1.1210e+00,  1.2085e+00, -4.9007e+00, -4.5596e-01]]],\n",
       "\n",
       "\n",
       "         [[[-2.4469e+00, -1.5813e-01,  8.6683e-01,  1.0579e-01],\n",
       "           [ 7.6142e+00,  1.8472e-01,  1.7754e+00, -2.4287e+00],\n",
       "           [-1.5843e+00, -3.8692e+00,  1.5984e+00, -1.2831e+00],\n",
       "           [-2.9487e+00, -1.4286e+00, -1.3025e+00,  7.6487e-01]],\n",
       "\n",
       "          [[-1.4083e+00,  1.4503e+00,  2.6906e+00,  6.2094e-01],\n",
       "           [ 2.6030e+00, -1.2592e+00, -4.0945e+00, -1.9360e+00],\n",
       "           [-3.3763e+00,  5.2300e+00,  6.4766e+00,  8.7871e-01],\n",
       "           [-1.4553e+00,  9.9507e-01,  2.5029e+00,  5.3702e-01]],\n",
       "\n",
       "          [[ 1.5852e+00,  7.8385e-01, -1.2153e-01, -7.6329e-01],\n",
       "           [ 1.5309e-01, -3.4391e-01, -6.9213e+00, -1.2151e-01],\n",
       "           [ 5.5724e+00, -2.7688e+00, -2.7317e-01, -7.0380e+00],\n",
       "           [ 5.7565e-01, -1.7756e+00,  4.3500e+00, -1.5623e+00]]]]])"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Right shape but no clue if this is actually doing that I want.\n",
    "res = torch.einsum('abij,acjk->abcik', x3, x4)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-27T23:58:04.809354Z",
     "start_time": "2020-09-27T23:58:04.766921Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3, 4, 4])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
