11/1 - Planning
---------------
Deadline: 12/31/20 (depending on what I decide to include here, this could end up being as short as a few days or as long as a few months. I think it's important to set some sort of soft timebox so this is it. Extending past this would be fine if I want to and wrapping up far earlier would also be fine.)

Final Product: 
    -addition of transformers-based data augmentation capabilitites to incendio. I expect this to include 3 methods though I'm open to change:
		-mask filling: swap 1 or more words in the source
		-text generation: truncate source and fill in the end with seq2seq
		-paraphrase: pretty simple, just use pretrained model
	Each method should have 3 possible interfaces:
		-base function: the core functionality. Pass in a string and return an augmented string.
		-composable random transform: It's possible this won't require anything different than the base function, but basically I want it to be super easy to plug these into a torch dataset. This would let us augment data on the fly. In reality, I suspect this would be way too slow and we'd want to pre-compute these, but it would be nice to have the option.
		-CLI: provide the option for a user to run something like "incendio augment data/searches.csv --out_path data/searches-augmented.csv --mode mask --mask_n 2" to create a new file with augmented text. Maybe should support other common file structures, e.g. 1 file for each item.

	-Optionally, this could include tasks such as:
		-CLI: class to make it easier to construct a train.py script given a Trainer. Basically, I've found it still ends up being annoyingly slow to construct very similar training scripts for each project, even with all the boilerplate incendio.Trainer provides. There's a wide range in possible complexity here: on the high end, I have some fuzzy vision of something that identifies all the possible args/kwargs used in a script and jams them all into one train func so we get them as command line options. But maybe that's overcomplicating things and I just need to enforce a stricter approach to what is called in the train script and then call them in order (get_data, get_callbacks, get_metrics, etc.).
		-finalize work from annotated gpt2 notebooks: port some attention-related layers/helpers to incendio. Some of this stuff might already be available in huggingface or pytorch but it wouldn't hurt to implement them myself anyway to better understand them.
		-finalize and port work from spatial attention notebooks: probably too computationally intensive to be actually useful at this point, and it sounds like similar concepts may already be common. But it would be cool.
		-layers/building blocks of lambda networks: would be cool and a good way to make me take the time to really understand what they're doing.
		-various einops-based layers
		-tensor debugger: inspired by image from einops tutorial, see if I can come up with something to make it easier to validate that a layer/model does what I think it does to a tensor. There are often cases where I think something's working but I want to 100% confirm that it's not jumbling up batches/axes in an unintended way. Could use a similar concept (perhaps an input image where every batch/row/column has a different "color" (rgb value)?) or see if I can do something similar to one of my img_wang strategies (attaching attributes to tensors to keep track of specific items in a batch; however, I've found it's hard to make these things stick. Many tensor operations seem to delete these, perhaps because they're copying or creating new tensors under the hood and base tensors don't have my added attributes. Might be solvable through monkeypatching.)
		-massive overhaul to trainer: make it possible to access everything during training (set self.xb, self.yb every batch)? Or maybe a wiser approach is to mimic lightning and make it easier for user to overwrite certain steps (e.g. instead of loss = self.criterion(y, y_hat) call loss = self.compute_loss() (unsure of args at the moment, maybe none?) where "compute_loss" is a method of trainer, as opposed to self.criterion which is an attribute containing a torch loss function. Idea is we want it to be easier to do custom stuff like passing x to a loss function for contrastive loss or training a seq2seq model without unnecessarily duplicating tensors.
		-Building + hosting docs - did I ever do this? I can't remember.
		-better readmes - in the spirit of fleshing out the library.
		-semi cheating since it's not incendio, but htools docs and readme - would be nice to update those too.

Concepts: 
    -interface design: practice writing useful, user-friendly components. I don't know that these need to be particularly customizable (i.e. user (me) will be using the finished product, not building custom variants), but they should be flexible enough that if useful new pipelines types emerge, I can easily add them.
	-features of python packaging: strengthen knowledge of CLIs, optional dependencies, etc. I like the idea of letting users install incendio (with no deps) or incendio[all] (all deps). 
	-optional: if implementing things like lambda layers or finishing off attention-related layers, this will involve some solid ML comprehension.

Tech: 
	-Can't think of anything new I'd need to use or that would be particularly useful here. Perhaps focusing on getting github actions working since I'm pretty sure Incendio's still failing whatever gets run on git pushes.
	
Requirements:

	-Complete the transformers augmentation bullet points outlined in Final Product.
	-Update docs and readme.
	-Other items are optional, though I think it would be a good idea to close out some of these items and this is a good opportunity. But I don't want to require it since this was initially mean to be a quick project to re-energize me prior to a larger, more ambitious project.

Pre-Mortem:
    What could go wrong?

	-I get bored and tempted by other project distractions, e.g. Liza.ai. 
		[SOLUTION] I made the requirements flexible enough that this project should be pretty quick to wrap up if I want to. If I find myself getting distracted, toss the optional ideas for now and just churn out the mandatory items. This should let me move on to other stuff pretty quickly. Also, this system has been highly effective so even if I get distracted it won't be enough to overpower my discipline. If img_wang didn't break me, I can't imagine a scenario where this does. Famous last words, etc., but I really don't see that happening.

	-I spend most of my time struggling with nbdev to get docs and workflows working, and this frustration ends up overpowering the intended refreshing benefits.
		[SOLUTION] Even if this is true, it would still be a worthwhile project - building docs, at least, seems pretty important. And so far the project's been pretty fun. And I can think of this as part of the reality of library building.

	-Code burnout due to work coding, incendio coding, and DS/Algos coding.
		[SOLUTION] Can back off the DS/Algos coding a little if truly necessary (e.g. weekends only? It would be nice to get the daily accumulation benefit but I do think that might be pushing things cognitively), or fit that in as post-5 pm work.
	
	-More and more ideas pile up and this turns into a never-ending project at the cost of other cool stuff (liza.ai, eleuther.ai, openmined research, fastai/lightning/spacy/allennlp/torch contributions).
		[SOLUTION] Would this be so bad? It would likely mean incendio turns into something pretty damn cool. There is the downside that putting off contributing to larger open source projects allows me to retain certain weaknesses (e.g. a little fuzziness about certain aspects of git that don't matter when you're the only contributor). This also hides the reality of working on open source: maybe if I contributed to big projects, I'd find that the reality is mostly fixing documention typos, writing unit tests, and occasionally tracking down obscure bugs in other people's code (in fact, I think this is pretty likely). If that's true, maybe I should confirm it soon so I don't spend too long in pursuit of a long-term goal I wouldn't enjoy. But I want to do a fun project now so I don't like the idea of picking something that has a significant chance of being unrewarding. This is a bit of a conundrum but it's maybe less of an issue to worry about in a pre-mortem and more of a tradeoff to be aware that I'm making.

	-Not knowing when to end. I wrote down so many optional items that this could last anywhere from days to years.
		[SOLUTION] I think I can play this by ear initially, but to provide some guidance, how about this: I'll say the project should last between 1 and 8 weeks, inclusive. If I want to end earlier or start later, I have to return here and write 100+ words justifying it.In other words, I can do what I want but I need to treat the decision with respect and put serious thought into it if I want to break from these rather flexible constraints.

Most of these problems don't sound like real problems. This project can be pretty short and low stakes if I want it to be so that seems reasonable. Img_wang had some unforeseen problems so I shouldn't be too confident in this. Perhaps this is a good section to add: what were the gaps between expectations and outcomes last time and why should I be confident they won't reoccur this time?

-not enjoying what I thought I'd enjoy: I'm less worried about that here because I've already spent quite a bit of time on library development and generally found it extremely enjoyable. I've always have found model training a bit exhausting in a way that library dev never has been.
-unclear finishing constraints: if anything this is even fuzzier with so many optional items, but I think the timeboxing helps.
-fulfilling the letter of the requirements but not the spirit: I'm not sure what that would even mean in this context. I suppose I could encounter a situation where I get docs working in something other than nbdev, or I run into a limit of how many github pages I can deploy (don't think that exists though). Not too worried about this.

11/1/20
-------
Progress: Officially chose project 3. Wrote plans and pre-mortem. 

Plans: Start working on paraphrasing augmentation. I'd like to have rough versions of all 3 augmentations, then I can think more about how to refactor them into a better, more consistent interface. That can be followed by torch transform and CLI, in that order.

11/2/20
-------
Progress: Wrote paraphrasing function. Started working on a more cohesive api: built initial version of ParaphrasePipeline (no pegasus pipeline for this task) and FillMaskTransform. Experimented a bit with a more general TransformerTransform but at the moment I'm a bit confused whether I want it to be a parent class, an abstract class, or a high level wrapper. Need to think about this - maybe simplest to build out the three separate transform classes first, then see how I might refactor them.

Plans: Write (or at least start) paraphrase transform and/or generation transform classes. Still need to think a bit about current implementation of mask transform: do we want this to act on strings, list-likes, or either? Mask transform also has the issue of n (number of words to mask) vs. n (number of variants to return).

11/3/20
-------
Progress: Ported parallelize func to htools and wrote docs. Updated parallelpipeline to accept either strings or list-like objects (tried to use multiprocessing but this was taking ages, maybe trying to pickle model?). Built first pass at ParaphraseTransform and GenerativeTransform. Adjusted fillmasktransform (and new ones) to allow passing in pipelines, mostly to speed up dev time - have to consider whether I want that functionality to stick around.

Plans: Continue building out tfms. Consider how I want to handle options (should params be provided at instantiation or in __call__? Might be nice to have defaults in __init__ but allow overriding them in __call__ for on the fly tfm purposes). Think about desired inputs/outputs and if my current implementation aligns: for ex, should n (# of variations) be inside tfm or should we just call __call__ multiple times, or maybe this should happen in my higher level wrapper TransformerTransform (this last one sounds better)? If finish, begin thinking about refactoring (do we want base class/mixin, wrapper, both? And how should everything fit into my existing RandomTransform framework - we could just wrap them, of course, but at some point it seems like we're creating a ton of different classes and maybe there's a point of over-abstraction here?)

11/4/20
-------
Progress: Wrote listlike() helper function (eventually should port this to htools). Made each transform able to handle lists or strings (both for __call__ and _preprocess - maybe overkill but might be nice to be able to preprocess lists all at once). Thought a bit about interfaces and possible refactoring strategies but everything seems just different enough that I'm not sure it calls for it. Adjusted generativetransform slightly to return list of strings rather than list of dicts.

Plans: Consider a single high level class to interact with, i.e. TransformerTransform('fill-mask'), and build if it seems like a good idea. Other possible tasks: port listlike to htools, write new tolist() func to accompany it, port transforms and pipelines to incendio, write docs.

11/5/20
-------
Progress: Experimented a bit with possibility of a base class but ultimately decided against it. Experimented a bit with possibility of a high level wrapper but also decided against that. (See paraphrasing-transform.ipynb for rationale for both.) Added reprs to all 3 transforms. Ported listlike to htools and wrote new `always_true` and `tolist` funcs (probably should test both though).

Plans: Investigate possible issue of max batch size in ParaphrasePipeline. Port classes to htools and write docs.

11/6/20
-------
Progress: Adjusted FillMask transform to handle lists more efficiently. Also updated its error handling to deal with sequences that are too short to mask while maintaining min_keep. Updated generative transform to get length from pipeline tokenizer rather than naive split + hacky adjustment. Tried mask and generative transforms on list of 100 sentences to benchmark speed and find bugs. Tried paraphrase transform on 10 sentences since it's very slow (maybe it's faster on GPU?) and started running on 100 sentences but haven't seen results yet.

Plans: Consider if I should do anything about FillMask returning 5 examples per row by default (inconsistent interface?). Other options: port classes to incendio, write docs, test paraphrase transform on GPU to see if it's faster. Look at results of paraphrase transform on 100 sentences.

11/7/20
-------
Progress: Found and fixed bug where FillMask tfm was dropping n-1 branches of the masking tree (this had quite a large effect, dramatically reducing the number of samples returned. Fixing it was not trivial.). Added ability to set n (a.k.a. topk) in fillmask tfm. Also added option to select best (most likely) or random sequences when specifying n in fillmask tfm.

Plans: Consider if we can make the n/topk/self.n interface a little simpler to understand for fillmask tfm. Consider my choices of defaults for fillmask tfm. Work on adding gpu support to paraphrase tfm. 

11/8/20
-------
Progress: Tested ParaphraseTFM gpu support in colab and timed it (~7x faster). Refactored nlp tfms a bit (still have it in my mind that we might be able to refactor our a base class, but right now it might just be for init which seems not worth it). Rebuilt FillMask n system a little bit: now we have self.n and self.max_n. Also adjusted local n param in FillMask __call__ to use -1 the way I used to use None (wanted None to work like the other tfms where it falls back to self.n).

Plans: Consider expanding what params are available in constructor (ideally all that are available in __call__; this reminds me of my desire for a better version of kwargs_fallback() than my current htools version. I'd prefer a decorator that auto adds all init kwargs to the func signature). Other option: finally get started porting to incendio and writing docs.

11/9/20
-------
Progress: Ported 4 classes to incendio and added a few examples for each without commentary. Documented some GenerativeTfm params. Added preprocess kwargs to __call__ for generativetfm. Experimented a bit with an @abstractattrs class decorator (see ipython) but didn't quite get it working yet.

Plans: Write more docstrings. Maybe add some explanation of examples. Maybe fiddle a bit more with @abstractattrs.

11/10/20
-------
Progress: Wrote docs for ParaphrasePipeline, ParaphraseTransform, GenerativeTransform, and part of FillMaskTransform.. Explored huggingface model hub a bit and found some other paraphrase model options (hoping some might be smaller and/or better).

Plans: Finish docs for FillMaskTransform. Maybe some written explanation of examples. Or pick one of outstanding TODOs below.

11/11/20
-------
Progress: Finished docs for FillMaskTransform. Started adding kwargs to more transform __call__ methods but after looking through transformers repo, realized this only seems available for text generation models. Started working on BackTranslationTransform but found out huggingface provides no pretrained _ to english models (only english to _). Looked through model hub a bit more and found some intriguingly named math models, but there are no examples.

Plans: Explore loading other paraphrase models in colab (faster to test on gpu and nice to avoid clogging up laptop storage if possible). Maybe try out some of the math models in colab (may require some guesswork or internet sleuthing to determine how to use these since no examples are provided; not sure what I'd end up doing with these but maybe once I see what they can do some ideas will arise).

11/12/20
-------
Progress: Tried loading 2 new paraphrase models in colab (turns out they're the same). They were less than half the size of my current paraphrase model and a bit faster but I had some trouble getting good results (they tend to continue past 1 sentence and end up getting cut off, though that didn't happen in the sample code snippet and it was trained on quora question pairs so it seems odd that it would learn to generate longer sequences). Discovered there's a TextToTextPipeline that seems compatible with pegasus. Started rewriting ParaphraseTransform to use it (probably good for consistency, and I think it does some other useful things like making sure we don't download the model multiple times). 

Plans: Continue working on new ParaphraseTransform and test it, ideally both on CPU and GPU. Confirm my decision to exclude smaller paraphrase model due to quality issues (if I can solve these, it would be appealing due to size/speed gains). Maybe test other tfms on GPU (realized Pipeline accepts a "device" arg but I'll have to check if pipeline does; regardless I think my method of manually placing pipe.model.to(DEVICE) should work).

11/13/20
--------
Plans: Tried all pipelines on colab and realized they don't use the GPU by default. Changed that behavior and added a warning if GPU is available and they're not using it. Fixed bug in new ParaphraseTransform where results were being incorrectly mapped back to inputs. Fixed bug in GenerativeTransform where some args weren't being passed to __call__ when input is listlike. Added way to get name from pipeline if a pipe is passed in.

Progress: Maybe try out smaller paraphrase pipeline again to see if I can get it working - if so, refactor to allow it; if not, make decision and close the book on that. Maybe worth one more look to see if we can refactor out a baseTfm since inits look so similar. Other options: pick one of todo items below. 

11/14/20
--------
Plans: Tried every paraphrase model I could find on GPU and none compared well to the pegasus model. Decided to leave that as is. Earlier today, it occurred to me that I can speed up fuzzykeydict with LSH or something similar and started experimenting with some components from datasketch package.

Progress: Brief diversion: see if I can feasibly upload stormlight model to Model Hub and serve the streamlit app temporarily using colab and ngrok (follow example in open tab). Want to see if this is possible before ROW is released. Afterwards, I'll get back to this. (If I decide that doesn't count as incendio work, I can take another look at my tfm classes and see if we can refactor out a base class.)

11/15/20
--------
Plans: Made a couple final tweaks to paraphrase tfm (basically decided to not worry about supporting non-pegasus models for now after a brief attempt at it - makes more sense to have user pass in pipe explicitly in that case. Updated docstrings a bit.). Then spent most of my time trying to get stormlight streamlit app running in colab with ngrok (first with docker, then without). Got it running successfully once but still need a few tweaks to ensure the process is reproducible with no user troubleshooting.

Progress: Try running through full colab streamlit process again from start to finish and confirm it works. Update readme with link to colab and brief description of what it does. For a more incendio-specific task, could start creating bare bones files for tfm CLI.

11/16/20
--------
Progress: Mostly worked on getting colab notebook working (still had a fair bit of troubleshooting to get done but I think it works now). Updated readme, added instructions to colab notebook, added colab-specific requirements.txt, added GPU support for text generation, and recorded a new gif for the readme. Uploaded gifs to imgur and wrote comment with some background. Very briefly started creating skeleton for incnedio text augmentation CLI.

Plans: Maybe write up a short description and post to r/stormlight_archive. Start fleshing out cli.py.

11/17/20
--------
Progress: Submitted post to r/stormlight_archive. Thought about cli interface a bit and started writing signature and docs (would be nice to have my CLIConstructor idea implemented for this). Decided to try randompipeline, brainstormed interface a bit, and started writing implementation.

Plans: Diagnose and fix randompipeline repr. Update tolist to allow repeating primitives n times. Consider higher level wrapper (NLPTransform) and or a different kind of random transform (pick 1 of n rather than apply tfm w/ prob p).

11/18/20
--------
Progress: Diagnosed and fixed randompipeline repr (this was because BasicPipeline assumed callables were all functions but here they're classes. Updated repr in htools). Wrote fancier version of tolist that can repeat primitives to a desired length, updated docstring, and re-ported to htools. Finalized and wrote docstrings for RandomPipeline.

Plans: Port RandomPipeline to Incendio (prob belongs in the data module but give this a little thought) and maybe start porting some examples. Could also begin on a different kind of random transform that always applies 1 of n transforms (rather than all n in order) or return to task of NLP transform cli.

11/19/20
--------
Progress: Wrote docs for plot_images and made it work on numpy images as well. Added a couple examples. Ported RandomPipeline and examples to incendio.data. Worked on CLI functionality (so far keeping it simple: 1 tfm and load from 1 csv -> 1 output csv).

Plans: Find better csv for testing (recent-grads.csv text col is too short). Try to get basic functionality working (output a few rows to csv). Next items: add error handling options to cli, maybe option to return df (when importing function instead of cli, if I want to allow that?). Consider reworking behavior where I return a nested list instead of a flat one (maybe allow options for both).

11/20/20
--------
Progress: Tried to figure out how to get cli working in setup.py (I've done this with click before but not fire). After much frustration, I made some progress: incendio is now recognized as a command, though it's calling generate automatically without any args. Also spent a while exploring various approaches to auto-bump version in htools (and, once that works, in all of my packages) but haven't found a satisfactory answer yet (extra args are apparently a bad idea for makefile commands, couldn't get bumpversion py package working).

Plans: Continue trying to get console script working or continue fleshing out script (running with python incendio/cli.py for now). Alternatively, take a break from this frustration and work a bit on minhash stuff for future version of fuzzykeydict. Or pick something from Todos below.

11/21/20
--------
Progress: Updated AxialEncoding, MultiAxialEncoding, and BloomEmbedding to have user-facing embedding_dim attributes for consistency with nn.Embedding. Updated all 3 transforms to return flat output by default and fixed bug in 1 where a couple kwargs weren't passed to the listlike version (also wasn't using param names before so I updated that for safety). Updated docstrings. Found text data to use to test CLI (quora question pairs from msan631). Moved CLI to scratch notebook for now since CLI will create pipeline every time which is slow for testing. Started experimenting with ways to re-attach ID cols to augmented output which is what motivated the switch to flat outputs.

Plans: Continue working on re-attaching ID cols to flat outputs in scratch notebook. If I need a change, could start work on cliRunner, return to abstractattrs cls decorator, or return to work on minhash stuff.

11/22/20
--------
Progress: Finished writing simple version of generate() function (more flexibility to pass in a source path or df; option to avoid saving output; force output to be flat; reattach ID cols after generation). Tried to use linear algebra 1d and prime factor seq2seq models in colab but couldn't get them working correctly, though I did find a couple t5 tuning tutorials that look useful. Messaged the the person who uploaded the models to see if he could provide examples.

Plans: Document generate and port to lib (thinking I should rename this and place it in nlp module, but could potentially go in data module). Still need to investigate how to make this available as CLI (maybe investigate nbdev's call_parse decorator). Other option: maybe return to abstractattrs deco.

11/23/20
--------
Progress:

Plans:




# TODO: outstanding tasks

- CLI to generate augmented text (to new csv or to many txt files, for example)
-high level wrapper for random tfm? Meaning for each sample, randomly pick 1 or n of fillmask/generate/paraphrase/backtranslate. Or maybe this should be a RandomPipeline that isn't text transform-specific?
- add stormlight model to huggingface model hub
- try out huggingface math models in colab (avoid taking up space on laptop):
    https://huggingface.co/mrm8488/t5-base-finetuned-math-linear-algebra-1d
    https://huggingface.co/mrm8488/t5-base-finetuned-math-linear-algebra-2d
    https://huggingface.co/mrm8488/t5-base-finetuned-math-qa-test
    https://huggingface.co/mrm8488/t5-base-finetuned-math-seq-next-term
    https://huggingface.co/mrm8488/t5-base-finetuned-math-calculus-differentiate
-abstractattrs decorator (might fit better in htools. see lsh scratch nb)
-extend lshforest to work with levenshtein/other similarity measure(s)?
    -update FuzzyKeyDict to use this (should be much more efficient for cases where len(dict) is large, which is usually the case for any kind of embeddings w2i dict). Consider whether this belongs in incendio.nlp instead.
-Update spellotape (and/or incendio?) to allow easy loading of domain embeddings and TLD embeddings.
-auto bump version (htools?)
-decorator: let kwargs in method optionally override any instance vars
-cliRunner: unsure exactly how this would work, but somehow find all the functions in a script, then add all their arguments (with default values) to a top level main function's signature which will then be called from the command line.
-updated version of log_cmd that logs full signature instead of just explicit commands (check first if I have any function that already does this - thought @debug could but doesn't look like it. Maybe I'm remember the combination of @debug and @log_stdout, but that's not quite the same.)
    -update log_cmd to handle lists/dicts/bool vars better
