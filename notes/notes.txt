Thoughts on axial encodings vs bloom embeddings

# Cat pros

-smaller emb matrices

# Cat cons

-different items will share large blocks of emb vectors. Maybe this works if we have some pre-defined knowledge of what words should be indexed close together. Sorting by frequency does this somewhat, but maybe would work better to first cluster by w2vec, sort so similar words are close together. Maybe this is why this is good for positional encodings: adjacent time steps share some (but not all) of embs

# Add pros

-words won't necessarily end up sharing embs w/ other words even if most of their rows are the same - the 1 unshared row could learn to be very different

# Add cons

-larger emb matrices  
-each row serves multiple purposes - maybe "pulled in multiple directions" by diff words

---
NLP augmentation pipeline and transform thoughts:

-huggingface pipelines can accept either a single string or an iterable container
    -> therefore, my paraphrasePipeline should do the same
-should transforms work on strings, list-likes, or either?
    Use cases:
        1. generate csv - in this case, we'll need to process a large number of items. This supports accepting list-likes. However, this will all happen under the hood so we could easily make __call__ process a single string and use the pipeline in a way that handles more.
        2. Torch dataset on-the-fly transform - process single str
        3. general function - unsure.
    Either way, we probably should have a function (whether it's __call__ or something internal) that processes a single str at a time. The only reason not to do this would be if I realize something about the way we batch strings makes this undesirable.

---
Problem:

1. Sometime during tonight's FillMask edits, __call__ now only returns n items for each round of masking. Before, this number grew quickly: when inputting 1 string, the first round would produce (by default) 5, the second would product 25, the third 125,... (5**n). I'm not sure the new version is bad - I think it may be faster - but I'm confused why this has changed.
